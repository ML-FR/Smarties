Content;Class;SubClass;class_labelized
"Agriculture is the cultivation of land and breeding of animals, plants and fungi to provide food, fiber, medicinal plants and other products to sustain and enhance life. Agriculture was the key development in the rise of sedentary human civilization, whereby farming of domesticated species created food surpluses that enabled people to live in cities. The study of agriculture is known as agricultural science. The history of agriculture dates back thousands of years people gathered wild grains at least 105,000 years ago, and began to plant them around 11,500 years ago, before they became domesticated. Pigs, sheep, and cattle were domesticated over 10,000 years ago. Crops originate from at least 11 regions of the world. Industrial agriculture based on large-scale monoculture has in the past century come to dominate agricultural output, though about 2 billion people worldwide still depend on subsistence agriculture.Modern agronomy, plant breeding, agrochemicals such as pesticides and fertilizers, and technological developments have sharply increased yields from cultivation, but at the same time have caused widespread ecological damage. Selective breeding and modern practices in animal husbandry have similarly increased the output of meat, but have raised concerns about animal welfare and environmental damage through contributions to global warming, depletion of aquifers, deforestation, antibiotic resistance, and growth hormones in industrially produced meat. Genetically modified organisms are widely used, although they are banned in several countries.The major agricultural products can be broadly grouped into foods, fibers, fuels, and raw materials (such as rubber). Classes of foods include cereals (grains), vegetables, fruits, oils, meat, milk and eggs. Over one third of the world's workers are employed in agriculture, second only to the service sector, although the number of agricultural workers in developed countries has decreased significantly over the past several centuries.The word agriculture is a late Middle English adaptation of Latin agricultūra, from ager, ""field"", which in its turn came from Greek αγρός, and cultūra, ""cultivation"" or ""growing"". Agriculture usually refers to human activities, although it is also observed in certain species of ant, termite and ambrosia beetle. To practice agriculture means to use natural resources to ""produce commodities which maintain life, including food, fiber, forest products, horticultural crops, and their related services."" This definition includes arable farming or agronomy, and horticulture, all terms for the growing of plants, animal husbandry and sometimes forestry.The development of agriculture enabled the human population to grow many times larger than could be sustained by hunting and gathering. Agriculture began independently in different parts of the globe, and included a diverse range of taxa. At least 11 separate regions of the Old and New World were involved as independent centers of origin. Wild grains were collected and eaten from at least 105,000 years ago. Rye was cultivated by at least 11,050 BC. From around 11,500 years ago, the eight Neolithic founder crops, emmer and einkorn wheat, hulled barley, peas, lentils, bitter vetch, chick peas and flax were cultivated in the Levant. Rice was domesticated in China between 11,500 and 6,200 BC with earliest known cultivation from 5,700 BC, followed by mung, soy and azuki beans. Sheep were domesticated in Mesopotamia between 13,000 and 11,000 years ago. Cattle were domesticated from the wild aurochs in the areas of modern Turkey and Pakistan some 10,500 years ago. Domestic pigs had multiple centres of origin in Eurasia, including Europe, East Asia and Southwest Asia, where wild boar were first domesticated about 10,500 years ago. In the Andes of South America, the potato was domesticated between 10,000 and 7,000 years ago, along with beans, coca, llamas, alpacas, and guinea pigs. Sugarcane and some root vegetables were domesticated in New Guinea around 9,000 years ago. Sorghum was domesticated in the Sahel region of Africa by 7,000 years ago. Cotton was domesticated in Peru by 5,600 years ago, and was independently domesticated in Eurasia. In Mesoamerica, wild teosinte was domesticated to maize by 6,000 years ago.Scholars have developed a number of hypotheses to explain the historical origins of agriculture. Studies of the transition from hunter-gatherer to agricultural societies indicate an initial period of intensification and increasing sedentism examples are the Natufian culture in the Levant, and the Early Chinese Neolithic in China. Then, wild stands that had previously been harvested started to be planted, and gradually came to be domesticated.In Eurasia, the Sumerians started to live in villages from about 8,000 BC, relying on the Tigris and Euphrates rivers and a canal system for irrigation. Ploughs appear in pictographs around 3,000 BC seed-ploughs around 2,300 BC. Farmers grew wheat, barley, vegetables such as lentils and onions, and fruits including dates, grapes, and figs. Ancient Egyptian agriculture relied on the  Nile River and its seasonal flooding. Farming started in the predynastic period at the end of the Paleolithic, after 10,000 BC. Staple food crops were grains such as wheat and barley, alongside industrial crops such as flax and papyrus. In India, wheat, barley, and jujube were domesticated by 9,000 BC, soon followed by sheep and goats. Cattle, sheep and goats were domesticated in Mehrgarh culture by 8,000–6,000 BC. Cotton was cultivated by the 5th-4th millennium BC. There archeological evidence of an animal-drawn plough from 2,500 BC in the Indus Valley Civilization.In China, from the 5th century BC there was a nationwide granary system and widespread silk farming. Water-powered grain mills were in use by  the 1st century BC, followed by irrigation. By the late 2nd century, heavy ploughs had been developed with iron ploughshares and mouldboards. These slowly spread westwards across Eurasia. Asian rice was domesticated 8,200–13,500 years ago in China, with a single genetic origin from the wild rice Oryza rufipogon. In ancient Greece and Rome, the major cereals were wheat, emmer, and barley, alongside vegetables including peas, beans, and olives. Sheep and goats were kept mainly for dairy products.In the Americas, crops domesticated in Mesoamerica (apart from teosinte) include squash, beans, and cocoa. The turkey was probably domesticated in Mexico or the American Southwest. The Aztecs developed irrigation systems, formed terraced hillsides, fertilized their soil, and developed chinampas or artificial islands. The Mayas used extensive canal and raised field systems to farm swampland from 400 BC. Coca was domesticated in the Andes, as were the peanut, tomato, tobacco, and pineapple. Cotton was domesticated in Peru by 3,600 BC. Animals, too, including llamas, alpacas, and guinea pigs were domesticated in the region. In North America, the indigenous people of the East domesticated crops such as sunflower, tobacco, squash and Chenopodium. Wild foods including wild rice and maple sugar were harvested. The domesticated strawberry is a hybrid of a Chilean and a North American species, developed by breeding in Europe and North America. The indigenous people of the Southwest and the Pacific Northwest practiced forest gardening and fire-stick farming. The natives controlled fire on a regional scale to create a low-intensity fire ecology which  sustained a low-density agriculture in loose rotation a sort of ""wild"" permaculture. A system of companion planting called the Three Sisters was developed on the Great Plains, the three crops being winter squash, maize, and climbing beans.Indigenous Australians, long supposed to have been nomadic hunter-gatherers, practised systematic burning to enhance natural productivity in fire-stick farming. The Gunditjmara and other groups developed eel farming and fish trapping systems from some 5,000 years ago. There is evidence of 'intensification' across the whole continent over that period. In two regions of Australia, the central west coast and eastern central Australia, early agriculture with crops of yams, native millet, and bush onions may have been practised in permanent settlements.In the Middle Ages, both in the Islamic world and in Europe, agriculture was transformed with improved techniques and the diffusion of crop plants, including the introduction of sugar, rice, cotton and fruit trees such as the orange to Europe by way of Al-Andalus. After 1492, the Columbian exchange brought New World crops such as maize, potatoes, tomatoes, sweet potatoes and manioc to Europe, and Old World crops such as wheat, barley, rice and turnips, and livestock including horses, cattle, sheep and goats to the Americas.Irrigation, crop rotation, and fertilizers were greatly developed in the past 200 years, starting with the British Agricultural Revolution, allowing global population to rise significantly. Since 1900, agriculture in the developed nations, and to a lesser extent in the developing world, has seen large rises in productivity as human labor has been replaced by mechanization, and assisted by synthetic fertilizers, pesticides, and selective breeding. The Haber-Bosch method allowed the synthesis of ammonium nitrate fertilizer on an industrial scale, greatly increasing crop yields and sustaining a further increase in global population. Modern agriculture has raised political issues including water pollution, biofuels, genetically modified organisms, tariffs and farm subsidies, leading to alternative approaches such as the organic movement.Pastoralism involves managing domesticated animals. In nomadic pastoralism, herds of livestock are moved from place to place in search of pasture, fodder, and water. This type of farming is practised in arid and semi-arid regions of Sahara, Central Asia and some parts of India.In shifting cultivation, a small area of a forest is cleared by cutting down all the trees and the area is burned. The land is then used for growing crops for several years. When the soil becomes less fertile, the area is then abandoned. Another patch of land is selected and the process is repeated. This type of farming is practiced mainly in areas with abundant rainfall where the forest regenerates quickly. This practice is used in Northeast India, Southeast Asia, and the Amazon Basin.Subsistence farming is practiced to satisfy family or local needs alone, with little left over for transport elsewhere. It is intensively practiced in Monsoon Asia and South-East Asia. If the typical subsistence farmer is equivalent to a smallholder, then there are an estimated 2.5 billion such farmers in 2018, cultivating about 60% of the earth's arable land.In intensive farming, the crops are cultivated to maximise profit, with a low fallow ratio and a high use of inputs. This type of farming is practiced mainly in highly developed countries.In the past century, agriculture has been characterized by increased productivity, the substitution of synthetic fertilizers and pesticides for labor, water pollution, and farm subsidies. In recent years there has been a backlash against the environmental effects of conventional agriculture, resulting in the organic, regenerative, and sustainable agriculture movements. One of the major forces behind this movement has been the European Union, which first certified organic food in 1991 and began reform of its Common Agricultural Policy (CAP) in 2005 to phase out commodity-linked farm subsidies, also known as decoupling. The growth of organic farming has renewed research in alternative technologies such as integrated pest management and selective breeding. Recent mainstream technological developments include genetically modified food. Demand for non-food biofuel crops, development of former farm lands, rising transportation costs, climate change, growing consumer demand in China and India, and population growth, are threatening food security in many parts of the world. The International Fund for Agricultural Development posits that an increase in smallholder agriculture may be part of the solution to concerns about food prices and overall food security, given the favorable experience of Vietnam. Soil degradation and diseases such as stem rust are major concerns globally approximately 40% of the world's agricultural land is seriously degraded. By 2015, the agricultural output of China was the largest in the world, followed by the European Union, India and the United States. Economists measure the total factor productivity of agriculture and by this measure agriculture in the United States is roughly 1.7 times more productive than it was in 1948.Following the three-sector theory, the number of people employed in agriculture and other primary activities (such as fishing) can be more than 80% in the least developed countries, and less than 2% in the most highly developed countries. Since the Industrial Revolution, many countries have made the transition to developed economies, and the proportion of people working in agriculture has steadily fallen. During the 16th century in Europe, for example, between 55 and 75% of the population was engaged in agriculture by the 19th century, this had dropped to between 35 and 65%. In the same countries today, the figure is less than 10%. At the start of the 21st century, some one billion people, or over 1/3 of the available work force, were employed in agriculture. It constitutes approximately 70% of the global employment of children, and in many countries employs the largest percentage of women of any industry. The service sector overtook the agricultural sector as the largest global employer in 2007.Agriculture, specifically farming, remains a hazardous industry, and farmers worldwide remain at high risk of work-related injuries, lung disease, noise-induced hearing loss, skin diseases, as well as certain cancers related to chemical use and prolonged sun exposure. On industrialized farms, injuries frequently involve the use of agricultural machinery, and a common cause of fatal agricultural injuries in developed countries is tractor rollovers. Pesticides and other chemicals used in farming can also be hazardous to worker health, and workers exposed to pesticides may experience illness or have children with birth defects. As an industry in which families commonly share in work and live on the farm itself, entire families can be at risk for injuries, illness, and death. Ages 0–6 may be an especially vulnerable population in agriculture common causes of fatal injuries among young farm workers include drowning, machinery and motor accidents, including with all-terrain vehicles.The International Labour Organization considers agriculture ""one of the most hazardous of all economic sectors."" It estimates that the annual work-related death toll among agricultural employees is at least 170,000, twice the average rate of other jobs. In addition, incidences of death, injury and illness related to agricultural activities often go unreported. The organization has developed the Safety and Health in Agriculture Convention, 2001, which covers the range of risks in the agriculture occupation, the prevention of these risks and the role that individuals and organizations engaged in agriculture should play.In America, agriculture has been identified by the National Institute for Occupational Safety and Health as a priority industry sector in the National Occupational Research Agenda to identify and provide intervention strategies for occupational health and safety issues.In the European Union, the European Agency for Safety and Health at Work has issued guidelines on implementing health and safety directives in agriculture, livestock farming, horticulture, and forestry.Overall production varies by country as listed.Cropping systems vary among farms depending on the available resources and constraints geography and climate of the farm government policy economic, social and political pressures and the philosophy and culture of the farmer.Shifting cultivation (or slash and burn) is a system in which forests are burnt, releasing nutrients to support cultivation of annual and then perennial crops for a period of several years. Then the plot is left fallow to regrow forest, and the farmer moves to a new plot, returning after many more years (10–20). This fallow period is shortened if population density grows, requiring the input of nutrients (fertilizer or manure) and some manual pest control. Annual cultivation is the next phase of intensity in which there is no fallow period. This requires even greater nutrient and pest control inputs.Further industrialization led to the use of monocultures, when one cultivar is planted on a large acreage. Because of the low biodiversity, nutrient use is uniform and pests tend to build up, necessitating the greater use of pesticides and fertilizers. Multiple cropping, in which several crops are grown sequentially in one year, and intercropping, when several crops are grown at the same time, are other kinds of annual cropping systems known as polycultures.In subtropical and arid environments, the timing and extent of agriculture may be limited by rainfall, either not allowing multiple annual crops in a year, or requiring irrigation. In all of these environments perennial crops are grown (coffee, chocolate) and systems are practiced such as agroforestry. In temperate environments, where ecosystems were predominantly grassland or prairie, highly productive annual farming is the dominant agricultural system.Important categories of food crops include cereals, legumes, forage, fruits and vegetables. Natural fibers include cotton, wool, hemp, silk and flax. Specific crops are cultivated in distinct growing regions throughout the world. Production is listed in millions of metric tons, based on FAO estimates.Animal husbandry is the breeding and raising of animals for meat, milk, eggs, or wool), and for work and transport. Working animals, including horses, mules, oxen, water buffalo, camels, llamas, alpacas, donkeys, and dogs, have for centuries been used to help cultivate fields, harvest crops, wrangle other animals, and transport farm products to buyers.Livestock production systems can be defined based on feed source, as grassland-based, mixed, and landless. As of  2010, 30% of Earth's ice- and water-free area was used for producing livestock, with the sector employing approximately 1.3 billion people. Between the 1960s and the 2000s, there was a significant increase in livestock production, both by numbers and by carcass weight, especially among beef, pigs and chickens, the latter of which had production increased by almost a factor of 10. Non-meat animals, such as milk cows and egg-producing chickens, also showed significant production increases. Global cattle, sheep and goat populations are expected to continue to increase sharply through 2050. Aquaculture or fish farming, the production of fish for human consumption in confined operations, is one of the fastest growing sectors of food production, growing at an average of 9% a year between 1975 and 2007.During the second half of the 20th century, producers using selective breeding focused on creating livestock breeds and crossbreeds that increased production, while mostly disregarding the need to preserve genetic diversity. This trend has led to a significant decrease in genetic diversity and resources among livestock breeds, leading to a corresponding decrease in disease resistance and local adaptations previously found among traditional breeds.Grassland based livestock production relies upon plant material such as shrubland, rangeland, and pastures for feeding ruminant animals. Outside nutrient inputs may be used, however manure is returned directly to the grassland as a major nutrient source. This system is particularly important in areas where crop production is not feasible because of climate or soil, representing 30–40 million pastoralists. Mixed production systems use grassland, fodder crops and grain feed crops as feed for ruminant and monogastric (one stomach mainly chickens and pigs) livestock. Manure is typically recycled in mixed systems as a fertilizer for crops.Landless systems rely upon feed from outside the farm, representing the de-linking of crop and livestock production found more prevalently in Organisation for Economic Co-operation and Development member countries. Synthetic fertilizers are more heavily relied upon for crop production and manure utilization becomes a challenge as well as a source for pollution. Industrialized countries use these operations to produce much of the global supplies of poultry and pork. Scientists estimate that 75% of the growth in livestock production between 2003 and 2030 will be in confined animal feeding operations, sometimes called factory farming. Much of this growth is happening in developing countries in Asia, with much smaller amounts of growth in Africa. Some of the practices used in commercial livestock production, including the usage of growth hormones, are controversial.Tillage is the practice of breaking up the soil with tools such as the plow or harrow to prepare for planting, for nutrient incorporation, or for pest control. Tillage varies in intensity from conventional to no-till. It may improve productivity by warming the soil, incorporating fertilizer and controlling weeds, but also renders soil more prone to erosion, triggers the decomposition of organic matter releasing CO2, and reduces the abundance and diversity of soil organisms.Pest control includes the management of weeds, insects, mites, and diseases. Chemical (pesticides), biological (biocontrol), mechanical (tillage), and cultural practices are used. Cultural practices include crop rotation, culling, cover crops, intercropping, composting, avoidance, and resistance. Integrated pest management attempts to use all of these methods to keep pest populations below the number which would cause economic loss, and recommends pesticides as a last resort.Nutrient management includes both the source of nutrient inputs for crop and livestock production, and the method of utilization of manure produced by livestock. Nutrient inputs can be chemical inorganic fertilizers, manure, green manure, compost and minerals. Crop nutrient use may also be managed using cultural techniques such as crop rotation or a fallow period. Manure is used either by holding livestock where the feed crop is growing, such as in managed intensive rotational grazing, or by spreading either dry or liquid formulations of manure on cropland or pastures.Water management is needed where rainfall is insufficient or variable, which occurs to some degree in most regions of the world. Some farmers use irrigation to supplement rainfall. In other areas such as the Great Plains in the U.S. and Canada, farmers use a fallow year to conserve soil moisture to use for growing a crop in the following year. Agriculture represents 70% of freshwater use worldwide.According to a report by the International Food Policy Research Institute, agricultural technologies will have the greatest impact on food production if adopted in combination with each other using a model that assessed how eleven technologies could impact agricultural productivity, food security and trade by 2050, the International Food Policy Research Institute found that the number of people at risk from hunger could be reduced by as much as 40% and food prices could be reduced by almost half.Payment for ecosystem services is a method of providing additional incentives to encourage farmers to conserve some aspects of the environment. Measures might include paying for reforestation upstream of a city, to improve the supply of fresh water.Crop alteration has been practiced by humankind for thousands of years, since the beginning of civilization. Altering crops through breeding practices changes the genetic make-up of a plant to develop crops with more beneficial characteristics for humans, for example, larger fruits or seeds, drought-tolerance, or resistance to pests. Significant advances in plant breeding ensued after the work of geneticist Gregor Mendel. His work on dominant and recessive alleles, although initially largely ignored for almost 50 years, gave plant breeders a better understanding of genetics and breeding techniques. Crop breeding includes techniques such as plant selection with desirable traits, self-pollination and cross-pollination, and molecular techniques that genetically modify the organism.Domestication of plants has, over the centuries increased yield, improved disease resistance and drought tolerance, eased harvest and improved the taste and nutritional value of crop plants. Careful selection and breeding have had enormous effects on the characteristics of crop plants. Plant selection and breeding in the 1920s and 1930s improved pasture (grasses and clover) in New Zealand. Extensive X-ray and ultraviolet induced mutagenesis efforts (i.e. primitive genetic engineering) during the 1950s produced the modern commercial varieties of grains such as wheat, corn (maize) and barley.The Green Revolution popularized the use of conventional hybridization to sharply increase yield by creating ""high-yielding varieties"". For example, average yields of corn (maize) in the US have increased from around 2.5 tons per hectare (t/ha) (40 bushels per acre) in 1900 to about 9.4 t/ha (150 bushels per acre) in 2001. Similarly, worldwide average wheat yields have increased from less than 1 t/ha in 1900 to more than 2.5 t/ha in 1990. South American average wheat yields are around 2 t/ha, African under 1 t/ha, and Egypt and Arabia up to 3.5 to 4 t/ha with irrigation. In contrast, the average wheat yield in countries such as France is over 8 t/ha. Variations in yields are due mainly to variation in climate, genetics, and the level of intensive farming techniques (use of fertilizers, chemical pest control, growth control to avoid lodging).Genetically modified organisms (GMO) are organisms whose genetic material has been altered by genetic engineering techniques generally known as recombinant DNA technology. Genetic engineering has expanded the genes available to breeders to utilize in creating desired germlines for new crops. Increased durability, nutritional content, insect and virus resistance and herbicide tolerance are a few of the attributes bred into crops through genetic engineering. For some, GMO crops cause food safety and food labeling concerns. Numerous countries have placed restrictions on the production, import or use of GMO foods and crops. Currently a global treaty, the Biosafety Protocol, regulates the trade of GMOs. There is ongoing discussion regarding the labeling of foods made from GMOs, and while the EU currently requires all GMO foods to be labeled, the US does not.Herbicide-resistant seed has a gene implanted into its genome that allows the plants to tolerate exposure to herbicides, including glyphosate. These seeds allow the farmer to grow a crop that can be sprayed with herbicides to control weeds without harming the resistant crop. Herbicide-tolerant crops are used by farmers worldwide. With the increasing use of herbicide-tolerant crops, comes an increase in the use of glyphosate-based herbicide sprays. In some areas glyphosate resistant weeds have developed, causing farmers to switch to other herbicides. Some studies also link widespread glyphosate usage to iron deficiencies in some crops, which is both a crop production and a nutritional quality concern, with potential economic and health implications.Other GMO crops used by growers include insect-resistant crops, which have a gene from the soil bacterium Bacillus thuringiensis (Bt), which produces a toxin specific to insects. These crops resist damage by insects. Some believe that similar or better pest-resistance traits can be acquired through traditional breeding practices, and resistance to various pests can be gained through hybridization or cross-pollination with wild species. In some cases, wild species are the primary source of resistance traits some tomato cultivars that have gained resistance to at least 19 diseases did so through crossing with wild populations of tomatoes.Agriculture imposes multiple external costs upon society through effects such as pesticide damage to nature (especially herbicides and insecticides), nutrient runoff, excessive water usage, and loss of natural environment. A 2000 assessment of agriculture in the UK determined total external costs for 1996 of £2,343 million, or £208 per hectare. A 2005 analysis of these costs in the US concluded that cropland imposes approximately $5 to $16 billion ($30 to $96 per hectare), while livestock production imposes $714 million. Both studies, which focused solely on the fiscal impacts, concluded that more should be done to internalize external costs. Neither included subsidies in their analysis, but they noted that subsidies also influence the cost of agriculture to society.Agriculture seeks to increase yield and to reduce costs. Yield increases with inputs such as fertilisers and removal of pathogens, predators, and competitors (such as weeds). Costs decrease with increasing scale of farm units, such as making fields larger this means removing hedges, ditches and other areas of habitat. Pesticides kill insects, plants and fungi. These and other measures have cut biodiversity to very low levels on intensively farmed land.In 2010, the International Resource Panel of the United Nations Environment Programme assessed the environmental impacts of consumption and production. It found that agriculture and food consumption are two of the most important drivers of environmental pressures, particularly habitat change, climate change, water use and toxic emissions. Agriculture is the main source of toxins released into the environment, including insecticides, especially those used on cotton. The 2011 UNEP Green Economy report states that ""[a]gricultural operations, excluding land use changes, produce approximately 13 per cent of anthropogenic global GHG emissions. This includes GHGs emitted by the use of inorganic fertilisers agro-chemical pesticides and herbicides (GHG emissions resulting from production of these inputs are included in industrial emissions) and fossil fuel-energy inputs. ""On average we find that the total amount of fresh residues from agricultural and forestry production for second- generation biofuel production amounts to 3.8 billion tonnes per year between 2011 and 2050 (with an average annual growth rate of 11 per cent throughout the period analysed, accounting for higher growth during early years, 48 per cent for 2011–2020 and an average 2 per cent annual expansion after 2020).""A senior UN official and co-author of a UN report detailing this problem, Henning Steinfeld, said ""Livestock are one of the most significant contributors to today's most serious environmental problems"". Livestock production occupies 70% of all land used for agriculture, or 30% of the land surface of the planet. It is one of the largest sources of greenhouse gases, responsible for 18% of the world's greenhouse gas emissions as measured in CO2 equivalents. By comparison, all transportation emits 13.5% of the CO2. It produces 65% of human-related nitrous oxide (which has 296 times the global warming potential of CO2,) and 37% of all human-induced methane (which is 23 times as warming as CO2.) It also generates 64% of the ammonia emission. Livestock expansion is cited as a key factor driving deforestation in the Amazon basin 70% of previously forested area is now occupied by pastures and the remainder used for feedcrops. Through deforestation and land degradation, livestock is also driving reductions in biodiversity. Furthermore, the UNEP states that ""methane emissions from global livestock are projected to increase by 60 per cent by 2030 under current practices and consumption patterns.""Land transformation, the use of land to yield goods and services, is the most substantial way humans alter the Earth's ecosystems, and is considered the driving force in the loss of biodiversity. Estimates of the amount of land transformed by humans vary from 39 to 50%. Land degradation, the long-term decline in ecosystem function and productivity, is estimated to be occurring on 24% of land worldwide, with cropland overrepresented. The UN-FAO report cites land management as the driving factor behind degradation and reports that 1.5 billion people rely upon the degrading land. Degradation can be deforestation, desertification, soil erosion, mineral depletion, or chemical degradation (acidification and salinization).Eutrophication, excessive nutrients in aquatic ecosystems resulting in algal blooms and anoxia, leads to fish kills, loss of biodiversity, and renders water unfit for drinking and other industrial uses. Excessive fertilization and manure application to cropland, as well as high livestock stocking densities cause nutrient (mainly nitrogen and phosphorus) runoff and leaching from agricultural land. These nutrients are major nonpoint pollutants contributing to eutrophication of aquatic ecosystems and pollution of groundwater, with harmful effects on human populations. Fertilisers also reduce terrestrial biodiversity by increasing competition for light, favouring those species that are able to benefit from the added nutrients.Agriculture accounts for 70 percent of withdrawals of freshwater resources. Agriculture is a major draw on water from aquifers, and currently draws from those underground water sources at an unsustainable rate. It is long known that aquifers in areas as diverse as northern China, the Upper Ganges and the western US are being depleted, and new research extends these problems to aquifers in Iran, Mexico and Saudi Arabia. Increasing pressure is being placed on water resources by industry and urban areas, meaning that water scarcity is increasing and agriculture is facing the challenge of producing more food for the world's growing population with reduced water resources. Agricultural water usage can also cause major environmental problems, including the destruction of natural wetlands, the spread of water-borne diseases, and land degradation through salinization and waterlogging, when irrigation is performed incorrectly.Pesticide use has increased since 1950 to 2.5 million short tons annually worldwide, yet crop loss from pests has remained relatively constant. The World Health Organization estimated in 1992 that three million pesticide poisonings occur annually, causing 220,000 deaths. Pesticides select for pesticide resistance in the pest population, leading to a condition termed the ""pesticide treadmill"" in which pest resistance warrants the development of a new pesticide.An alternative argument is that the way to ""save the environment"" and prevent famine is by using pesticides and intensive high yield farming, a view exemplified by a quote heading the Center for Global Food Issues website: 'Growing more per acre leaves more land for nature'. However, critics argue that a trade-off between the environment and a need for food is not inevitable, and that pesticides simply replace good agronomic practices such as crop rotation. The Push–pull agricultural pest management technique involves intercropping, using plant aromas to repel pests from crops (push) and to lure them to a place from which they can then be removed (pull).Global warming and agriculture are interrelated on a global scale. Global warming affects agriculture through changes in average temperatures, rainfall, and weather extremes (like storms and heat waves) changes in pests and diseases changes in atmospheric carbon dioxide and ground-level ozone concentrations changes in the nutritional quality of some foods and changes in sea level. Global warming is already affecting agriculture, with effects unevenly distributed across the world. Future climate change will probably negatively affect crop production in low latitude countries, while effects in northern latitudes may be positive or negative. Global warming will probably increase the risk of food insecurity for some vulnerable groups, such as the poor.Animal husbandry is also responsible for greenhouse gas production of CO2 and a percentage of the world's methane, and future land infertility, and the displacement of wildlife. Agriculture contributes to climate change by anthropogenic emissions of greenhouse gases, and by the conversion of non-agricultural land such as forest for agricultural use. Agriculture, forestry and land-use change contributed around 20 to 25% to global annual emissions in 2010. A range of policies can reduce the risk of negative climate change impacts on agriculture, and greenhouse gas emissions from the agriculture sector.Current farming methods have resulted in over-stretched water resources, high levels of erosion and reduced soil fertility. There is not enough water to continue farming using current practices therefore how critical water, land, and ecosystem resources are used to boost crop yields must be reconsidered. A solution would be to give value to ecosystems, recognizing environmental and livelihood tradeoffs, and balancing the rights of a variety of users and interests. Inequities that result when such measures are adopted would need to be addressed, such as the reallocation of water from poor to rich, the clearing of land to make way for more productive farmland, or the preservation of a wetland system that limits fishing rights.Technological advancements help provide farmers with tools and resources to make farming more sustainable. Technology permits innovations like conservation tillage, a farming process which helps prevent land loss to erosion, reduces water pollution, and enhances carbon sequestration.According to a report by the International Food Policy Research Institute (IFPRI), agricultural technologies will have the greatest impact on food production if adopted in combination with each other using a model that assessed how eleven technologies could impact agricultural productivity, food security and trade by 2050, IFPRI found that the number of people at risk from hunger could be reduced by as much as 40% and food prices could be reduced by almost half. The caloric demand of Earth's projected population, with current climate change predictions, can be satisfied by additional improvement of agricultural methods, expansion of agricultural areas, and a sustainability-oriented consumer mindset.Since the 1940s, agricultural productivity has increased dramatically, due largely to the increased use of energy-intensive mechanization, fertilizers and pesticides. The vast majority of this energy input comes from fossil fuel sources. Between the 1960s and the 1980s, the Green Revolution transformed agriculture around the globe, with world grain production increasing significantly (between 70% and 390% for wheat and 60% to 150% for rice, depending on geographic area) as world population doubled. Heavy reliance on petrochemicals has raised concerns that oil shortages could increase costs and reduce agricultural output.Industrialized agriculture depends on fossil fuels in two fundamental ways: direct consumption on the farm and manufacture of inputs used on the farm. Direct consumption includes the use of lubricants and fuels to operate farm vehicles and machinery.Indirect consumption includes the manufacture of fertilizers, pesticides, and farm machinery. In particular, the production of nitrogen fertilizer can account for over half of agricultural energy usage. Together, direct and indirect consumption by US farms accounts for about 2% of the nation's energy use. Direct and indirect energy consumption by U.S. farms peaked in 1979, and has since gradually declined. Food systems encompass not just agriculture but off-farm processing, packaging, transporting, marketing, consumption, and disposal of food and food-related items. Agriculture accounts for less than one-fifth of food system energy use in the US.Agricultural economics refers to economics as it relates to the ""production, distribution and consumption of [agricultural] goods and services"". Combining agricultural production with general theories of marketing and business as a discipline of study began in the late 1800s, and grew significantly through the 20th century. Although the study of agricultural economics is relatively recent, major trends in agriculture have significantly affected national and international economies throughout history, ranging from tenant farmers and sharecropping in the post-American Civil War Southern United States to the European feudal system of manorialism. In the United States, and elsewhere, food costs attributed to food processing, distribution, and agricultural marketing, sometimes referred to as the value chain, have risen while the costs attributed to farming have declined. This is related to the greater efficiency of farming, combined with the increased level of value addition (e.g. more highly processed products) provided by the supply chain. Market concentration has increased in the sector as well, and although the total effect of the increased market concentration is likely increased efficiency, the changes redistribute economic surplus from producers (farmers) and consumers, and may have negative implications for rural communities.National government policies can significantly change the economic marketplace for agricultural products, in the form of taxation, subsidies, tariffs and other measures. Since at least the 1960s, a combination of trade restrictions, exchange rate policies and subsidies have affected farmers in both the developing and the developed world. In the 1980s, non-subsidized farmers in developing countries experienced adverse effects from national policies that created artificially low global prices for farm products. Between the mid-1980s and the early 2000s, several international agreements limited agricultural tariffs, subsidies and other trade restrictions.However, as of  2009, there was still a significant amount of policy-driven distortion in global agricultural product prices. The three agricultural products with the greatest amount of trade distortion were sugar, milk and rice, mainly due to taxation. Among the oilseeds, sesame had the greatest amount of taxation, but overall, feed grains and oilseeds had much lower levels of taxation than livestock products. Since the 1980s, policy-driven distortions have seen a greater decrease among livestock products than crops during the worldwide reforms in agricultural policy. Despite this progress, certain crops, such as cotton, still see subsidies in developed countries artificially deflating global prices, causing hardship in developing countries with non-subsidized farmers. Unprocessed commodities such as corn, soybeans, and cattle are generally graded to indicate quality, affecting the price the producer receives. Commodities are generally reported by production quantities, such as volume, number or weight.Agricultural science is a broad multidisciplinary field of biology that encompasses the parts of exact, natural, economic and social sciences used in the practice and understanding of agriculture. It covers topics such as agronomy, plant breeding and genetics, plant pathology, crop modelling, soil science, entomology, production techniques and improvement, study of pests and their management, and study of adverse environmental effects such as soil degradation, waste management, and bioremediation.The scientific study of agriculture began in the 18th century, when Johann Friedrich Mayer conducted experiments on the use of gypsum (hydrated calcium sulphate) as a fertilizer. Research became more systematic when in 1843, John Lawes and Henry Gilbert began a set of long-term agronomy field experiments at Rothamsted Research Station in England some of them, such as the Park Grass Experiment, are still running. In America, the Hatch Act of 1887 provided funding for what it was the first to call ""agricultural science"", driven by farmers' interest in fertilizers. In agricultural entomology, the USDA began to research biological control in 1881 it instituted its first large program in 1905, searching Europe and Japan for natural enemies of the gypsy moth and brown-tail moth, establishing parasitoids (such as solitary wasps) and predators of both pests in the USA.Agricultural policy is the set of government decisions and actions relating to domestic agriculture and imports of foreign agricultural products. Governments usually implement agricultural policies with the goal of achieving a specific outcome in the domestic agricultural product markets. Some overarching themes include risk management and adjustment (including policies related to climate change, food safety and natural disasters), economic stability (including policies related to taxes), natural resources and environmental sustainability (especially water policy), research and development, and market access for domestic commodities (including relations with global organizations and agreements with other countries). Agricultural policy can also touch on food quality, ensuring that the food supply is of a consistent and known quality, food security, ensuring that the food supply meets the population's needs, and conservation. Policy programs can range from financial programs, such as subsidies, to encouraging producers to enroll in voluntary quality assurance programs.There are many influences on the creation of agricultural policy, including consumers, agribusiness, trade lobbies and other groups. Agribusiness interests hold a large amount of influence over policy making, in the form of lobbying and campaign contributions. Political action groups, including those interested in environmental issues and labor unions, also provide influence, as do lobbying organizations representing individual agricultural commodities. The Food and Agriculture Organization of the United Nations (FAO) leads international efforts to defeat hunger and provides a forum for the negotiation of global agricultural regulations and agreements. Dr. Samuel Jutzi, director of FAO's animal production and health division, states that lobbying by large corporations has stopped reforms that would improve human health and the environment. For example, proposals in 2010 for a voluntary code of conduct for the livestock industry that would have provided incentives for improving standards for health, and environmental regulations, such as the number of animals an area of land can support without long-term damage, were successfully defeated due to large food company pressure.Food and Agriculture OrganizationUnited States Department of AgricultureAgriculture material from the World Bank Group""Agriculture collected news and commentary"". The New York Times. ""Agriculture collected news and commentary"". The Guardian.";environmental disaster;Agriculture;0
"The oil tanker Amoco Cadiz ran aground on Portsall Rocks, 5 km (3.1 mi) from the coast of Brittany, France, on 16 March 1978, and ultimately split in three and sank, all together resulting in the largest oil spill of its kind in history to that date.NOAA estimates that the total oil spill amounted to 220,880 metric tonnes of oil.En route from the Persian Gulf to Rotterdam, Netherlands, via a scheduled stop at Lyme Bay, Great Britain, the ship encountered stormy weather with gale conditions and high seas while in the English Channel. At around 09:45, a heavy wave slammed into the ship's rudder and it was found that she was no longer responding to the helm. This was due to the shearing of Whitworth thread studs in the Hastie four ram steering gear, built under licence in Spain, causing a loss of hydraulic fluid. Attempts to repair the damage and regain control of the ship were made but proved unsuccessful. While the message ""no longer manoeuvrable"" and asking other vessels to stand by was transmitted at 10:20, no call for tug assistance was issued until 11:20.The German tug Pacific responded to Amoco Cadiz at 11:28, offering assistance under a Lloyd's Open Form (see below). It arrived on the scene at 12:20, but because of the stormy sea, a tow line was not in place until 14:00, and broke off at 16:15. Several attempts were made to establish another tow line and Amoco Cadiz dropped its anchor trying to halt its drift. A successful tow line was in place at 20:55, but this measure proved incapable of preventing the supertanker from drifting towards the coast because of its huge mass and Force 10 storm winds.At 21:04 Amoco Cadiz ran aground the first time, flooding its engines, and again at 21:39, this time ripping open the hull and starting the oil spill. Her crew was rescued by French Naval Aviation helicopters at midnight, and her captain and one officer remained aboard until 05:00 the next morning.At 10:00 on 17 March the vessel broke in two, releasing its entire cargo of 1.6 million barrels (250,000 m3) of oil, and broke again eleven days later from the buffeting of high stormy seas. The wreckage was later completely destroyed with depth charges by the French Navy.An argument arose between the captain of Amoco Cadiz, Pasquale Bardari, and that of the captain of the German tug Pacific, Hartmut Weinert, on the issue of Lloyd's Open Form (LOF).  Captain Weinert thought this a classic LOF case, an oil tanker with damage to its steering gear, rough weather and getting closer to the shore by the minute.At the time of the accident, the ship and the cargo were valued at about US$40 million, so Captain Weinert's company could, in the event of success, have received a large award.  Captain Bardari of the Cadiz, on the instructions of his owners, wanted ""....towage rate to Lyme Bay.""The argument dragged on from 11:28 when Pacific first made contact with Amoco Cadiz until 16:00 when Captain Bardari finally received approval to accept the LOF from the ship's owners in Chicago. However, this dispute did not delay the salvage operation significantly, because tugging preparations had already started. Captain Weinert was aware that if he were to succeed in bringing the tanker into Lyme Bay on the English coast, his owners could arrest the ship in the English High Court in pursuit of a claim for salvage.It was incorrectly reported in the press at the time that, after long negotiations on financial terms between the ship's captain and the master of a West German tugboat and two unsuccessful towing attempts, the towline finally broke during the argument and the ship drifted onto the rocks. This version of events became fixed in the public mind although in fact delay was caused by Captain Bardari of Amoco Cadiz contacting her owners in Chicago for instructions. The delay in sending a distress message meant that the larger tug Seefalke, which might have been in range an hour earlier, was no longer nearby when the distress call was made.Amoco Cadiz contained 1,604,500 barrels (219,797 tons) of light crude oil from Ras Tanura, Saudi Arabia and Kharg Island, Iran. Additionally, she had nearly 4,000 tonnes bunker oil. Severe weather resulted in the complete breakup of the ship before any oil could be pumped out of the wreck, resulting in its entire cargo of crude oil (belonging to Shell) and 4,000 tons of fuel oil being spilled into the sea.A 12 mi (19 km) long slick and heavy pools of oil spread onto 45 mi (72 km) of the French shoreline by northwesterly winds. Prevailing westerly winds during the following month spread the oil approximately 100 mi (160 km) east along the coast. One week after the accident, oil had reached Côtes d'Armor.Oil penetrated the sand on several beaches to a depth of 20 inches (500 mm). Sub-surface oil separated into two or three layers due to the extensive sand transfer that occurred on the beaches during rough weather. Piers and slips in the small harbors from Porspoder to Brehat Island were covered with oil. Other affected areas included the pink granite rock beaches of Trégastel and Perros-Guirec, as well as the tourist beaches at Plougasnou. The total extent of oiling one month after the spill included approximately 200 miles (320 km) of coastline. Beaches of 76 different Breton communities were oiled.Oil persisted for only a few weeks along the exposed rocky shores that experienced moderate to high wave action. In the areas sheltered from wave action, however, the oil persisted in the form of an asphalt crust for several years.The isolated location of the grounding and rough seas hampered cleanup efforts for the two weeks following the incident.As mandated in the ""Polmar Plan"", the French Navy was responsible for all offshore operations while the Civil Safety Service was responsible for shore cleanup activities. Although the total quantity of collected oil and water reached 100,000 tons, less than 20,000 tons of oil were recovered from this liquid after treatment in refining plants.The nature of the oil and rough seas contributed to the rapid formation of a ""chocolate mousse"" emulsification of oil and water. This viscous emulsification greatly complicated the cleanup efforts. French authorities decided not to use dispersants in sensitive areas or the coastal fringe where water depth was less than 50 metres (160 ft). Had dispersant been applied from the air in the vicinity of the spill source, the formation of mousse might have been prevented.At the time, Amoco Cadiz incident resulted in the largest loss of marine life ever recorded from an oil spill. Mortalities of most animals occurred over the two months following the spill. Two weeks following the accident, millions of dead molluscs, sea urchins, and other bottom dwelling organisms washed ashore.Diving birds constituted the majority of the nearly 20,000 dead birds that were recovered. The oyster mortality from the spill was estimated at 9,000 tons. Fishermen in the area caught fish with skin ulcerations and tumors.Some of the fish caught in the area reportedly had a strong taste of petroleum. Although echinoderm and small crustacean populations almost completely disappeared, the populations of many species recovered within a year. Cleanup activities on rocky shores, such as pressure-washing, also caused habitat impacts.The Amoco Cadiz spill was one of the most studied oil spills in history. Many studies remain in progress. This was the largest recorded spill in history and was the first spill in which estuarine tidal rivers were oiled. No follow-up mitigation existed to deal with asphalt formation and problems that resulted after the initial aggressive cleanup.Additional erosion of beaches occurred in several places where no attempt was made to restore the gravel that was removed to lower the beach face. Many of the affected marshes, mudflats, and sandy beaches, were low-energy areas. Evidence of oiled beach sediments can still be seen in some of these sheltered areas. Layers of sub-surface oil still remain buried in many of the impacted beaches.The ship and spill features in one of Steve Forbert's songs about oil pollution. Speedy J has a song named ""Amoco Cadiz"" on his album A Shocking Hobby. French popstar Alain Barriere had a disco hit in France with a song called ""Amoco"".In 1978, it was estimated to have caused US$250 million in damage to fisheries and tourist amenities.  The French government presented claims totalling 1.9 billion french francs to United States courts (using the 1978 exchange rate and with interest added this came to at least US$1.6 billion). In 1984, U.S. District Court Judge Frank J. McGarr held that Amoco was liable for damages when he issued his trial verdict, after 3 1/2 years of legal proceedings. Further, the judge ruled that Amoco had put off needed maintenance on the vessel in order to keep it at sea.In 1992, US oil giant Amoco has decided not to appeal against the US court order that it must pay US$200 million to the French government. Torrey Canyon oil spill - nearby and similar disaster in 1967MT Haven - formerly Amoco Milford Haven, sister ship of Amoco Cadiz that also sank and caused an oil spill disaster.List of oil spillsOil Spills: A Case Study of the Amoco Cadiz Oil Spill at Green NatureThose were the days - Some information on 18 March-24 MarchCentre of Documentation, Research and Experimentation on Accidental Water Pollution - Amoco Cadiz";environmental disaster;Amoco Cadiz oil spill;0
"The Bhopal disaster, also referred to as the Bhopal gas tragedy, was a gas leak incident on the night of 2–3 December 1984 at the Union Carbide India Limited (UCIL) pesticide plant in Bhopal, Madhya Pradesh, India. It was considered as of 2010 to be the world's worst industrial disaster.Over 500,000 people were exposed to methyl isocyanate (MIC) gas. The highly toxic substance made its way into and around the shanty towns located near the plant.Estimates vary on the death toll. The official immediate death toll was 2,259. The government of Madhya Pradesh confirmed a total of 3,787 deaths related to the gas release. A government affidavit in 2006 stated that the leak caused 558,125 injuries, including 38,478 temporary partial injuries and approximately 3,900 severely and permanently disabling injuries. Others estimate that 8,000 died within two weeks, and another 8,000 or more have since died from gas-related diseases.The cause of the disaster remains under debate. The Indian government and local activists argue that slack management and deferred maintenance created a situation where routine pipe maintenance caused a backflow of water into a MIC tank, triggering the disaster. Union Carbide Corporation (UCC) argues water entered the tank through an act of sabotage.The owner of the factory, UCIL, was majority owned by UCC, with Indian Government-controlled banks and the Indian public holding a 49.1 percent stake. In 1989, UCC paid $470 million ($907 million in 2014 dollars) to settle litigation stemming from the disaster. In 1994, UCC sold its stake in UCIL to Eveready Industries India Limited (EIIL), which subsequently merged with McLeod Russel (India) Ltd. Eveready ended clean-up on the site in 1998, when it terminated its 99-year lease and turned over control of the site to the state government of Madhya Pradesh. Dow Chemical Company purchased UCC in 2001, seventeen years after the disaster.Civil and criminal cases were filed in the District Court of Bhopal, India, involving UCC and Warren Anderson, UCC CEO at the time of the disaster. In June 2010, seven former employees, including the former UCIL chairman, were convicted in Bhopal of causing death by negligence and sentenced to two years imprisonment and a fine of about $2,000 each, the maximum punishment allowed by Indian law. An eighth former employee was also convicted, but perished before the judgement was passed. Anderson similarly passed away on 29 September 2014.The UCIL factory was built in 1969 to produce the pesticide Sevin (UCC's brand name for carbaryl) using methyl isocyanate (MIC) as an intermediate. An MIC production plant was added to the UCIL site in 1979.  The chemical process employed in the Bhopal plant had methylamine reacting with phosgene to form MIC, which was then reacted with 1-naphthol to form the final product, carbaryl. Another manufacturer, Bayer, also used this MIC-intermediate process at the chemical plant once owned by UCC at Institute, West Virginia, in the United States.After the Bhopal plant was built, other manufacturers (including Bayer) produced carbaryl without MIC, though at a greater manufacturing cost. This ""route"" differed from the MIC-free routes used elsewhere, in which the same raw materials were combined in a different manufacturing order, with phosgene first reacting with naphthol to form a chloroformate ester, which was then reacted with methylamine. In the early 1980s, the demand for pesticides had fallen, but production continued, leading to build-up of stores of unused MIC where that method was used.In 1976, two local trade unions complained of pollution within the plant. In 1981, a worker was accidentally splashed with phosgene as he was carrying out a maintenance job of the plant's pipes. In a panic, he removed his gas mask and inhaled a large amount of toxic phosgene gas, leading to his death just 72 hours later.In January 1982, a phosgene leak exposed 24 workers, all of whom were admitted to a hospital. None of the workers had been ordered to wear protective masks. One month later, in February 1982, an MIC leak affected 18 workers. In August 1982, a chemical engineer came into contact with liquid MIC, resulting in burns over 30 percent of his body. Later that same year, in October 1982, there was another MIC leak. In attempting to stop the leak, the MIC supervisor suffered severe chemical burns and two other workers were severely exposed to the gases. During 1983 and 1984, there were leaks of MIC, chlorine, monomethylamine, phosgene, and carbon tetrachloride, sometimes in combination.The Bhopal UCIL facility housed three underground 68,000 liters liquid MIC storage tanks: E610, E611, and E619. In the months leading up to the December leak, liquid MIC production was in progress and being used to fill these tanks. UCC safety regulations specified that no one tank should be filled more than 50% (here, 30 tons) with liquid MIC. Each tank was pressurized with inert nitrogen gas. This pressurization allowed liquid MIC to be pumped out of each tank as needed, and also kept impurities out of the tanks.In late October 1984, tank E610 lost the ability to effectively contain most of its nitrogen gas pressure. It meant that the liquid MIC contained within could not be pumped out. At the time of this failure, tank E610 contained 42 tons of liquid MIC. Shortly after this failure, MIC production was halted at the Bhopal facility, and parts of the plant were shut down for maintenance. Maintenance included the shutdown of the plant's flare tower so that a corroded pipe could be repaired. With the flare tower still out of service, production of carbaryl was resumed in late November, using MIC stored in the two tanks still in service. An attempt to re-establish pressure in tank E610 on 1 December failed, so the 42 tons of liquid MIC contained within still could not be pumped out of it.In early December 1984, most of the plant's MIC related safety systems were malfunctioning and many valves and lines were in poor condition. In addition, several vent gas scrubbers had been out of service as well as the steam boiler, intended to clean the pipes.  During the late evening hours of 2 December 1984, water was believed to have entered a side pipe and into Tank E610 whilst trying to unclog it, which contained 42 tons of MIC that had been there since late October.The introduction of water into the tank subsequently resulted in a runaway exothermic reaction, which was accelerated by contaminants, high ambient temperatures and various other factors, such as the presence of iron from corroding non-stainless steel pipelines. The pressure in tank E610, although initially normal at 10:30 p.m., had increased by a factor of five to 10 psi (34.5 to 69 kPa) by 11 p.m. Two different senior refinery employees assumed the reading was instrumentation malfunction.  By 11:30 p.m., workers in the MIC area were feeling the effects of minor exposure to MIC gas, and began to look for a leak. One was found by 11:45 p.m., and reported to the MIC supervisor on duty at the time. The decision was made to address the problem after a 12:15 a.m. tea break, and in the meantime, employees were instructed to continue looking for leaks. The incident was discussed by MIC area employees during the break.In the five minutes after the tea break ended at 12:40 a.m., the reaction in tank E610 reached a critical state at an alarming speed. Temperatures in the tank were indicated off its scale, maxed out beyond  25 °C (77 °F), and the pressure in the tank was indicated at 40 psi (275.8 kPa). One employee witnessed a concrete slab above tank E610 crack as the emergency relief valve burst open, and pressure in the tank continued to increase to 55 psi (379.2 kPa) even after atmospheric venting of toxic MIC gas had begun. Direct atmospheric venting should have been prevented or at least partially mitigated by at least three safety devices which were malfunctioning, not in use, insufficiently sized or otherwise rendered inoperable:A refrigeration system meant to cool tanks containing liquid MIC, shut down in January 1982, and whose freon had been removed in June 1984. Since the MIC storage system assumed refrigeration, its high temperature alarm, set to sound at 11 °C (52 °F) had long since been disconnected, and tank storage temperatures ranged between 15 °C (59 °F) and 40 °C (104 °F)A flare tower, to burn the MIC gas as it escaped, which had had a connecting pipe removed for maintenance, and was improperly sized to neutralise a leak of the size produced by tank E610A vent gas scrubber, which had been deactivated at the time and was in 'standby' mode, and similarly had insufficient caustic soda and power to safely stop a leak of the magnitude producedAbout 30 metric tons of MIC escaped from the tank into the atmosphere in 45 to 60 minutes. This would increase to 40 metric tons within two hours time. The gases were blown in a southeasterly direction over Bhopal.A UCIL employee triggered the plant's alarm system at 12:50 a.m. as the concentration of gas in and around the plant became difficult to tolerate. Activation of the system triggered two siren alarms: one that sounded inside the UCIL plant, and a second directed outward to the public and the city of Bhopal. The two siren systems had been decoupled from one another in 1982, so that it was possible to leave the factory warning siren on while turning off the public one, and this is exactly what was done: the public siren briefly sounded at 12:50 a.m. and was quickly turned off, as per company procedure meant to avoid alarming the public around the factory over tiny leaks. Workers, meanwhile, evacuated the UCIL plant, travelling upwind.Bhopal's superintendent of police was informed by telephone, by a town inspector, that residents of the neighbourhood of Chola (about 2 km from the plant) were fleeing a gas leak at approximately 1 a.m. Calls to the UCIL plant by police between 1:25 and 2:10 a.m. gave assurances twice that ""everything is OK"", and on the last attempt made, ""we don't know what has happened, sir"". With the lack of timely information exchange between UCIL and Bhopal authorities, the city's Hamidia Hospital was first told that the gas leak was suspected to be ammonia, then phosgene. Finally, they received an updated report that it was ""MIC"", about which hospital staff had never heard of, had no antidote for, and received no immediate information about.The MIC gas leak emanating from tank E610 petered out at approximately 2:00 a.m. Fifteen minutes later, the plant's public siren was sounded for an extended period of time, after first having been quickly silenced an hour and a half earlier. Some minutes after the public siren sounded, a UCIL employee walked to a police control room to both inform them of the leak (their first acknowledgement that one had occurred at all), and that ""the leak had been plugged."" Most city residents who were exposed to the MIC gas were first made aware of the leak by exposure to the gas itself, or by opening their doors to investigate commotion, rather than having been instructed to shelter in place, or to evacuate before the arrival of the gas in the first place.The initial effects of exposure were coughing, severe eye irritation and a feeling of suffocation, burning in the respiratory tract, blepharospasm, breathlessness, stomach pains and vomiting. People awakened by these symptoms fled away from the plant. Those who ran inhaled more than those who had a vehicle to ride. Owing to their height, children and other people of shorter stature inhaled higher concentrations.Thousands of people had died by the following morning.Primary causes of deaths were choking, reflexogenic circulatory collapse and pulmonary oedema. Findings during autopsies revealed changes not only in the lungs but also cerebral oedema, tubular necrosis of the kidneys, fatty degeneration of the liver and necrotising enteritis. The stillbirth rate increased by up to 300% and neonatal mortality rate by around 200%.Apart from MIC, based on laboratory simulation conditions, the gas cloud most likely also contained chloroform, dichloromethane, hydrogen chloride, methyl amine, dimethylamine, trimethylamine and carbon dioxide, that was either present in the tank or was produced in the storage tank when MIC, chloroform and water reacted. The gas cloud, composed mainly of materials denser than air, stayed close to the ground and spread in the southeasterly direction affecting the nearby communities. The chemical reactions may have produced a liquid or solid aerosol.Laboratory investigations by CSIR and UCC scientists failed to demonstrate the presence of hydrogen cyanide.In the immediate aftermath, the plant was closed to outsiders (including UCC) by the Indian government, which subsequently failed to make data public, contributing to the confusion. The initial investigation was conducted entirely by the Council of Scientific and Industrial Research (CSIR) and the Central Bureau of Investigation. The UCC chairman and CEO Warren Anderson, together with a technical team, immediately traveled to India. Upon arrival Anderson was placed under house arrest and urged by the Indian government to leave the country within 24 hours. Union Carbide organized a team of international medical experts, as well as supplies and equipment, to work with the local Bhopal medical community, and the UCC technical team began assessing the cause of the gas leak.The health care system immediately became overloaded. In the severely affected areas, nearly 70 percent were under-qualified doctors. Medical staff were unprepared for the thousands of casualties. Doctors and hospitals were not aware of proper treatment methods for MIC gas inhalation.There were mass funerals and cremations. Photographer Pablo Bartholemew, on commission with press agency Rapho, took an iconic color photograph of a burial on December 4, Bhopal gas disaster girl. Another photographer present, Raghu Rai, took a black and white photo. The photographers did not ask for the identity of the father or child as she was buried, and no relative has since confirmed it. As such, the identity of the girl remains unknown. Both photos became symbolic of the suffering of victims of the Bhopal disaster, and Bartholomew's went on to win the 1984 World Press Photo of the Year.Within a few days, trees in the vicinity became barren and bloated animal carcasses had to be disposed of. 170,000 people were treated at hospitals and temporary dispensaries 2,000 buffalo, goats, and other animals were collected and buried. Supplies, including food, became scarce owing to suppliers' safety fears. Fishing was prohibited causing further supply shortages.Lacking any safe alternative, on 16 December, tanks 611 and 619 were emptied of the remaining MIC by reactivating the plant and continuing the manufacture of pesticide. Despite safety precautions such as having water carrying helicopters continually overflying the plant, this led to a second mass evacuation from Bhopal. The Government of India passed the ""Bhopal Gas Leak Disaster Act"" that gave the government rights to represent all victims, whether or not in India. Complaints of lack of information or misinformation were widespread. An Indian government spokesman said, ""Carbide is more interested in getting information from us than in helping our relief work"".Formal statements were issued that air, water, vegetation and foodstuffs were safe, but warned not to consume fish. The number of children exposed to the gases was at least 200,000. Within weeks, the State Government established a number of hospitals, clinics and mobile units in the gas-affected area to treat the victims.Legal proceedings involving UCC, the United States and Indian governments, local Bhopal authorities, and the disaster victims started immediately after the catastrophe. The Indian Government passed the Bhopal Gas Leak Act in March 1985, allowing the Government of India to act as the legal representative for victims of the disaster, leading to the beginning of legal proceedings. Initial lawsuits were generated in the United States federal court system. On April 17, 1985, Federal District court judge John F. Keenan (overseeing one lawsuit) suggested that ""'fundamental human decency' required Union Carbide to provide between $5 million and $10 million to immediately help the injured"" and suggested the money could be quickly distributed through the International Red Cross. UCC, on the notion that doing so did not constitute an admission of liability and the figure could be credited toward any future settlement or judgement, offered a $5 million relief fund two days later. The Indian government turned down the offer.In March 1986 UCC proposed a settlement figure, endorsed by plaintiffs' U.S. attorneys, of $350 million that would, according to the company, ""generate a fund for Bhopal victims of between $500–600 million over 20 years"". In May, litigation was transferred from the United States to Indian courts by a U.S. District Court ruling. Following an appeal of this decision, the U.S. Court of Appeals affirmed the transfer, judging, in January 1987, that UCIL was a ""separate entity, owned, managed and operated exclusively by Indian citizens in India"".The Government of India refused the offer from Union Carbide and claimed US$3.3 billion. The Indian Supreme Court told both sides to come to an agreement and ""start with a clean slate"" in November 1988. Eventually, in an out-of-court settlement reached in February 1989, Union Carbide agreed to pay US$470 million for damages caused in the Bhopal disaster. The amount was immediately paid.Throughout 1990, the Indian Supreme Court heard appeals against the settlement. In October 1991, the Supreme Court upheld the original $470 million, dismissing any other outstanding petitions that challenged the original decision. The Court ordered the Indian government ""to purchase, out of settlement fund, a group medical insurance policy to cover 100,000 persons who may later develop symptoms"" and cover any shortfall in the settlement fund. It also requested UCC and its subsidiary UCIL ""voluntarily"" fund a hospital in Bhopal, at an estimated $17 million, to specifically treat victims of the Bhopal disaster. The company agreed to this.In 1991, the local Bhopal authorities charged Anderson, who had retired in 1986, with manslaughter, a crime that carries a maximum penalty of 10 years in prison. He was declared a fugitive from justice by the Chief Judicial Magistrate of Bhopal on 1 February 1992 for failing to appear at the court hearings in a culpable homicide case in which he was named the chief defendant. Orders were passed to the Government of India to press for an extradition from the United States. The U.S. Supreme Court refused to hear an appeal of the decision of the lower federal courts in October 1993, meaning that victims of the Bhopal disaster could not seek damages in a U.S. court.In 2004, the Indian Supreme Court ordered the Indian government to release any remaining settlement funds to victims. And in September 2006, the Welfare Commission for Bhopal Gas Victims announced that all original compensation claims and revised petitions had been ""cleared"". The Second Circuit Court of Appeals in New York City upheld the dismissal of remaining claims in the case of Bano v. Union Carbide Corporation in 2006. This move blocked plaintiffs' motions for class certification and claims for property damages and remediation. In the view of UCC, ""the ruling reaffirms UCC's long-held positions and finally puts to rest—both procedurally and substantively—the issues raised in the class action complaint first filed against Union Carbide in 1999 by Haseena Bi and several organisations representing the residents of Bhopal"".In June 2010, seven former employees of UCIL, all Indian nationals and many in their 70s, were convicted of causing death by negligence: Keshub Mahindra, former non-executive chairman of Union Carbide India Limited V. P. Gokhale, managing director Kishore Kamdar, vice-president J. Mukund, works manager S. P. Chowdhury, production manager K. V. Shetty, plant superintendent and S. I. Qureshi, production assistant. They were each sentenced to two years imprisonment and fined Rs.100,000 (US$2,124). All were released on bail shortly after the verdict.US Federal class action litigation, Sahu v. Union Carbide and Warren Anderson, had been filed in 1999 under the U.S. Alien Torts Claims Act (ATCA), which provides for civil remedies for ""crimes against humanity."" It sought damages for personal injury, medical monitoring and injunctive relief in the form of clean-up of the drinking water supplies for residential areas near the Bhopal plant. The lawsuit was dismissed in 2012 and subsequent appeal denied. Anderson died in 2014.In 2018 The Atlantic, called it the ""world’s worst industrial disaster.""Some data about the health effects are still not available. The Indian Council of Medical Research (ICMR) was forbidden to publish health effect data until 1994.A total of 36 wards were marked by the authorities as being ""gas affected,"" affecting a population of 520,000. Of these, 200,000 were below 15 years of age, and 3,000 were pregnant women. The official immediate death toll was 2,259, and in 1991, 3,928 deaths had been officially certified. Ingrid Eckerman estimated 8,000 died within two weeks.The government of Madhya Pradesh confirmed a total of 3,787 deaths related to the gas release.Later, the affected area was expanded to include 700,000 citizens. A government affidavit in 2006 stated the leak caused 558,125 injuries including 38,478 temporary partial injuries and approximately 3,900 severely and permanently disabling injuries.A cohort of 80,021 exposed people was registered, along with a control group, a cohort of 15,931 people from areas not exposed to MIC. Nearly every year since 1986, they have answered the same questionnaire. It shows overmortality and overmorbidity in the exposed group. Bias and confounding factors cannot be excluded from the study. Because of migration and other factors, 75% of the cohort is lost, as the ones who moved out are not followed.A number of clinical studies are performed. The quality varies, but the different reports support each other. Studied and reported long term health effects are:Eyes: Chronic conjunctivitis, scars on cornea, corneal opacities, early cataractsRespiratory tracts: Obstructive and/or restrictive disease, pulmonary fibrosis, aggravation of TB and chronic bronchitisNeurological system: Impairment of memory, finer motor skills, numbness etc.Psychological problems: Post traumatic stress disorder (PTSD)Children’s health: Peri- and neonatal death rates increased. Failure to grow, intellectual impairment, etc.Missing or insufficient fields for research are female reproduction, chromosomal aberrations, cancer, immune deficiency, neurological sequelae, post traumatic stress disorder (PTSD) and children born after the disaster. Late cases that might never be highlighted are respiratory insufficiency, cardiac insufficiency (cor pulmonale), cancer and tuberculosis.A 2014 report in Mother Jones quotes a ""spokesperson for the Bhopal Medical Appeal, which runs free health clinics for survivors"" as saying ""An estimated 120,000 to 150,000 survivors still struggle with serious medical conditions including nerve damage, growth problems, gynecological disorders, respiratory issues, birth defects, and elevated rates of cancer and tuberculosis.""The Government of India had focused primarily on increasing the hospital-based services for gas victims thus hospitals had been built after the disaster. When UCC wanted to sell its shares in UCIL, it was directed by the Supreme Court to finance a 500-bed hospital for the medical care of the survivors. Thus, Bhopal Memorial Hospital and Research Centre (BMHRC) was inaugurated in 1998 and was obliged to give free care for survivors for eight years. BMHRC was a 350-bedded super speciality hospital where heart surgery and hemodialysis were done. There was a dearth of gynaecology, obstetrics and paediatrics. Eight mini-units (outreach health centres) were started and free health care for gas victims were to be offered until 2006. The management had also faced problems with strikes, and the quality of the health care being disputed. Sambhavna Trust is a charitable trust, registered in 1995, that gives modern as well as ayurvedic treatments to gas victims, free of charge.When the factory was closed in 1986, pipes, drums and tanks were sold. The MIC and the Sevin plants are still there, as are storages of different residues. Isolation material is falling down and spreading. The area around the plant was used as a dumping area for hazardous chemicals. In 1982 tubewells in the vicinity of the UCIL factory had to be abandoned and tests in 1989 performed by UCC's laboratory revealed that soil and water samples collected from near the factory and inside the plant were toxic to fish. Several other studies had also shown polluted soil and groundwater in the area. Reported polluting compounds include 1-naphthol, naphthalene, Sevin, tarry residue, mercury, toxic organochlorines, volatile organochlorine compounds, chromium, copper, nickel, lead, hexachloroethane, hexachlorobutadiene, and the pesticide HCH.In order to provide safe drinking water to the population around the UCIL factory, Government of Madhya Pradesh presented a scheme for improvement of water supply. In December 2008, the Madhya Pradesh High Court decided that the toxic waste should be incinerated at Ankleshwar in Gujarat, which was met by protests from activists all over India. On 8 June 2012, the Centre for incineration of toxic Bhopal waste agreed to pay ₹250 million (US$3.7 million) to dispose of UCIL chemical plants waste in Germany. On 9 August 2012, Supreme court directed the Union and Madhya Pradesh Governments to take immediate steps for disposal of toxic waste lying around and inside the factory within six months.A U.S. court rejected the lawsuit blaming UCC for causing soil and water pollution around the site of the plant and ruled that responsibility for remedial measures or related claims rested with the State Government and not with UCC. In 2005, the state government invited various Indian architects to enter their ""concept for development of a memorial complex for Bhopal gas tragedy victims at the site of Union Carbide"". In 2011, a conference was held on the site, with participants from European universities which was aimed for the same.33 of the 50 planned work-sheds for gas victims started. All except one was closed down by 1992. 1986, the MP government invested in the Special Industrial Area Bhopal. 152 of the planned 200 work sheds were built and in 2000, 16 were partially functioning. It was estimated that 50,000 persons need alternative jobs, and that less than 100 gas victims had found regular employment under the government's scheme. The government also planned 2,486 flats in two- and four-story buildings in what is called the ""widow's colony"" outside Bhopal. The water did not reach the upper floors and it was not possible to keep cattle which were their primary occupation. Infrastructure like buses, schools, etc. were missing for at least a decade.Immediate relieves were decided two days after the tragedy. Relief measures commenced in 1985 when food was distributed for a short period along with ration cards. Madhya Pradesh government's finance department allocated ₹874 million (US$13 million) for victim relief in July 1985. Widow pension of ₹200 (US$3.00)/per month (later ₹750 (US$11)) were provided. The government also decided to pay ₹1,500 (US$22) to families with monthly income ₹500 (US$7.50) or less. As a result of the interim relief, more children were able to attend school, more money was spent on treatment and food, and housing also eventually improved. From 1990 interim relief of ₹200 (US$3.00) was paid to everyone in the family who was born before the disaster.The final compensation, including interim relief for personal injury was for the majority ₹25,000 (US$370). For death claim, the average sum paid out was ₹62,000 (US$920). Each claimant were to be categorised by a doctor. In court, the claimants were expected to prove ""beyond reasonable doubt"" that death or injury in each case was attributable to exposure. In 1992, 44 percent of the claimants still had to be medically examined.By the end of October 2003, according to the Bhopal Gas Tragedy Relief and Rehabilitation Department, compensation had been awarded to 554,895 people for injuries received and 15,310 survivors of those killed. The average amount to families of the dead was $2,200.In 2007, 1,029,517 cases were registered and decided. Number of awarded cases were 574,304 and number of rejected cases 455,213. Total compensation awarded was ₹15,465 million (US$230 million). On 24 June 2010, the Union Cabinet of the Government of India approved a ₹12,650 million (US$190 million) aid package which would be funded by Indian taxpayers through the government.In 1985, Henry Waxman, a California Democrat, called for a U.S. government inquiry into the Bhopal disaster, which resulted in U.S. legislation regarding the accidental release of toxic chemicals in the United States.There are two main lines of argument involving the disaster.The ""Corporate Negligence"" point of view argues that the disaster was caused by a potent combination of under-maintained and decaying facilities, a weak attitude towards safety, and an undertrained workforce, culminating in worker actions that inadvertently enabled water to penetrate the MIC tanks in the absence of properly working safeguards.The ""Worker Sabotage"" point of view argues that it was not physically possible for the water to enter the tank without concerted human effort, and that extensive testimony and engineering analysis leads to a conclusion that water entered the tank when a rogue individual employee hooked a water hose directly to an empty valve on the side of the tank. This point of view further argues that the Indian government took extensive actions to hide this possibility in order to attach blame to UCC.Theories differ as to how the water entered the tank. At the time, workers were cleaning out a clogged pipe with water about 400 feet from the tank. They claimed that they were not told to isolate the tank with a pipe slip-blind plate. The operators assumed that owing to bad maintenance and leaking valves, it was possible for the water to leak into the tank.This water entry route could not be reproduced despite strenuous efforts by motivated parties. UCC claims that a ""disgruntled worker"" deliberately connecting a hose to a pressure gauge connection was the real cause.Early the next morning, a UCIL manager asked the instrument engineer to replace the gauge. UCIL's investigation team found no evidence of the necessary connection the investigation was totally controlled by the government, denying UCC investigators access to the tank or interviews with the operators.This point of view argues that management (and to some extent, local government) underinvested in safety, which allowed for a dangerous working environment to develop. Factors cited include the filling of the MIC tanks beyond recommended levels, poor maintenance after the plant ceased MIC production at the end of 1984, allowing several safety systems to be inoperable due to poor maintenance, and switching off safety systems to save money— including the MIC tank refrigeration system which could have mitigated the disaster severity, and non-existent catastrophe management plans. Other factors identified by government inquiries included undersized safety devices and the dependence on manual operations. Specific plant management deficiencies that were identified include the lack of skilled operators, reduction of safety management, insufficient maintenance, and inadequate emergency action plans.UnderinvestmentUnderinvestment is cited as contributing to an environment. Attempts to reduce expenses affected the factory's employees and their conditions. Kurzman argues that ""cuts ... meant less stringent quality control and thus looser safety rules. A pipe leaked? Don't replace it, employees said they were told ... MIC workers needed more training? They could do with less. Promotions were halted, seriously affecting employee morale and driving some of the most skilled ... elsewhere"". Workers were forced to use English manuals, even though only a few had a grasp of the language.Subsequent research highlights a gradual deterioration of safety practices in regard to the MIC, which had become less relevant to plant operations. By 1984, only six of the original twelve operators were still working with MIC and the number of supervisory personnel had also been halved. No maintenance supervisor was placed on the night shift and instrument readings were taken every two hours, rather than the previous and required one-hour readings. Workers made complaints about the cuts through their union but were ignored. One employee was fired after going on a 15-day hunger strike. 70% of the plant's employees were fined before the disaster for refusing to deviate from the proper safety regulations under pressure from the management.In addition, some observers, such as those writing in the Trade Environmental Database (TED) Case Studies as part of the Mandala Project from American University, have pointed to ""serious communication problems and management gaps between Union Carbide and its Indian operation"", characterised by ""the parent companies [sic] hands-off approach to its overseas operation"" and ""cross-cultural barriers"".Adequacy of equipment and safety regulationsThe factory was not well equipped to handle the gas created by the sudden addition of water to the MIC tank. The MIC tank alarms had not been working for four years and there was only one manual back-up system, compared to a four-stage system used in the United States. The flare tower and several vent gas scrubbers had been out of service for five months before the disaster. Only one gas scrubber was operating: it could not treat such a large amount of MIC with sodium hydroxide (caustic soda), which would have brought the concentration down to a safe level. The flare tower could only handle a quarter of the gas that leaked in 1984, and moreover it was out of order at the time of the incident. To reduce energy costs, the refrigeration system was idle. The MIC was kept at 20 degrees Celsius, not the 4.5 degrees advised by the manual. Even the steam boiler, intended to clean the pipes, was non-operational for unknown reasons. Slip-blind plates that would have prevented water from pipes being cleaned from leaking into the MIC tanks, had the valves been faulty, were not installed and their installation had been omitted from the cleaning checklist. As MIC is water-soluble, deluge guns were in place to contain escaping gases from the stack. The water pressure was too weak for the guns to spray high enough to reach the gas which would have reduced the concentration of escaping gas significantly. In addition to it, carbon steel valves were used at the factory, even though they were known to corrode when exposed to acid.According to the operators, the MIC tank pressure gauge had been malfunctioning for roughly a week. Other tanks were used, rather than repairing the gauge. The build-up in temperature and pressure is believed to have affected the magnitude of the gas release. UCC admitted in their own investigation report that most of the safety systems were not functioning on the night of 3 December 1984. The design of the MIC plant, following government guidelines, was ""Indianized"" by UCIL engineers to maximise the use of indigenous materials and products. Mumbai-based Humphreys and Glasgow Consultants Pvt. Ltd., were the main consultants, Larsen & Toubro fabricated the MIC storage tanks, and Taylor of India Ltd. provided the instrumentation. In 1998, during civil action suits in India, it emerged that the plant was not prepared for problems. No action plans had been established to cope with incidents of this magnitude. This included not informing local authorities of the quantities or dangers of chemicals used and manufactured at Bhopal.Safety auditsSafety audits were done every year in the US and European UCC plants, but only every two years in other parts of the world. Before a ""Business Confidential"" safety audit by UCC in May 1982, the senior officials of the corporation were well aware of ""a total of 61 hazards, 30 of them major and 11 minor in the dangerous phosgene/methyl isocyanate units"" in Bhopal. In the audit 1982, it was indicated that worker performance was below standards. Ten major concerns were listed. UCIL prepared an action plan, but UCC never sent a follow-up team to Bhopal. Many of the items in the 1982 report were temporarily fixed, but by 1984, conditions had again deteriorated. In September 1984, an internal UCC report on the West Virginia plant in the USA revealed a number of defects and malfunctions. It warned that ""a runaway reaction could occur in the MIC unit storage tanks, and that the planned response would not be timely or effective enough to prevent catastrophic failure of the tanks"". This report was never forwarded to the Bhopal plant, although the main design was the same.Now owned by Dow Chemical Company, Union Carbide maintains a website dedicated to the tragedy and claims that the incident was the result of sabotage, stating that sufficient safety systems were in place and operative to prevent the intrusion of water.The impossibility of the ""negligence"" argumentAccording to the ""Corporate Negligence"" argument, workers had been cleaning out pipes with water nearby. This water was diverted due to a combination of improper maintenance, leaking and clogging, and eventually ended up in the MIC storage tank. Indian scientists also suggested that additional water might have been introduced as a ""back-flow"" from a defectively designed vent-gas scrubber. None of these theoretical routes of entry were ever successfully demonstrated during tests by the Central Bureau of Investigators (CBI) and UCIL engineers.A Union Carbide commissioned analysis conducted by Arthur D. Little claims that the Negligence argument was impossible for several tangible reasons:The pipes being used by the nearby workers were only 1/2 inch in diameter and were physically incapable of producing enough hydraulic pressure to raise water the more than 10 feet that would have been necessary to enable the water to ""backflow"" into the MIC tank.A key intermediate valve would have had to be open for the Negligence argument to apply. This valve was ""tagged"" closed, meaning that it had been inspected and found to be closed. While it is possible for open valves to clog over time, the only way a closed valve allows penetration is if there is leakage, and 1985 tests carried out by the government of India found this valve to be non-leaking.In order for water to have reached the MIC tank from the pipe-cleaning area, it would have had to flow through a significant network of pipes ranging from 6 to 8 inches in diameter, before rising ten feet and flowing into the MIC tank. Had this occurred, most of the water that was in those pipes at the time the tank had its critical reaction would have remained in those pipes, as there was no drain for them. Investigation by the Indian government in 1985 revealed that the pipes were bone dry.The Union Carbide commissioned Arthur D. Little report concludes that it is likely that a single employee secretly and deliberately introduced a large amount of water into the MIC tank by removing a meter and connecting a water hose directly to the tank through the metering port.UCC claims the plant staff falsified numerous records to distance themselves from the incident and absolve themselves of blame, and that the Indian Government impeded its investigation and declined to prosecute the employee responsible, presumably because that would weaken its allegations of negligence by Union Carbide.The evidence in favor of this point of view includes:A key witness (the ""tea boy"") testified that when he entered the control room at 12:15 am, prior to the disaster, the ""atmosphere was tense and quiet"".Another key witness (the ""instrument supervisor"") testified that when he arrived at the scene immediately following the incident, he noticed that the local pressure indicator on the critical Tank 610 was missing, and that he had found a hose lying next to the empty manhead created by the missing pressure indicator, and that the hose had had water running out of it.This testimony was corroborated by other witnesses.Graphological analysis revealed major attempts to alter logfiles and destroy log evidence.Other logfiles show that the control team had attempted to purge 1 ton of material out of Tank 610 immediately prior to the disaster. An attempt was then made to cover up this transfer via log alteration. Water is heavier than MIC, and the transfer line is attached to the bottom of the tank. The Arthur D. Little report concludes from this that the transfer was an effort to transfer water out of Tank 610 that had been discovered there.A third key witness (the ""off-duty employee of another unit"") stated that ""he had been told by a close friend of one of the MIC operators that water had entered through a tube that had been connected to the tank."" This had been discovered by the other MIC operators (so the story was recounted) who then tried to open and close valves to prevent the release.A fourth key witness (the ""operator from a different unit"") stated that after the release, two MIC operators had told him that water had entered the tank through a pressure gauge.The Little report argues that this evidence demonstrates that the following chronology took place:At 10:20pm, the tank was at normal pressure, indicating the absence of water.At 10:45pm, a shift change took place, after which the MIC storage area ""would be completely deserted"".During this period, a ""disgruntled operator entered the storage area and hooked up one of the readily available rubber water hoses to Tank 610, with the intention of contaminating and spoiling the tank's contents.""Water began to flow, beginning the chemical reaction that caused the disaster.After midnight, control room operators saw the pressure rising and realized there was a problem with Tank 610. They discovered the water connection, and decided to transfer one ton of the contents out to try and remove the water.The disaster then occurred, a major release of poisonous gas.The cover-up activities discovered during the investigation then took place.After over 30 years, S.P. Choudhary, former MIC Production Manager, broke the silence and told the truth about the disaster that it was not an accident but the result of a sabotage that claimed thousands of lives, a former official of the Union Carbide India Limited (UCIL) told the district and sessions court.The theory of design defect was floated by the central government in its endeavour to do justice to the victims of the tragedy. Everyone else who was part of investigations into the case ""just toed the line of the central government....The government and the CBI suppressed the actual truth and saved the real perpetrators of the crime, the counsel, Anirban Roy told the court.""In November 2017, appearing for two accused S P Chaudhary and J Mukund, their advocate Anirban Roy told the district court on Monday that disgruntled plant operator M L Verma was behind the sabotage because he was unhappy with his seniors. Roy argued that the theory about defects in the plant causing the mishap was imaginary. He said truth had always been suppressed and it’s for the CBI to bring it out.The counsel argued that there were discrepancies in the statements given by persons who were operating the plant at that time but the central agency chose not to investigate the case properly because it always wanted to prove that it was a mishap, and not sabotage. He alleged that Verma was unhappy with Chaudhary and Mukund.The corporation denied the claim that the valves on the tank were malfunctioning, and claimed that the documented evidence gathered after the incident showed that the valve close to the plant's water-washing operation was closed and was leak-tight. Furthermore, process safety systems had prevented water from entering the tank by accident. Carbide states that the safety concerns identified in 1982 were all allayed before 1984 and had nothing to do with the incident.The company admitted that the safety systems in place would not have been able to prevent a chemical reaction of that magnitude from causing a leak. According to Carbide, ""in designing the plant's safety systems, a chemical reaction of this magnitude was not factored in"" because ""the tank's gas storage system was designed to automatically prevent such a large amount of water from being inadvertently introduced into the system"" and ""process safety systems—in place and operational—would have prevented water from entering the tank by accident"". Instead, they claim that ""employee sabotage—not faulty design or operation—was the cause of the tragedy"".Tactical responseThe company stresses the immediate action taken after the disaster and its continued commitment to helping the victims. On 4 December, the day following the leak, Union Carbide sent material aid and several international medical experts to assist the medical facilities in Bhopal.Financial responseThe primary financial restitution paid by UCC was negotiated in 1989, when the Indian Supreme Court approved a settlement of US$470 million (₹1,055 crore (equivalent to ₹80 billion or US$1.2 billion in 2017)). This amount was immediately paid by UCC to the Indian government. The company states that the restitution paid ""was $120 million more than plaintiffs' lawyers had told U.S. courts was fair"" and that the Indian Supreme Court stated in its opinion that ""compensation levels under the settlement were far greater than would normally be payable under Indian law.""In the immediate aftermath of the disaster, Union Carbide states on its website that it put $2 million into the Indian prime minister's immediate disaster relief fund on 11 December 1984. The corporation established the Employees' Bhopal Relief Fund in February 1985, which raised more than $5 million for immediate relief. According to Union Carbide, in August 1987, they made an additional $4.6 million in humanitarian interim relief available.Union Carbide stated that it also undertook several steps to provide continuing aid to the victims of the Bhopal disaster. The sale of its 50.9 percent interest in UCIL in April 1992 and establishment of a charitable trust to contribute to the building of a local hospital. The sale was finalised in November 1994. The hospital was begun in October 1995 and was opened in 2001. The company provided a fund with around $90 million from sale of its UCIL stock. In 1991, the trust had amounted approximately $100 million. The hospital catered for the treatment of heart, lung and eye problems. UCC also provided a $2.2 million grant to Arizona State University to establish a vocational-technical center in Bhopal, which was opened, but was later closed by the state government. They also donated $5 million to the Indian Red Cross after the disaster. They also developed a Responsible Care system with other members of the chemical industry as a response to the Bhopal crisis, which was designed to help prevent such an event in the future.UCC chairman and CEO Warren Anderson was arrested and released on bail by the Madhya Pradesh Police in Bhopal on 7 December 1984. Anderson was taken to UCC's house after which he was released six hours later on $2,100 bail and flown out on a government plane. These actions were allegedly taken under the direction of then chief secretary of the state, who was possibly instructed from chief minister's office, who himself flew out of Bhopal immediately. Later in 1987, the Indian government summoned Anderson, eight other executives and two company affiliates with homicide charges to appear in Indian court. In response, Union Carbide said the company is not under Indian jurisdiction.From 2014, Dow is a named respondent in a number of ongoing cases arising from Union Carbide’s business in Bhopal.Chemicals abandoned at the plant continue to leak and pollute the groundwater. Whether the chemicals pose a health hazard is disputed. Contamination at the site and surrounding area was not caused by the gas leakage. The area around the plant was used as a dumping ground for hazardous chemicals and by 1982 water wells in the vicinity of the UCIL factory had to be abandoned. UCC states that ""after the incident, UCIL began clean-up work at the site under the direction of Indian central and state government authorities"", which was continued after 1994 by the successor to UCIL. The successor, Eveready Industries India, Limited (EIIL), ended cleanup on the site in 1998, when it terminated its 99-year lease and turned over control of the site to the state government of Madhya Pradesh.UCC's laboratory tests in 1989 revealed that soil and water samples collected from near the factory were toxic to fish. Twenty-one areas inside the plant were reported to be highly polluted. In 1991 the municipal authorities declared that water from over 100 wells was hazardous for health if used for drinking. In 1994 it was reported that 21% of the factory premises were seriously contaminated with chemicals. Beginning in 1999, studies made by Greenpeace and others from soil, groundwater, well water and vegetables from the residential areas around UCIL and from the UCIL factory area show contamination with a range of toxic heavy metals and chemical compounds. Substances found, according to the reports, are naphthol, naphthalene, Sevin, tarry residues, alpha naphthol, mercury, organochlorines, chromium, copper, nickel, lead, hexachlorethane, hexachlorobutadiene, pesticide HCH (BHC), volatile organic compounds and halo-organics. Many of these contaminants were also found in breast milk of women living near the area.Soil tests were conducted by Greenpeace in 1999. One sample (IT9012) from ""sediment collected from drain under former Sevin plant"" showed mercury levels to be at ""20,000 and 6 million times"" higher than expected levels. Organochlorine compounds at elevated levels were also present in groundwater collected from (sample IT9040) a 4.4 meter depth ""bore-hole within the former UCIL site"". This sample was obtained from a source posted with a warning sign which read ""Water unfit for consumption"".Chemicals that have been linked to various forms of cancer were also discovered, as well as trichloroethylene, known to impair fetal development, at 50 times above safety limits specified by the U.S. Environmental Protection Agency (EPA). In 2002, an inquiry by Fact-Finding Mission on Bhopal found a number of toxins, including mercury, lead, 1,3,5 trichlorobenzene, dichloromethane and chloroform, in nursing women's breast milk.A 2004 BBC Radio 5 broadcast reported the site is contaminated with toxic chemicals including benzene hexachloride and mercury, held in open containers or loose on the ground. A drinking water sample from a well near the site had levels of contamination 500 times higher than the maximum limits recommended by the World Health Organization. In 2009, the Centre for Science and Environment, a Delhi-based pollution monitoring lab, released test results showing pesticide groundwater contamination up to three kilometres from the factory. Also in 2009, the BBC took a water sample from a frequently used hand pump, located just north of the plant. The sample, tested in UK, was found to contain 1,000 times the World Health Organization's recommended maximum amount of carbon tetrachloride, a carcinogenic toxin.In 2010, a British photojournalist who ventured into the abandoned Union Carbide factory to investigate allegations of abandoned, leaking toxins, was hospitalized in Bhopal for a week after he was exposed to the chemicals. Doctors at the Sambhavna Clinic treated him with oxygen, painkillers and anti-inflammatories following a severe respiratory reaction to toxic dust inside the factory.In October 2011, the Institute of Environmental Management and Assessment published an article and video by two British environmental scientists, showing the current state of the plant, landfill and solar evaporation ponds and calling for renewed international efforts to provide the necessary skills to clean up the site and contaminated groundwater.In 1999, a Hindi film dealing with the tragedy, Bhopal Express, was released. The film stars Kay Kay Menon and Naseeruddin Shah.Amulya Malladi's 2002 novel A Breath of Fresh Air relates the story of a mother and son who develop health issues as a result of exposure to gas at Bhopal. The book is based on Malladi's recollections of Bhopal during the incident.Indra Sinha released Animal's People in 2007. The novel tells the story of a boy who is born with a spinal condition due to effects of the gas. The book was shortlisted for the Man Booker Prize.In 2014, to coincide with the 30th anniversary of the disaster, historical-drama Bhopal: A Prayer for Rain was released, starring Martin Sheen as Union Carbide CEO Warren Anderson, Kal Penn, and Mischa Barton. The film earned global praise and LA Times critic Martin Tsai said the film was ""ambitious and shattering"" and that ""Although the real-life events took place three decades ago, the cautionary tale could not be more relevant.""Arundhati Roy's 2017 novel The Ministry of Utmost Happiness which deals with many contemporary political issues in India also features several characters dealing with the aftermath of the gas leak still.Since 1984, individual activists have played a role in the aftermath of the tragedy. The best-known is Satinath Sarangi (Sathyu), a metallurgic engineer who arrived at Bhopal the day after the leakage. He founded several activist groups, as well as Sambhavna Trust, the clinic for gas affected patients, where he is the manager. Other activists include Rashida Bee and Champa Devi Shukla, who received the Goldman Prize in 2004, Abdul Jabbar and Rachna Dhingra.Soon after the accident, representatives from different activist groups arrived. The activists worked on organising the gas victims, which led to violent repression from the police and the government.Numerous actions have been performed: demonstrations, sit-ins, hunger strikes, marches combined with pamphlets, books, and articles. Every anniversary, actions are performed. Often these include marches around Old Bhopal, ending with burning an effigy of Warren Anderson.Cooperation with international NGOs including Pesticide Action Network UK and Greenpeace started soon after the tragedy. One of the earliest reports is the Trade Union report from ILO 1985.In 1992, a session of the Permanent Peoples' Tribunal on Industrial Hazards and Human Rights took place in Bhopal, and in 1996, the ""Charter on Industrial Hazards and Human Rights"" was adopted.In 1994, the International Medical Commission on Bhopal (IMCB) met in Bhopal. Their work contributed to long term health effects being officially recognised.Important international actions have been the tour to Europe and United States in 2003, the marches to Delhi in 2006 and 2008, all including hunger strikes, and the Bhopal Europe Bus Tour in 2009.At least 14 different NGOs were immediately engaged. The first disaster reports were published by activist organisations, Eklavya and the Delhi Science Forum.Around ten local organisations, engaged on long term, have been identified. Two of the most active organisations are the women's organisations—Bhopal Gas Peedit Mahila-Stationery Karmachari Sangh and Bhopal Gas Peedit Mahila Udyog Sangthan.More than 15 national organisations have been engaged along with a number of international organisations.Some of the most important organisations are:International Campaign for Justice in Bhopal (ICJB), coordinates international activities.Bhopal Medical Appeal, collects funds for the Sambhavna Trust.Sambhavna Trust or Bhopal People's Health and Documentation Clinic. Provides medical care for gas affected patients and those living in water-contaminated area.Chingari Trust, provides medical care for children being born in Bhopal with malformations and brain damages.Students for Bhopal, based in USA.International Medical Commission on Bhopal, provided medical information 1994–2000.On 3 December 2004, the twentieth anniversary of the disaster, a man falsely claiming to be a Dow representative named Jude Finisterra was interviewed on BBC World News. He claimed that the company had agreed to clean up the site and compensate those harmed in the incident, by liquidating Union Carbide for US$12 billion. Dow quickly issued a statement saying that they had no employee by that name—that he was an impostor, not affiliated with Dow, and that his claims were a hoax. The BBC later broadcast a correction and an apology.Jude Finisterra was actually Andy Bichlbaum, a member of the activist prankster group The Yes Men. In 2002, The Yes Men issued a fake press release explaining why Dow refused to take responsibility for the disaster and started up a website, at ""DowEthics.com"", designed to look like the real Dow website, but containing hoax information.The release of an email cache related to intelligence research organisation Stratfor was leaked by WikiLeaks on 27 February 2012. It revealed that Dow Chemical had engaged Stratfor to spy on the public and personal lives of activists involved in the Bhopal disaster, including the Yes Men. E-mails to Dow representatives from hired security analysts list the YouTube videos liked, Twitter and Facebook posts made and the public appearances of these activists. Journalists, film-makers and authors who were investigating Bhopal and covering the issue of ongoing contamination, such as Jack Laurenson and Max Carlson, were also placed under surveillance. Stratfor released a statement condemning the revelation by Wikileaks while neither confirming nor denying the accuracy of the reports, and would only state that it had acted within the bounds of the law. Dow Chemical also refrained to comment on the matter.Ingrid Eckerman, a member of the International Medical Commission on Bhopal, has been denied a visa to visit India.List of industrial disastersSystem accidentEnvironmental racismEnvironmental racism in EuropeBroughton E (10 May 2005). ""The Bhopal disaster and its aftermath: a review"". Environmental Health. 4 (1): 6 pages. doi:10.1186/1476-069X-4-6. PMC 1142333 . PMID 15882472. Carbon monoxide, Phosgene and Methyl isocyanate. Unit Safety Procedures Manual.  Union Carbide India Limited, Agricultural Products Division: Bhopal (1978)Cassels, J (1993). The Uncertain Promise of Law: Lessons From Bhopal. University of Toronto Press. Chouhan TR,  et al. (2004) [1994]. Bhopal: the Inside Story — Carbide Workers Speak Out on the World's Worst Industrial Disaster. US and India: The Apex Press and Other India Press. ISBN 1-891843-30-3 and ISBN 81-85569-65-7.  Main author Chouhan was an operator at the plant. Contains many technical details.Chouhan TR (2005). ""The Unfolding of Bhopal Disaster"". Journal of Loss Prevention in the process industry. 18 (4–6): 205–208. doi:10.1016/j.jlp.2005.07.025. Dhara VR, Gassert TH (September 2005). ""The Bhopal gas tragedy: Evidence for cyanide poisoning not convincing"" (PDF). Current Science. 89 (6): 923–5. Archived from the original (PDF) on 30 October 2008. D'Silva T (2006). The Black Box of Bhopal: A Closer Look at the World's Deadliest Industrial Disaster. Victoria, B.C.: Trafford. ISBN 1-4120-8412-1.  Review Written by a retired former employee of UCC who was a member of the investigation committee. Includes several original documents including correspondence between UCIL and the Ministries of the Government of India.Eckerman I (2001). Chemical Industry and Public Health—Bhopal as an example (PDF).  Essay for MPH. A short overview, 57 pages, 82 references.Eckerman I (2005). The Bhopal Saga—Causes and Consequences of the World's Largest Industrial Disaster. India: Universities Press. ISBN 81-7371-515-7.  Preview Google books All known facts 1960s – 2003, systematised and analysed. 283 pages, over 200 references.Eckerman I (2006). ""The Bhopal Disaster 1984 – working conditions and the role of the trade unions"" (PDF). Asian Pacific Newsletter on occupational health and safety. 13 (2). Archived from the original (PDF) on 16 July 2011. Eckerman I (2011). ""Bhopal Gas Catastrophy 1984: Causes and consequences"".  In Nriagu JO. Encyclopedia of Environmental Health. 1. Burlington: Elsevier. pp. 302–316. doi:10.1016/B978-0-444-52272-6.00359-7. ISBN 978-0-444-52272-6. Eckerman I (2013). ""Bhopal Gas Catastrophe 1984: Causes and Consequences"". Reference Module in Earth Systems and Environmental Sciences. Elsevier. ISBN 978-0-12-409548-9. Gassert TH, Dhara VR (September 2005). ""Debate on cyanide poisoning in Bhopal victims"" (PDF). Current Science. 89 (6). Archived from the original (PDF) on 30 October 2008. Hanna B, Morehouse W, Sarangi S (2005). The Bhopal Reader. Remembering Twenty Years of the World's Worst Industrial Disaster. US: The Apex Press.  ISBN 1-891843-32-X USA, ISBN 81-85569-70-3 India. Reprinting and annotating landmark writing from across the years.Johnson S, Sahu R, Jadon N, Duca C (2009). Contamination of soil and water inside and outside the Union Carbide India Limited, Bhopal. New Delhi: Centre for Science and Environment.  In Down to EarthKalelkar AS, Little AD (1998). Investigation of Large-magnitude incidents: Bhopal as a Case Study (PDF). Archived from the original (PDF) on 30 October 2008.  London: The Institution of Chemical Engineers Conference on Preventing Major Chemical AccidentsKovel J (2002). The Enemy of Nature: The End of Capitalism or the End of the World?. London: Zed Books. ISBN 978-1-55266-255-7. Kulling P, Lorin H (1987). The Toxic Gas Disaster in Bhopal December 2–3, 1984. Stockholm: National Defence Research Institute.  [In Swedish]Kurzman, D. (1987). A Killing Wind: Inside Union Carbide and the Bhopal Catastrophe. New York: McGraw-Hill.Labunska I, Stephenson A, Brigden K, Stringer R, Santillo D, Johnston PA (1999). The Bhopal Legacy. Toxic contaminants at the former Union Carbide factory site, Bhopal, India: 15 years after the Bhopal accident (PDF). Archived from the original (PDF) on 30 October 2008. Greenpeace Research Laboratories, Department of Biological Sciences, University of Exeter, Exeter UKLepowski W (19 December 1994). ""Ten Years Later: Bhopal"". Chemical and Engineering News. Methyl Isocyanate. Union Carbide F-41443A – 7/76. New York: Union Carbide Corporation. 1976. Operating Manual Part II. Methyl Isocyanate Unit. Union Carbide India Limited, Agricultural Products Division (1979). Ranjan N, Sarangi S, Padmanabhan VT, Holleran S, Ramakrishnan R, Varma DR (2003). ""Methyl Isocyanate Exposure and Growth Patterns of Adolescents in Bhopal Methyl Isocyanate Exposure and Growth Patterns of Adolescents in Bhopal"". JAMA. 290 (14): 1856–7. doi:10.1001/jama.290.14.1856. PMID 14532313. Sriramachari S (2004). ""The Bhopal gas tragedy: An environmental disaster"" (PDF). Current Science. 86: 905–920. Archived from the original (PDF) on 30 October 2008. Stringer R, Labunska I, Brigden K, Santillo D (2003). Chemical Stockpiles at Union Carbide India Limited in Bhopal: An investigation (Technical Note 12/2002) (PDF). Greenpeace Research Laboratories. Shrishti (2002). Toxic present—toxic future. A report on Human and Environmental Chemical Contamination around the Bhopal disaster site. Delhi: The Other Media. Varadarajan S,  et al. (1985). Report on Scientific Studies on the Factors Related to Bhopal Toxic Gas Leakage. New Delhi: Indian Council of Scientific and Industrial Research.  https://bhopalgasdisaster.files.wordpress.com/2014/12/csir-report-on-scientific-studies-december-1985.pdf>Weir D (1987). The Bhopal Syndrome: Pesticides, Environment and Health. San Francisco: Sierra Club Books. ISBN 0-87156-718-0. Lapierre D, Moro J (2009). Five Past Midnight in Bhopal: The Epic Story of the World's Deadliest Industrial Disaster. Hachette Digital, Inc. ISBN 9780446561242. ""Review 'Bhopal: A Prayer for Rain' an effective cautionary tale"". See also http://www.pressreader.com/india/hindustan-times-st-indore/20160721/281603829819042Methyl Isocyanate. Union Carbide F-41443A – 7/76. Union Carbide Corporation, New York (1976)Carbon monoxide, Phosgene and Methyl isocyanate. Unit Safety Procedures Manual. Union Carbide India Limited, Agricultural Products Division: Bhopal (1978)Operating Manual Part II. Methyl Isocyanate Unit. Union Carbide India Limited, Agricultural Products Division (1979).Bhopal Methyl Isocyanate Incident. Investigation Team Report. Union Carbide Corporation, Danbury, CT (1985).Presence of Toxic Ingredients in Soil/Water Samples Inside Plant Premises. Union Carbide Corporation, US (1989).International Campaign for Justice in BhopalBhopal Medical AppealBhopal Gas Tragedy Relief & Rehabilitation Department at the Government of Madhya PradeshBhopal Information Center, Union CarbideIndia Environmental Portal Updated news on Bhopal Gas DisasterBhopal:Anatomy of a Crisis Paul Shrivastava, Paul Chapman Publishing, 1987, ISBN 1-85396-192-2";environmental disaster;Bhopal disaster;0
"Loss of biodiversity or biodiversity loss is the extinction of species (human, plant or animal) worldwide, and also the local reduction or loss of species in a certain habitat.The latter phenomenon can be temporary or permanent, depending on whether the environmental degradation that leads to the loss is reversible through ecological restoration / ecological resilience or effectively permanent (e.g. through land loss). Global extinction has so far been proven to be irreversible.Even though permanent global species loss is a more dramatic phenomenon than regional changes in species composition, even minor changes from a healthy stable state can have dramatic influence on the food web and the food chain insofar as reductions in only one species can adversely affect the entire chain (coextinction), leading to an overall reduction in biodiversity, possible alternative stable states of an ecosystem notwithstanding. Ecological effects of biodiversity are usually counteracted by its loss. Reduced biodiversity in particular leads to reduced ecosystem services and eventually poses an immediate danger for food security, also for humankind.The current rate of global diversity loss is estimated to be 100 to 1000 times higher than the (naturally occurring) background extinction rate and expected to still grow in the upcoming years.Locally bounded loss rates can be measured using species richness and its variation over time. Raw counts may not be as ecologically relevant as relative or absolute abundances. Taking into account the relative frequencies, a considerable number of biodiversity indexes has been developed. Besides richness, evenness and heterogeneity are considered to be the main dimensions along which diversity can be measured.As with all diversity measures, it is essential to accurately classify the spatial and temporal scope of the observation. ""Definitions tend to become less precise as the complexityof the subject increases and the associated spatial and temporal scales widen. Biodiversity itself is not a single concept but can be split up into various scales (e.g. ecosystem diversity vs. habitat diversity or even biodiversity vs. habitat d.) or different subcategories (e.g. phylogenetic diversity, species diversity, genetic diversity, nucleotide diversity). The question of net loss in confined regions is often a matter of debate but longer observation times are generally thought to be beneficial to loss estimates.To compare rates between different geographic regions latitudinal gradients in species diversity should also be considered.Major factors for biotic stress and the ensuing accelerating loss rate are, amongst other threats:Habitat loss and degradationLand use intensification (and ensuing land loss/habitat loss) has been identified to be a significant factor in loss of ecological services due to direct effects as well as biodiversity loss.Climate change through heat stress and drought stressExcessive nutrient load and other forms of pollutionOver-exploitation and unsustainable use (e.g. unsustainable fishing methods) we are currently using 25% more natural resources than the planetArmed conflict, which disrupts human livelihoods and institutions, contributes to habitat loss, and intensifies over-exploitation of economically valuable species, leading to population declines and local extinctions.Invasive alien species that effectively compete for a niche, replacing indigenous speciesIn 2017, various publications describe the dramatic reduction in absolute insect biomass and number of species in Germany and North America over a period of 27 years. As possible reasons for the decline, the authors highlight neonicotinoids and other agrochemicals. Writing in the journal PLOS One, authors Hallman, Sorg, et al (2017), conclude that ""the widespread insect biomass decline is alarming.""Anup Shah (2014). ""Loss of Biodiversity and Extinctions"". globalissues.org. ""How does Biodiversity loss affect me and everyone else?"". panda.org. ""TOPICS IN BIODIVERSITY LOSS"". Global Change Project of the Paleontological Research Institution. Worm, B. Barbier, E. B. Beaumont, N. Duffy, J. E. Folke, C. Halpern, B. S. Jackson, J. B. C. Lotze, H. K. Micheli, F. Palumbi, S. R. Sala, E. Selkoe, K. A. Stachowicz, J. J. Watson, R. (2006-11-03). ""Impacts of Biodiversity Loss on Ocean Ecosystem Services"". Science. American Association for the Advancement of Science (AAAS). 314 (5800): 787–790. doi:10.1126/science.1132294. ISSN 0036-8075. Waldron, Anthony Miller, Daniel C. Redding, Dave Mooers, Arne Kuhn, Tyler S. Nibbelink, Nate Roberts, J. Timmons Tobias, Joseph A. Gittleman, John L. (2017-10-25). ""Reductions in global biodiversity loss predicted from conservation spending"". Nature. Springer Nature. 551 (7680): 364–367. doi:10.1038/nature24295. ISSN 0028-0836. Charles Perrings (2008). Biodiversity Loss: Economic and Ecological Issues. Cambridge University Press. ISBN 978-0521588669. Neil Griffin, ed. (2015). Biodiversity Loss in the 21st Century. Ml Books International - Ips. ISBN 978-1632390943. Alexander Wood (2000). The Root Causes of Biodiversity Loss. Routledge. ISBN 978-1853836992. ""Forests, desertification and biodiversity"". United Nations Sustainable Development. Retrieved 2018-03-05. ""Climate Change and Biodiversity Loss"". Center for Health and the Global Environment. 2017-07-19. Retrieved 2018-03-05.";environmental disaster;Biodiversity crisis;0
"Loss of biodiversity or biodiversity loss is the extinction of species (human, plant or animal) worldwide, and also the local reduction or loss of species in a certain habitat.The latter phenomenon can be temporary or permanent, depending on whether the environmental degradation that leads to the loss is reversible through ecological restoration / ecological resilience or effectively permanent (e.g. through land loss). Global extinction has so far been proven to be irreversible.Even though permanent global species loss is a more dramatic phenomenon than regional changes in species composition, even minor changes from a healthy stable state can have dramatic influence on the food web and the food chain insofar as reductions in only one species can adversely affect the entire chain (coextinction), leading to an overall reduction in biodiversity, possible alternative stable states of an ecosystem notwithstanding. Ecological effects of biodiversity are usually counteracted by its loss. Reduced biodiversity in particular leads to reduced ecosystem services and eventually poses an immediate danger for food security, also for humankind.The current rate of global diversity loss is estimated to be 100 to 1000 times higher than the (naturally occurring) background extinction rate and expected to still grow in the upcoming years.Locally bounded loss rates can be measured using species richness and its variation over time. Raw counts may not be as ecologically relevant as relative or absolute abundances. Taking into account the relative frequencies, a considerable number of biodiversity indexes has been developed. Besides richness, evenness and heterogeneity are considered to be the main dimensions along which diversity can be measured.As with all diversity measures, it is essential to accurately classify the spatial and temporal scope of the observation. ""Definitions tend to become less precise as the complexityof the subject increases and the associated spatial and temporal scales widen. Biodiversity itself is not a single concept but can be split up into various scales (e.g. ecosystem diversity vs. habitat diversity or even biodiversity vs. habitat d.) or different subcategories (e.g. phylogenetic diversity, species diversity, genetic diversity, nucleotide diversity). The question of net loss in confined regions is often a matter of debate but longer observation times are generally thought to be beneficial to loss estimates.To compare rates between different geographic regions latitudinal gradients in species diversity should also be considered.Major factors for biotic stress and the ensuing accelerating loss rate are, amongst other threats:Habitat loss and degradationLand use intensification (and ensuing land loss/habitat loss) has been identified to be a significant factor in loss of ecological services due to direct effects as well as biodiversity loss.Climate change through heat stress and drought stressExcessive nutrient load and other forms of pollutionOver-exploitation and unsustainable use (e.g. unsustainable fishing methods) we are currently using 25% more natural resources than the planetArmed conflict, which disrupts human livelihoods and institutions, contributes to habitat loss, and intensifies over-exploitation of economically valuable species, leading to population declines and local extinctions.Invasive alien species that effectively compete for a niche, replacing indigenous speciesIn 2017, various publications describe the dramatic reduction in absolute insect biomass and number of species in Germany and North America over a period of 27 years. As possible reasons for the decline, the authors highlight neonicotinoids and other agrochemicals. Writing in the journal PLOS One, authors Hallman, Sorg, et al (2017), conclude that ""the widespread insect biomass decline is alarming.""Anup Shah (2014). ""Loss of Biodiversity and Extinctions"". globalissues.org. ""How does Biodiversity loss affect me and everyone else?"". panda.org. ""TOPICS IN BIODIVERSITY LOSS"". Global Change Project of the Paleontological Research Institution. Worm, B. Barbier, E. B. Beaumont, N. Duffy, J. E. Folke, C. Halpern, B. S. Jackson, J. B. C. Lotze, H. K. Micheli, F. Palumbi, S. R. Sala, E. Selkoe, K. A. Stachowicz, J. J. Watson, R. (2006-11-03). ""Impacts of Biodiversity Loss on Ocean Ecosystem Services"". Science. American Association for the Advancement of Science (AAAS). 314 (5800): 787–790. doi:10.1126/science.1132294. ISSN 0036-8075. Waldron, Anthony Miller, Daniel C. Redding, Dave Mooers, Arne Kuhn, Tyler S. Nibbelink, Nate Roberts, J. Timmons Tobias, Joseph A. Gittleman, John L. (2017-10-25). ""Reductions in global biodiversity loss predicted from conservation spending"". Nature. Springer Nature. 551 (7680): 364–367. doi:10.1038/nature24295. ISSN 0028-0836. Charles Perrings (2008). Biodiversity Loss: Economic and Ecological Issues. Cambridge University Press. ISBN 978-0521588669. Neil Griffin, ed. (2015). Biodiversity Loss in the 21st Century. Ml Books International - Ips. ISBN 978-1632390943. Alexander Wood (2000). The Root Causes of Biodiversity Loss. Routledge. ISBN 978-1853836992. ""Forests, desertification and biodiversity"". United Nations Sustainable Development. Retrieved 2018-03-05. ""Climate Change and Biodiversity Loss"". Center for Health and the Global Environment. 2017-07-19. Retrieved 2018-03-05.";environmental disaster;Biodiversity loss;0
"The Chernobyl disaster, also referred to as the Chernobyl accident, was a catastrophic nuclear accident.  It occurred on 25–26 April 1986 in the No. 4 light water graphite moderated reactor at the Chernobyl Nuclear Power Plant near the now-abandoned town of Pripyat, in northern Ukrainian Soviet Socialist Republic, Soviet Union, approximately 104 km (65 mi) north of Kiev.The event occurred during a late-night safety test which simulated a station blackout power-failure, in the course of which safety systems were intentionally turned off. A combination of inherent reactor design flaws and the reactor operators arranging the core in a manner contrary to the checklist for the test, eventually resulted in uncontrolled reaction conditions. Water flashed into steam generating a destructive steam explosion and a subsequent open-air graphite fire. This fire produced considerable updrafts for about nine days. These lofted plumes of fission products into the atmosphere. The estimated radioactive inventory that was released during this very hot fire phase approximately equaled in magnitude the airborne fission products released in the initial destructive explosion. This radioactive material precipitated onto parts of the western USSR and Europe.During the accident, steam-blast effects caused two deaths within the facility one immediately after the explosion, and the other, compounded by a lethal dose of radiation. Over the coming days and weeks, 134 servicemen were hospitalized with acute radiation sickness(ARS), of which 28 firemen and employees died in the days-to-months afterward from the radiation effects. In addition, approximately fourteen radiation induced cancer deaths among this group of 134 hospitalized survivors, were to follow within the next ten years (1996). Among the wider population, an excess of 15 childhood thyroid cancer deaths were documented as of 2011. It will take further time and investigation to definitively determine the elevated relative risk of cancer among the surviving employees, those that were initially hospitalized with ARS and the population at large.The Chernobyl accident is considered the most disastrous nuclear power plant accident in history, both in terms of cost and casualties. It is one of only two nuclear energy accidents classified as a level 7 event (the maximum classification) on the International Nuclear Event Scale, the other being the Fukushima Daiichi nuclear disaster in Japan in 2011. The struggle to safeguard against scenarios which were perceived as having the potential for greater catastrophe, together with later decontamination efforts of the surroundings, ultimately involved over 500,000 workers and cost an estimated 18 billion rubles.The remains of the No. 4 reactor building were enclosed in a large cover which was named the ""Object Shelter"", often known as the sarcophagus. The purpose of the structure was to reduce the spread of the remaining radioactive dust and debris from the wreckage and the protection of the wreckage from further weathering. The sarcophagus was finished in December 1986 at a time when what was left of the reactor was entering the cold shut-down phase. The enclosure was not intended as a radiation shield, but was built quickly as occupational safety for the crews of the other undamaged reactors at the power station, with No. 3 continuing to produce electricity up into 2000.The accident motivated safety upgrades on all remaining Soviet-designed reactors in the RBMK (Chernobyl No. 4) family, of which eleven continued to power electric grids as of 2013.The disaster began during a systems test on 26 April 1986 at reactor 4 of the Chernobyl plant near Pripyat and in proximity to the administrative border with Belarus and the Dnieper River.  There was a sudden and unexpected power surge. When operators attempted an emergency shutdown, a much larger spike in power output occurred. This second spike led to a reactor vessel rupture and a series of steam explosions. These events exposed the graphite moderator of the reactor to air, causing it to ignite. For the next week, the resulting fire sent long plumes of highly radioactive fallout into the atmosphere over an extensive geographical area, including Pripyat. The plumes drifted over large parts of the western Soviet Union and Europe.  According to official post-Soviet data, about 60% of the fallout landed in Belarus.Thirty-six hours after the accident, Soviet officials enacted a 10-kilometre exclusion zone, which resulted in the rapid evacuation of 49,000 people primarily from Pripyat, the nearest large population centre.  Although not communicated at the time, an immediate evacuation of the town following the accident was not advisable as the road leading out of the town had heavy nuclear fallout hotspots deposited on it. Initially, the town itself was comparatively safe due to the favourable wind direction. Until the winds began to change direction, shelter in place was considered the best safety measure for the town.During the accident the wind changed direction, the different plumes from the reactor did have different ratios of radioisotopes in them indicates that the relative release rates of different elements from the accident site was changing.As plumes and subsequent fallout continued to be generated, the evacuation zone was increased from 10 to 30 km about one week after the accident. A further 68,000 persons were evacuated, including from the town of Chernobyl itself. The surveying and detection of isolated fallout hotspots outside this zone over the following year eventually resulted in 135,000 long-term evacuees in total agreeing to be moved. The near tripling in the total number of permanently resettled persons between 1986 and 2000 from the most severely contaminated areas to approximately 350,000  is regarded as largely political in nature, with the majority of the rest evacuated in an effort to redeem loss in trust in the government, which was most common around 1990. Many thousands of these evacuees would have been ""better off staying home."" Risk analysis in 2007, supported by DNA biomarkers, has determined that the ""people still living unofficially in the abandoned lands around Chernobyl"" have a lower risk of dying as a result of the elevated doses of radiation in the rural areas than ""if they were exposed to the air pollution health risk in a large city such as nearby Kiev.""In 2017 Philip Thomas, Professor of Risk Management at the University of Bristol used the years of potential life lost metric to conclude that, ""Relocation was unjustified for 75% of the 335,000 people relocated after Chernobyl"", finding that just 900 people within the 220,000 relocated during the second evacuation would have lost 3 months' of life expectancy by staying home and that, ""none should have been asked to leave"". For comparison, Thomas found that the average resident of London, a city of ~8 million, loses 4.5 months of life due to air pollution.Russia, Ukraine, and Belarus have been burdened with the continuing and substantial decontamination and monthly compensation costs  of the Chernobyl accident. Although certain initiatives are legitimate, as the director of the UN Development Program Kalman Mizsei noted, “an industry has been built on this unfortunate event,” with a “vast interest in creating a false picture.”The accident raised the already heightened concerns about fission reactors worldwide, and while most concern was focused on those of the same unusual design, hundreds of disparate electric-power reactor proposals, including those under construction at Chernobyl, reactor No.5 and 6, were eventually cancelled. With the worldwide issue generally being due to the ballooning in costs for new nuclear reactor safety system standards and the legal costs in dealing with the increasingly hostile/anxious public opinion. There was a precipitous drop in the prior rate of new startups after 1986.The accident also raised concerns about the cavalier safety culture in the Soviet nuclear power industry, slowing industry growth and forcing the Soviet government to become less secretive about its procedures. The government coverup of the Chernobyl disaster was a catalyst for glasnost, which ""paved the way for reforms leading to the Soviet collapse"".A report by the International Atomic Energy Agency examines the environmental consequences of the accident. The United Nations Scientific Committee on the Effects of Atomic Radiation has estimated a global collective dose of radiation exposure from the accident ""equivalent on average to 21 additional days of world exposure to natural background radiation"" individual doses were far higher than the global mean among those most exposed, including 530,000 primarily male recovery workers (the Chernobyl liquidators) who averaged an effective dose equivalent to an extra 50 years of typical natural background radiation exposure each.Estimates of the number of deaths that will eventually result from the accident vary enormously disparities reflect both the lack of solid scientific data and the different methodologies used to quantify mortality—whether the discussion is confined to specific geographical areas or extends worldwide, and whether the deaths are immediate, short term, or long term.In 1994, thirty-one deaths were directly attributed to the accident, all among the reactor staff and emergency workers. As of the 2008 report by the United Nations Scientific Committee on the Effects of Atomic Radiation, and the total number of confirmed deaths from radiation was 64 and was expected to continue to rise.The Chernobyl Forum predicts that the eventual death toll could reach 4,000 among those exposed to the highest levels of radiation (200,000 emergency workers, 116,000 evacuees and 270,000 residents of the most contaminated areas) this figure is a total causal death toll prediction, combining the deaths of approximately 50 emergency workers who died soon after the accident from acute radiation syndrome, 15 children who have died of thyroid cancer and a future predicted total of 3935 deaths from radiation-induced cancer and leukaemia.In a peer-reviewed paper in the International Journal of Cancer in 2006, the authors expanded the discussion on those exposed to all of Europe (but following a different conclusion methodology to the Chernobyl Forum study, which arrived at the total predicted death toll of 4,000 after cancer survival rates were factored in) they stated, without entering into a discussion on deaths, that in terms of total excess cancers attributed to the accident:The risk projections suggest that by now [2006] Chernobyl may have caused about 1000 cases of thyroid cancer and 4000 cases of other cancers in Europe, representing about 0.01% of all incident cancers since the accident. Models predict that by 2065 about 16,000 cases of thyroid cancer and 25,000 cases of other cancers may be expected due to radiation from the accident, whereas several hundred million cancer cases are expected from other causes.Two anti-nuclear advocacy groups have publicized non-peer-reviewed estimates that include mortality estimates for those who were exposed to even smaller amounts of radiation. The Union of Concerned Scientists (UCS) calculated that, among the hundreds of millions of people exposed worldwide, there will be an eventual 50,000 excess cancer cases, resulting in 25,000 excess cancer deaths, excluding thyroid cancer. However, these calculations are based on a simple linear no-threshold model multiplication and the misapplication of the collective dose, which the International Commission on Radiological Protection (ICRP) states ""should not be done"" as using the collective dose is ""inappropriate to use in risk projections"".Along similar lines to the UCS approach, the 2006 TORCH report, commissioned by the European Greens political party, likewise simplistically calculates an eventual 30,000 to 60,000 excess cancer deaths in total, around the globe.The Russian founder of that region's chapter of Greenpeace authored a book titled Chernobyl: Consequences of the Catastrophe for People and the Environment, which suggests that among the billions of people worldwide who were exposed to radioactive contamination from the disaster, nearly a million premature cancer deaths occurred between 1986 and 2004. Greenpeace itself advocates a figure of at least 200,000 or more. The book was not peer reviewed prior to its publication, and it has been heavily criticized of the five reviews published in the academic press, four considered the book severely flawed and contradictory, and one praised it while noting some shortcomings. The review by M. I. Balonov published by the New York Academy of Sciences concludes that the report is of negative value because it has very little scientific merit while being highly misleading to the lay reader. It characterized the estimate of nearly a million deaths as more in the realm of science fiction than science.On 26 April 1986, at 01:23 (UTC+3), reactor four suffered a catastrophic power increase, leading to explosions in its core. As the reactor had not been encased by any kind of hard containment vessel, this dispersed large quantities of radioactive isotopes into the atmosphere and caused an open-air fire that increased the emission of radioactive particles carried by the smoke. The accident occurred during an experiment scheduled to test the viability of a potential safety emergency core cooling feature, which required a normal reactor shutdown procedure.In steady state operation, a significant fraction (over 6%) of the power from a nuclear reactor is derived not from fission but from the decay heat of its accumulated fission products. This heat continues for some time after the chain reaction is stopped (e.g., following an emergency SCRAM) and active cooling may be required to prevent core damage. RBMK reactors like those at Chernobyl use water as a coolant. Reactor 4 at Chernobyl consisted of about 1,600 individual fuel channels, each of which required coolant flow of 28 metric tons (28,000 litres or 7,400 US gallons) per hour.Since cooling pumps require electricity to cool a reactor after a SCRAM, in the event of a power grid failure, Chernobyl's reactors had three backup diesel generators these could start up in 15 seconds, but took 60–75 seconds to attain full speed and reach the 5.5‑megawatt (MW) output required to run one main pump.To solve this one-minute gap – considered an unacceptable safety risk – it had been theorized that rotational energy from the steam turbine (as it wound down under residual steam pressure) could be used to generate the required electrical power. Analysis indicated that this residual momentum and steam pressure might be sufficient to run the coolant pumps for 45 seconds, bridging the gap between an external power failure and the full availability of the emergency generators.This capability still needed to be confirmed experimentally, and previous tests had ended unsuccessfully. An initial test carried out in 1982 indicated that the excitation voltage of the turbine-generator was insufficient it did not maintain the desired magnetic field after the turbine trip. The system was modified, and the test was repeated in 1984 but again proved unsuccessful. In 1985, the tests were attempted a third time but also yielded negative results. The test procedure would be repeated in 1986, and it was scheduled to take place during the maintenance shutdown of Reactor Four.The test focused on the switching sequences of the electrical supplies for the reactor. The test procedure was expected to begin with an automatic emergency shutdown. No detrimental effect on the safety of the reactor was anticipated, so the test programme was not formally coordinated with either the chief designer of the reactor (NIKIET) or the scientific manager. Instead, it was approved only by the director of the plant (and even this approval was not consistent with established procedures).According to the test parameters, the thermal output of the reactor should have been no lower than 700 MW at the start of the experiment. If test conditions had been as planned, the procedure would almost certainly have been carried out safely the eventual disaster resulted from attempts to boost the reactor output once the experiment had been started, which was inconsistent with approved procedure.The Chernobyl power plant had been in operation for two years without the capability to ride through the first 60–75 seconds of a total loss of electric power, and thus lacked an important safety feature. The station managers presumably wished to correct this at the first opportunity, which may explain why they continued the test even when serious problems arose, and why the requisite approval for the test had not been sought from the Soviet nuclear oversight regulator (even though there was a representative at the complex of 4 reactors).The experimental procedure was intended to run as follows:The reactor was to be running at a low power level, between 700 MW and 800 MW.The steam-turbine generator was to be run up to full speed.When these conditions were achieved, the steam supply for the turbine generator was to be closed off.Turbine generator performance was to be recorded to determine whether it could provide the bridging power for coolant pumps until the emergency diesel generators were sequenced to start and provide power to the cooling pumps automatically.After the emergency generators reached normal operating speed and voltage, the turbine generator would be allowed to continue to freewheel down.The conditions to run the test were established before the day shift of 25 April 1986. The day-shift workers had been instructed in advance and were familiar with the established procedures. A special team of electrical engineers was present to test the new voltage regulating system. As planned, a gradual reduction in the output of the power unit was begun at 01:06 on 25 April, and the power level had reached 50% of its nominal 3200 MW thermal level by the beginning of the day shift.At this point, another regional power station unexpectedly went offline, and the Kiev electrical grid controller requested that the further reduction of Chernobyl's output be postponed, as power was needed to satisfy the peak evening demand. The Chernobyl plant director agreed, and postponed the test. Despite this delay, preparations for the test not affecting the reactor's power were carried out, including the disabling of the emergency core cooling system or ECCS, a passive/active system of core cooling intended to provide water to the core in a loss-of-coolant accident. Given the other events that unfolded, the system would have been of limited use, but its disabling as a ""routine"" step of the test is an illustration of the inherent lack of attention to safety for this test. In addition, had the reactor been shut down for the day as planned, it is possible that more preparation would have been taken in advance of the test.At 23:04, the Kiev grid controller allowed the reactor shutdown to resume. This delay had some serious consequences: the day shift had long since departed, the evening shift was also preparing to leave, and the night shift would not take over until midnight, well into the job. According to plan, the test should have been finished during the day shift, and the night shift would only have had to maintain decay heat cooling systems in an otherwise shut-down plant.The night shift had very limited time to prepare for and carry out the experiment. A further rapid decrease in the power level from 50% was executed during the shift change-over. Alexander Akimov was chief of the night shift, and Leonid Toptunov was the operator responsible for the reactor's operational regimen, including the movement of the control rods. Toptunov was a young engineer who had worked independently as a senior engineer for approximately three months.The test plan called for a gradual decrease in power output from reactor 4 to a thermal level of 700–1000 MW. An output of 700 MW was reached at 00:05 on 26 April. Due to the reactor's production of  a fission byproduct, xenon-135, which is a reaction-inhibiting neutron absorber, core power continued to decrease without further operator action—a process known as reactor poisoning. This continuing decrease in power occurred because in steady state operation, xenon-135 is ""burned off"" as quickly as it is created from decaying iodine-135 by absorbing neutrons from the ongoing chain reaction to become highly stable xenon-136. When the reactor power was lowered, previously produced high quantities of iodine-135 decayed into the neutron-absorbing xenon-135 faster than the reduced neutron flux could burn it off. As the reactor power output dropped further, to approximately 500 MW, Toptunov mistakenly inserted the control rods too far—the exact circumstances leading to this are unknown because Akimov and Toptunov both died in the hospital on 10 and 14 May respectively. This combination of factors put the reactor into an unintended near-shutdown state, with a power output of 30 MW thermal or less.The reactor was now producing 5 percent of the minimum initial power level established as safe for the test. Control-room personnel decided to restore power by disabling the automatic system governing the control rods and manually extracting the majority of the reactor control rods to their upper limits. Several minutes elapsed between their extraction and the point that the power output began to increase and subsequently stabilize at 160–200 MW (thermal), a much smaller value than the planned 700 MW. The rapid reduction in the power during the initial shutdown, and the subsequent operation at a level of less than 200 MW led to increased poisoning of the reactor core by the accumulation of xenon-135. This restricted any further rise of reactor power, and made it necessary to extract additional control rods from the reactor core in order to counteract the poisoning.The operation of the reactor at the low power level and high poisoning level was accompanied by unstable core temperature and coolant flow, and possibly by instability of neutron flux, which triggered alarms. The control room received repeated emergency signals regarding the levels in the steam/water separator drums, and large excursions or variations in the flow rate of feed water, as well as from relief valves opened to relieve excess steam into a turbine condenser, and from the neutron power controller. Between 00:35 and 00:45, emergency alarm signals concerning thermal-hydraulic parameters were ignored, apparently to preserve the reactor power level.When the power level of 200 MW was achieved, preparation for the experiment continued. As part of the test plan, extra water pumps were activated at 01:05 on 26 April, increasing the water flow. The increased coolant flow rate through the reactor produced an increase in the inlet coolant temperature of the reactor core (the coolant no longer having sufficient time to release its heat in the turbine and cooling towers), which now more closely approached the nucleate boiling temperature of water, reducing the safety margin.The flow exceeded the allowed limit at 01:19, triggering an alarm of low steam pressure in the steam separators. At the same time, the extra water flow lowered the overall core temperature and reduced the existing steam voids in the core and the steam separators. Since water weakly absorbs neutrons (and the higher density of liquid water makes it a better absorber than steam), turning on additional pumps decreased the reactor power further still. The crew responded by turning off two of the circulation pumps to reduce feedwater flow, in an effort to increase steam pressure, and by removing more manual control rods to maintain power.All these actions led to an extremely unstable reactor configuration. Nearly all of the control rods were removed manually, including all but 18 of the ""fail-safe"" manually operated rods of the minimal 28 which were intended to remain fully inserted to control the reactor even in the event of a loss of coolant, out of a total 211 control rods. While the emergency SCRAM system that would insert all control rods to shut down the reactor could still be activated manually (through the ""AZ-5"" button), the automated system that could do the same had been disabled to maintain the power level, and many other automated and even passive safety features of the reactor had been bypassed. Further, the reactor coolant pumping had been reduced, which had limited margin so any power excursion would produce boiling, thereby reducing neutron absorption by the water. The reactor was in an unstable configuration that was outside the safe operating envelope established by the designers. If anything pushed it into supercriticality, it was unable to recover automatically.At 1:23:04 a.m., the experiment began. Four of the main circulating pumps (MCP) were active of the eight total, six are normally active during regular operation. The steam to the turbines was shut off, beginning a run-down of the turbine generator. The diesel generators started and sequentially picked up loads the generators were to have completely picked up the MCPs' power needs by 01:23:43. In the interim, the power for the MCPs was to be supplied by the turbine generator as it coasted down. As the momentum of the turbine generator decreased, so did the power it produced for the pumps. The water flow rate decreased, leading to increased formation of steam voids (bubbles) in the core.Because of the positive void coefficient of the RBMK reactor at low reactor power levels, it was now primed to embark on a positive feedback loop, in which the formation of steam voids reduced the ability of the liquid water coolant to absorb neutrons, which in turn increased the reactor's power output. This caused yet more water to flash into steam, giving a further power increase. During almost the entire period of the experiment the automatic control system successfully counteracted this positive feedback, inserting control rods into the reactor core to limit the power rise. This system had control of only 12 rods, and nearly all others had been manually retracted.At 1:23:40, as recorded by the SKALA centralized control system, a SCRAM (emergency shutdown) of the reactor was initiated. The SCRAM was started when the EPS-5 button (also known as the AZ-5 button) of the reactor emergency protection system was pressed: this engaged the drive mechanism on all control rods to fully insert them, including the manual control rods that had been withdrawn earlier. The reason why the EPS-5 button was pressed is not known, whether it was done as an emergency measure in response to rising temperatures, or simply as a routine method of shutting down the reactor upon completion of the experiment.There is a view that the SCRAM may have been ordered as a response to the unexpected rapid power increase, although there is no recorded data proving this. Some have suggested that the button was not manually pressed, that the SCRAM signal was automatically produced by the emergency protection system, but the SKALA registered a manual SCRAM signal. Despite this, the question as to when or even whether the EPS-5 button was pressed has been the subject of debate. There have been assertions that the manual SCRAM was initiated due to the initial rapid power acceleration. Others have suggested that the button was not pressed until the reactor began to self-destruct, while others believe that it happened earlier and in calm conditions.In any case, when the EPS-5 button was pressed, the insertion of control rods into the reactor core began. The control rod insertion mechanism moved the rods at 0.4 m/s, so that the rods took 18 to 20 seconds to travel the full height of the core, about 7 metres. A bigger problem was the design of the RBMK control rods, each of which had a graphite neutron moderator rod attached to the end to boost reactor output by displacing water when the control rod section had been fully withdrawn from the reactor. Thus, when a control rod was at maximum extraction, a neutron-moderating graphite extension was centered in the core with a 1.25 m column of water above and below it. Therefore, injecting a control rod downward into the reactor during a SCRAM initially displaced (neutron-absorbing) water in the lower portion of the reactor with (neutron-moderating) graphite on its way out of the core. As a result, an emergency SCRAM initially increased the reaction rate in the lower part of the core as the graphite section of rods moving out of the reactor displaced water coolant. This behaviour was revealed when the initial insertion of control rods in another RBMK reactor at Ignalina Nuclear Power Plant in 1983 induced a power spike, but since the subsequent SCRAM of that reactor was successful, the information was disseminated but deemed of little importance.A few seconds into the SCRAM, a power spike occurred, and the core overheated, causing some of the fuel rods to fracture, blocking the control rod columns and jamming the control rods at one-third insertion, with the graphite displacers still in the lower part of the core. Within three seconds the reactor output rose above 530 MW.The subsequent course of events was not registered by instruments it is known only as a result of mathematical simulation. Apparently, the power spike caused an increase in fuel temperature and steam buildup, leading to a rapid increase in steam pressure. This caused the fuel cladding to fail, releasing the fuel elements into the coolant, and rupturing the channels in which these elements were located.Then, according to some estimations, the reactor jumped to around 30,000 MW thermal, ten times the normal operational output. The last reading on the control panel was 33,000 MW. It was not possible to reconstruct the precise sequence of the processes that led to the destruction of the reactor and the power unit building, but a steam explosion, like the explosion of a steam boiler from excess vapour pressure, appears to have been the next event. There is a general understanding that it was explosive steam pressure from the damaged fuel channels escaping into the reactor's exterior cooling structure that caused the explosion that destroyed the reactor casing, tearing off and blasting the upper plate, to which the entire reactor assembly is fastened, through the roof of the reactor building. This is believed to be the first explosion that many heard. This explosion ruptured further fuel channels, as well as severing most of the coolant lines feeding the reactor chamber, and as a result the remaining coolant flashed to steam and escaped the reactor core. The total water loss in combination with a high positive void coefficient further increased the reactor's thermal power.A second, more powerful explosion occurred about two or three seconds after the first this explosion dispersed the damaged core and effectively terminated the nuclear chain reaction. This explosion also compromised more of the reactor containment vessel and ejected hot lumps of graphite moderator. The ejected graphite and the demolished channels still in the remains of the reactor vessel caught fire on exposure to air, greatly contributing to the spread of radioactive fallout and the contamination of outlying areas.According to observers outside Unit 4, burning lumps of material and sparks shot into the air above the reactor. Some of them fell onto the roof of the machine hall and started a fire. About 25 percent of the red-hot graphite blocks and overheated material from the fuel channels was ejected. Parts of the graphite blocks and fuel channels were out of the reactor building. As a result of the damage to the building an airflow through the core was established by the high temperature of the core. The air ignited the hot graphite and started a graphite fire.After the larger explosion, a number of employees at the power station went outside to get a clearer view of the extent of the damage. One such survivor, Alexander Yuvchenko, recounts that once he stopped outside and looked up towards the reactor hall, he saw a ""very beautiful"" LASER-like beam of light bluish light caused by the ionization of air that appeared to ""flood up into infinity"".There were initially several hypotheses about the nature of the second explosion. One view was that the second explosion was caused by hydrogen, which had been produced either by the overheated steam-zirconium reaction or by the reaction of red-hot graphite with steam that produced hydrogen and carbon monoxide. Another hypothesis, by Checherov, published in 1998, was that the second explosion was a thermal explosion of the reactor as a result of the uncontrollable escape of fast neutrons caused by the complete water loss in the reactor core. A third hypothesis was that the second explosion was another steam explosion. According to this version, the first explosion was a more minor steam explosion in the circulating loop, causing a loss of coolant flow and pressure that in turn caused the water still in the core to flash to steam. This second explosion then did the majority of the damage to the reactor and containment building.The force of the second explosion and the ratio of xenon radioisotopes released after the accident (a vital tool in nuclear forensics) indicated to Yuri V. Dubasov in a 2009 publication (suggested before him by Checherov in 1998), that the second explosion could have been a nuclear power transient resulting from core material melting in the absence of its water coolant and moderator. Dubasov argues that the reactor did not simply undergo a runaway delayed-supercritical/exponential increase in power into the multi-gigawatt power range, which is somewhat similar to the conditions of a normal reactor coming up to its commercial power level (with the notable exception that Chernobyl's older RBMK reactor design had the largest positive void coefficient of reactivity of any reactor then operating commercially), permitting a dangerous ""positive feedback""/runaway condition, given the lack of inherent safety stops when power levels began to increase above the commercial level. Although a positive-feedback power excursion that increased until the reactor disassembled itself by means of its internal energy and external steam explosions is the more accepted explanation for the cause of the explosions, Dubasov argues instead that a runaway prompt supercriticality occurred, with the internal physics being more similar to the explosion of a fizzled nuclear weapon, and that this failed/fizzle event produced the second explosion.This nuclear fizzle hypothesis, then mostly defended by Dubasov, was examined further in 2017 by retired physicist Lars-Erik De Geer in an analysis that puts the hypothesized fizzle event as the more probable cause of the first explosion. The more energetic second explosion, which produced the majority of the damage, has been estimated by Dubasov in 2009 as equivalent to 40 billion joules of energy, the equivalent of about ten tons of TNT. Both the 2009 and 2017 analyses argue that the nuclear fizzle event, whether producing the second or first explosion, consisted of a prompt chain reaction (as opposed to the consensus delayed neutron mediated chain-reaction) that was limited to a small portion of the reactor core, since expected self-disassembly occurs rapidly in fizzle events.Contrary to safety regulations, bitumen, a combustible material, had been used in the construction of the roof of the reactor building and the turbine hall. Ejected material ignited at least five fires on the roof of the adjacent reactor 3, which was still operating. It was imperative to put those fires out and protect the cooling systems of reactor 3. Inside reactor 3, the chief of the night shift, Yuri Bagdasarov, wanted to shut down the reactor immediately, but chief engineer Nikolai Fomin would not allow this. The operators were given respirators and potassium iodide tablets and told to continue working. At 05:00, Bagdasarov made his own decision to shut down the reactor, leaving only those operators there who had to work the emergency cooling systems.Approximate radiation intensity levels at different locations at Chernobyl reactor site shortly after the explosion are shown in the below table. A dose of 500 roentgens (~5 Sv) delivered over an hour is usually lethal for human beings.Based on the image of the plantThe radiation levels in the worst-hit areas of the reactor building have been estimated to be 5.6 roentgens per second (R/s), equivalent to more than 20,000 roentgens per hour. A lethal dose is around 500 roentgens (~5 Gy) over 5 hours, so in some areas, unprotected workers received fatal doses in less than a minute. However, a dosimeter capable of measuring up to 1000 R/s was buried in the rubble of a collapsed part of the building, and another one failed when turned on. All remaining dosimeters had limits of 0.001 R/s and therefore read ""off scale"". Thus, the reactor crew could ascertain only that the radiation levels were somewhere above 0.001 R/s (3.6 R/h), while the true levels were much higher in some areas.Because of the inaccurate low readings, the reactor crew chief Alexander Akimov assumed that the reactor was intact. The evidence of pieces of graphite and reactor fuel lying around the building was ignored, and the readings of another dosimeter brought in by 04:30 were dismissed under the assumption that the new dosimeter must have been defective. Akimov stayed with his crew in the reactor building until morning, sending members of his crew to try to pump water into the reactor. None of them wore any protective gear. Most, including Akimov, died from radiation exposure within three weeks.Shortly after the accident, firefighters arrived to try to extinguish the fires. First on the scene was a Chernobyl Power Station firefighter brigade under the command of Lieutenant Volodymyr Pravik, who died on 9 May 1986 of acute radiation sickness. They were not told how dangerously radioactive the smoke and the debris were, and may not even have known that the accident was anything more than a regular electrical fire: ""We didn't know it was the reactor. No one had told us.""Grigorii Khmel, the driver of one of the fire engines, later described what happened:We arrived there at 10 or 15 minutes to two in the morning.... We saw graphite scattered about. Misha asked: ""Is that graphite?"" I kicked it away. But one of the fighters on the other truck picked it up. ""It's hot,"" he said. The pieces of graphite were of different sizes, some big, some small, enough to pick them up...We didn't know much about radiation. Even those who worked there had no idea. There was no water left in the trucks. Misha filled a cistern and we aimed the water at the top. Then those boys who died went up to the roof – Vashchik, Kolya and others, and Volodya Pravik.... They went up the ladder ... and I never saw them again.Anatoli Zakharov, a fireman stationed in Chernobyl since 1980, offers a different description in 2008:I remember joking to the others, ""There must be an incredible amount of radiation here. We'll be lucky if we're all still alive in the morning.""He also said:Of course we knew! If we'd followed regulations, we would never have gone near the reactor. But it was a moral obligation – our duty. We were like kamikaze.The immediate priority was to extinguish fires on the roof of the station and the area around the building containing Reactor No. 4 to protect No. 3 and keep its core cooling systems intact. The fires were extinguished by 5:00, but many firefighters received high doses of radiation. The fire inside reactor 4 continued to burn until 10 May 1986 it is possible that well over half of the graphite burned out.The fire was extinguished by a combined effort of helicopters dropping over 5000 metric tons of sand, lead, clay, and neutron-absorbing boron onto the burning reactor and injection of liquid nitrogen. It is now known that virtually none of the neutron absorbers reached the core.From eyewitness accounts of the firefighters involved before they died (as reported on the CBC television series Witness), one described his experience of the radiation as ""tasting like metal"", and feeling a sensation similar to that of pins and needles all over his face. (This is similar to the description given by Louis Slotin, a Manhattan Project physicist who died days after a fatal radiation overdose from a criticality accident.)The explosion and fire threw hot particles of the nuclear fuel and also far more dangerous fission products, radioactive isotopes such as caesium-137, iodine-131, strontium-90 and other radionuclides, into the air: the residents of the surrounding area observed the radioactive cloud on the night of the explosion.Equipment assembled included remote-controlled bulldozers and robot-carts that could detect radioactivity and carry hot debris. Valery Legasov (first deputy director of the Kurchatov Institute of Atomic Energy in Moscow) said, in 1987: ""But we learned that robots are not the great remedy for everything. Where there was very high radiation, the robot ceased to be a robot—the electronics quit working.""1:26:03 – fire alarm activated1:28 – arrival of local firefighters, Pravik's guard1:35 – arrival of firefighters from Pripyat, Kibenok's guard1:40 – arrival of Telyatnikov2:10 – turbine hall roof fire extinguished2:30 – main reactor hall roof fires suppressed3:30 – arrival of Kiev firefighters4:50 – fires mostly localized6:35 – all fires extinguished‡‡With the exception of the fire contained inside Reactor 4, which continued to burn for many days.The nearby city of Pripyat was not immediately evacuated. The townspeople went about their usual business, completely oblivious to what had just happened. However, within a few hours of the explosion, dozens of people fell ill. Later, they reported severe headaches and metallic tastes in their mouths, along with uncontrollable fits of coughing and vomiting.As the plant was run by authorities in Moscow, the government of Ukraine did not receive prompt information on the accident.Valentyna Shevchenko, then Chairman of the Presidium of Verkhovna Rada Supreme Soviet of the Ukrainian SSR, recalls that Ukraine's acting Minister of Internal Affairs Vasyl Durdynets phoned her at work at 9 am to report current affairs only at the end of the conversation did he add that there had been a fire at the Chernobyl nuclear power plant, but it was extinguished and everything was fine. When Shevchenko asked ""How are the people?"", he replied that there was nothing to be concerned about: ""Some are celebrating a wedding, others are gardening, and others are fishing in the Pripyat River"".Shevchenko then spoke over the phone to Volodymyr Shcherbytsky, Head of the Central Committee of the CPU and de facto head of state, who said he anticipated a delegation of the state commission headed by the deputy chairman of the Council of Ministers of USSR.A commission was set up the same day (26 April) to investigate the accident. It was headed by Valery Legasov, First Deputy Director of the Kurchatov Institute of Atomic Energy, and included leading nuclear specialist Evgeny Velikhov, hydro-meteorologist Yuri Izrael, radiologist Leonid Ilyin and others. They flew to Boryspil International Airport and arrived at the power plant in the evening of 26 April. By that time two people had already died and 52 were hospitalized. The delegation soon had ample evidence that the reactor was destroyed and extremely high levels of radiation had caused a number of cases of radiation exposure. In the early hours of 27 April, over 24 hours after the initial blast, they ordered the evacuation of Pripyat. Initially it was decided to evacuate the population for three days later this was made permanent.By 11:00 on 27 April, buses had arrived in Pripyat to start the evacuation. The evacuation began at 14:00. A translated excerpt of the evacuation announcement follows:For the attention of the residents of Pripyat! The City Council informs you that due to the accident at Chernobyl Power Station in the city of Pripyat the radioactive conditions in the vicinity are deteriorating. The Communist Party, its officials and the armed forces are taking necessary steps to combat this. Nevertheless, with the view to keep people as safe and healthy as possible, the children being top priority, we need to temporarily evacuate the citizens in the nearest towns of Kiev region. For these reasons, starting from April 27, 1986 2 pm each apartment block will be able to have a bus at its disposal, supervised by the police and the city officials. It is highly advisable to take your documents, some vital personal belongings and a certain amount of food, just in case, with you. The senior executives of public and industrial facilities of the city has decided on the list of employees needed to stay in Pripyat to maintain these facilities in a good working order. All the houses will be guarded by the police during the evacuation period. Comrades, leaving your residences temporarily please make sure you have turned off the lights, electrical equipment and water and shut the windows. Please keep calm and orderly in the process of this short-term evacuation.To expedite the evacuation, residents were told to bring only what was necessary, and that they would remain evacuated for approximately three days. As a result, most personal belongings were left behind, and remain there today. By 15:00, 53,000 people were evacuated to various villages of the Kiev region. The next day, talks began for evacuating people from the 10 km zone. Ten days after the accident, the evacuation area was expanded to 30 km (19 mi). This ""exclusion zone"" has remained ever since, although its shape has changed and its size has been expanded.Evacuation began long before the accident was publicly acknowledged by the Soviet Union. On the morning of 28 April, radiation levels set off alarms at the Forsmark Nuclear Power Plant in Sweden, over 1,000 kilometres (620 mi) from the Chernobyl Plant.  Sweden determined that the radiation had originated elsewhere, and the Swedish government contacted the Soviet government,  The Soviet government denied being the source of the radiation, until the Swedish government advised the Soviet government that a report was being made to the International Atomic Energy Authority.At 21:02 the evening of 28 April, a 20-second announcement was read in the TV news programme Vremya:There has been an accident at the Chernobyl Nuclear Power Plant. One of the nuclear reactors was damaged. The effects of the accident are being remedied. Assistance has been provided for any affected people. An investigative commission has been set up.This was the entirety of the announcement of the accident. The Telegraph Agency of the Soviet Union (TASS) then discussed Three Mile Island and other American nuclear accidents, an example of the common Soviet tactic of emphasizing foreign disasters when one occurred in the Soviet Union. The mention of a commission, however, indicated to observers the seriousness of the incident, and subsequent state radio broadcasts were replaced with classical music, which was a common method of preparing the public for an announcement of a tragedy.Around the same time, ABC News released its report about the disaster.Shevchenko was the first of the Ukrainian state top officials to arrive at the disaster site early on 28 April. There she spoke with members of medical staff and people, who were calm and hopeful that they could soon return to their homes. Shevchenko returned home near midnight, stopping at a radiological checkpoint in Vilcha, one of the first that were set up soon after the accident.There was a notification from Moscow that there was no reason to postpone the 1 May International Workers' Day celebrations in Kiev (including the annual parade), but on 30 April a meeting of the Political bureau of the Central Committee of CPU took place to discuss the plan for the upcoming celebration. Scientists were reporting that the radiological background in Kiev city was normal. At the meeting, which was finished at 18:00, it was decided to shorten celebrations from the regular 3.5–4 to under 2 hours.Several buildings in Pripyat were officially kept open after the disaster to be used by workers still involved with the plant. These included the Jupiter Factory which closed in 1996 and the Azure Swimming Pool which closed in 1998.Two floors of bubbler pools beneath the reactor served as a large water reservoir for the emergency cooling pumps and as a pressure suppression system capable of condensing steam in case of a small broken steam pipe the third floor above them, below the reactor, served as a steam tunnel. The steam released by a broken pipe was supposed to enter the steam tunnel and be led into the pools to bubble through a layer of water. After the disaster, the pools and the basement were flooded because of ruptured cooling water pipes and accumulated firefighting water, and constituted a serious steam explosion risk.The smoldering graphite, fuel and other material above, at more than 1200 °C, started to burn through the reactor floor and mixed with molten concrete from the reactor lining, creating corium, a radioactive semi-liquid material comparable to lava. If this mixture had melted through the floor into the pool of water, it was feared it could have created a serious steam explosion that would have ejected more radioactive material from the reactor. It became necessary to drain the pool.The bubbler pool could be drained by opening its sluice gates. However, the valves controlling it were underwater, located in a flooded corridor in the basement. So volunteers in wetsuits and respirators (for protection against radioactive aerosols) and equipped with dosimeters, entered the knee-deep radioactive water and managed to open the valves. These were the engineers Alexei Ananenko and Valeri Bezpalov (who knew where the valves were), accompanied by the shift supervisor Boris Baranov.  Upon succeeding and emerging from the water, according to many English language news articles, books and the prominent BBC docudrama Surviving Disaster – Chernobyl Nuclear, the three knew it was a suicide-mission and began suffering from radiation sickness and died soon after. Some sources also incorrectly claimed that they died there in the plant. However, research by Andrew Leatherbarrow, author of the 2016 book Chernobyl 01:23:40, determined that the frequently recounted story is a gross exaggeration. Alexei Ananenko continues to work in the nuclear energy industry, and rebuffs the growth of the Chernobyl media sensationalism surrounding him. While Valeri Bezpalov was found to still be alive by Leatherbarrow, the 65-year-old Baranov had lived until 2005 and had died of heart failure.Once the bubbler pool gates were opened by the Ananenko team, fire brigade pumps were then used to drain the basement. The operation was not completed until 8 May, after 20,000 metric tons of highly radioactive water were pumped out.With the bubbler pool gone, a meltdown was less likely to produce a powerful steam explosion. To do so, the molten core would now have to reach the water table below the reactor. To reduce the likelihood of this, it was decided to freeze the earth beneath the reactor, which would also stabilize the foundations. Using oil drilling equipment, the injection of liquid nitrogen began on 4 May. It was estimated that 25 metric tons of liquid nitrogen per day would be required to keep the soil frozen at −100 °C. This idea was soon scrapped and the bottom room where the cooling system would have been installed was filled with concrete.It is likely that intense alpha radiation hydrolysed the water, generating a low-pH hydrogen peroxide (H2O2) solution akin to an oxidizing acid. Conversion of bubbler pool water to H2O2 is confirmed by the presence in the Chernobyl lavas of studtite and metastudtite, the only minerals that contain peroxide.The worst of the radioactive debris was collected inside what was left of the reactor. During clean-up operations, remotely controlled machinery were used to try to move the most highly radioactive debris, but these failed in the harsh conditions. Consequently, the most highly radioactive materials were shoveled by Chernobyl liquidators from the military wearing heavy protective gear (dubbed ""bio-robots"" by the military) these soldiers could only spend a maximum of 40 seconds working on the rooftops of the surrounding buildings because of the extremely high doses of radiation given off by the blocks of graphite and other debris. Though the soldiers were only supposed to perform the role of the ""bio-robot"" a maximum of once, some soldiers reported having done this task five or six times.  The reactor itself was covered with bags of sand, lead and boric acid dropped from helicopters: some 5000 metric tons of material were dropped during the week that followed the accident.  Historians estimate that about 600 Soviet pilots risked dangerous levels of radiation to fly  the thousands of flights needed to cover reactor No. 4 in this attempt to seal off radiation.At the time there was still fear that the reactor could re-enter a self-sustaining nuclear chain-reaction and explode again, and a new containment structure was planned to prevent rain entering and triggering such an explosion, and to prevent further release of radioactive material. This was the largest civil engineering task in history, involving a quarter of a million construction workers who all reached their official lifetime limits of radiation. The Ukrainian filmmaker Vladimir Shevchenko captured film footage of an Mi-8 helicopter as its main rotor collided with a nearby construction crane cable, causing the helicopter to fall near the damaged reactor building and killing its four-man crew on 2nd October 1986. By December 1986, a large concrete sarcophagus had been erected to seal off the reactor and its contents. A unique ""clean up"" medal was given to the workers.Although many of the radioactive emergency vehicles were buried in trenches, many of the vehicles used by the liquidators, including the helicopters,  still remain parked in a field in the Chernobyl area. Scavengers have since removed many functioning, but highly radioactive, parts. During the construction of the sarcophagus, a scientific team re-entered the reactor as part of an investigation dubbed ""Complex Expedition"", to locate and contain nuclear fuel in a way that could not lead to another explosion. These scientists manually collected cold fuel rods, but great heat was still emanating from the core. Rates of radiation in different parts of the building were monitored by drilling holes into the reactor and inserting long metal detector tubes. The scientists were exposed to high levels of radiation and radioactive dust.After six months of investigation, in December 1986, they discovered with the help of a remote camera an intensely radioactive mass in the basement of Unit Four, more than two metres wide and weighing hundreds of tons, which they called ""the elephant's foot"" for its wrinkled appearance. The mass was composed of sand, glass and a large amount of nuclear fuel that had escaped from the reactor. The concrete beneath the reactor was steaming hot, and was breached by solidified lava and spectacular unknown crystalline forms termed chernobylite. It was concluded that there was no further risk of explosion.Liquidators worked under deplorable conditions, poorly informed and with poor protection. Many, if not most of them, exceeded radiation safety limits. Some exceeded limits by over 100 times—leading to rapid death.The official contaminated zones became stage to a massive clean-up effort lasting seven months. The official reason for such early (and dangerous) decontamination efforts, rather than allowing time for natural decay, was that the land must be re-peopled and brought back into cultivation. Indeed, within fifteen months 75% of the land was under cultivation, even though only a third of the evacuated villages were resettled. Defence forces must have done much of the work. Yet this land was of marginal agricultural value. According to historian David Marples, the administration had a psychological purpose for the clean-up: they wished to forestall panic regarding nuclear energy, and even to restart the Chernobyl power station.There were two official explanations of the accident.The first official explanation of the accident, later acknowledged to be erroneous, was published in August 1986. It effectively placed the blame on the power plant operators. To investigate the causes of the accident the IAEA created a group known as the International Nuclear Safety Advisory Group (INSAG), which in its report of 1986, INSAG-1, on the whole also supported this view, based on the data provided by the Soviets and the oral statements of specialists. In this view, the catastrophic accident was caused by gross violations of operating rules and regulations. ""During preparation and testing of the turbine generator under run-down conditions using the auxiliary load, personnel disconnected a series of technical protection systems and breached the most important operational safety provisions for conducting a technical exercise.""The operator error was probably due to their lack of knowledge of nuclear reactor physics and engineering, as well as lack of experience and training. According to these allegations, at the time of the accident the reactor was being operated with many key safety systems turned off, most notably the Emergency Core Cooling System (ECCS), LAR (Local Automatic control system), and AZ (emergency power reduction system). Personnel had an insufficiently detailed understanding of technical procedures involved with the nuclear reactor, and knowingly ignored regulations to speed test completion.The developers of the reactor plant considered this combination of events to be impossible and therefore did not allow for the creation of emergency protection systems capable of preventing the combination of events that led to the crisis, namely the intentional disabling of emergency protection equipment plus the violation of operating procedures. Thus the primary cause of the accident was the extremely improbable combination of rule infringement plus the operational routine allowed by the power station staff.In this analysis of the causes of the accident, deficiencies in the reactor design and in the operating regulations that made the accident possible were set aside and mentioned only casually. Serious critical observations covered only general questions and did not address the specific reasons for the accident.The following general picture arose from these observations, and several procedural irregularities also helped to make the accident possible, one of which was insufficient communication between the safety officers and the operators in charge of the experiment being run that night.The reactor operators disabled safety systems down to the generators, which the test was really about. The main process computer, SKALA, was running in such a way that the main control computer could not shut down the reactor or even reduce power. Normally the computer would have started to insert all of the control rods. The computer would have also started the ""Emergency Core Protection System"" that introduces 24 control rods into the active zone within 2.5 seconds, which is still slow by 1986 standards. All control was transferred from the process computer to the human operators.On the subject of the disconnection of safety systems, Valery Legasov said, in 1987, that ""[i]t was like airplane pilots experimenting with the engines in flight"".This view is reflected in numerous publications and also artistic works on the theme of the Chernobyl accident that appeared immediately after the accident, and for a long time remained dominant in the public consciousness and in popular publications.Ukraine has declassified a number of KGB documents from the period between 1971 and 1988 related to the Chernobyl plant, mentioning for example previous reports of structural damages caused by negligence during construction of the plant (such as splitting of concrete layers) that were never acted upon. They document over 29 emergency situations in the plant during this period, 8 of which were caused by negligence or poor competence on the part of personnel.In 1991 a Commission of the USSR State Committee for the Supervision of Safety in Industry and Nuclear Power reassessed the causes and circumstances of the Chernobyl accident and came to new insights and conclusions. Based on it, in 1992 the IAEA Nuclear Safety Advisory Group (INSAG) published an additional report, INSAG-7, which reviewed ""that part of the INSAG-1 report in which primary attention is given to the reasons for the accident,"" and was included the USSR State Commission report as Appendix I.In this INSAG report, most of the earlier accusations against staff for breach of regulations were acknowledged to be either erroneous, based on incorrect information obtained in August 1986, or less relevant. This report reflected a different view of the main reasons for the accident, presented in Appendix I. According to this account, the operators' actions in turning off the Emergency Core Cooling System, interfering with the settings on the protection equipment, and blocking the level and pressure in the separator drum did not contribute to the original cause of the accident and its magnitude, although they may have been a breach of regulations. In fact, turning off the emergency system designed to prevent the two turbine generators from stopping was not a violation of regulations.Human factors, however, contributed to the conditions that led to the disaster. These included operating the reactor at a low power level—less than 700 MW—a level documented in the run-down test programme, and operating with a small operational reactivity margin (ORM). The 1986 assertions of Soviet experts notwithstanding, regulations did not prohibit operating the reactor at this low power level.However, regulations did forbid operating the reactor with a small margin of reactivity. Yet ""post-accident studies have shown that the way in which the real role of the ORM is reflected in the Operating Procedures and design documentation for the RBMK-1000 is extremely contradictory"", and furthermore, ""ORM was not treated as an operational safety limit, violation of which could lead to an accident"".According to the INSAG-7 Report, the chief reasons for the accident lie in the peculiarities of physics and in the construction of the reactor. There are two such reasons:The reactor had a dangerously large positive void coefficient of reactivity. The void coefficient is a measurement of how a reactor responds to increased steam formation in the water coolant. Most other reactor designs have a negative coefficient, i.e. the nuclear reaction rate slows when steam bubbles form in the coolant, since as the vapour phase in the reactor increases, fewer neutrons are slowed down. Faster neutrons are less likely to split uranium atoms, so the reactor produces less power (a negative feedback). Chernobyl's RBMK reactor, however, used solid graphite as a neutron moderator to slow down the neutrons, and the water in it, on the contrary, acts like a harmful neutron absorber. Thus neutrons are slowed down even if steam bubbles form in the water. Furthermore, because steam absorbs neutrons much less readily than water, increasing the intensity of vaporization means that more neutrons are able to split uranium atoms, increasing the reactor's power output. This makes the RBMK design very unstable at low power levels, and prone to suddenly increasing energy production to a dangerous level. This behaviour is counter-intuitive, and this property of the reactor was unknown to the crew.A more significant flaw was in the design of the control rods that are inserted into the reactor to slow down the reaction. In the RBMK reactor design, the lower part of each control rod was made of graphite and was 1.3 metres shorter than necessary, and in the space beneath the rods were hollow channels filled with water. The upper part of the rod, the truly functional part that absorbs the neutrons and thereby halts the reaction, was made of boron carbide. With this design, when the rods are inserted into the reactor from the uppermost position, the graphite parts initially displace some water (which absorbs neutrons, as mentioned above), effectively causing fewer neutrons to be absorbed initially. Thus for the first few seconds of control rod activation, reactor power output is increased, rather than reduced as desired. This behaviour is counter-intuitive and was not known to the reactor operators.Other deficiencies besides these were noted in the RBMK-1000 reactor design, as were its non-compliance with accepted standards and with the requirements of nuclear reactor safety.While INSAG-1 and INSAG-7 reports both identified operator error as an issue of concern, the INSAG-7 identified that there were numerous other issues that were contributing factors that led to the incident.  These contributing factors include:The plant was not designed to safety standards in effect and incorporated unsafe features""Inadequate safety analysis"" was performedThere was ""insufficient attention to independent safety review""""Operating  procedures not founded satisfactorily in safety analysis""Safety information not adequately and effectively communicated between operators, and between operators and designersThe operators did not adequately understand safety aspects of the plantOperators did not sufficiently respect formal requirements of operational and test proceduresThe regulatory regime was insufficient to effectively counter pressures for productionThere was a ""general lack of safety culture in nuclear matters at the national level as well as locally""Both views were heavily lobbied by different groups, including the reactor's designers, power plant personnel, and the Soviet and Ukrainian governments. According to the IAEA's 1986 analysis, the main cause of the accident was the operators' actions. But according to the IAEA's 1993 revised analysis the main cause was the reactor's design. One reason there were such contradictory viewpoints and so much debate about the causes of the Chernobyl accident was that the primary data covering the disaster, as registered by the instruments and sensors, were not completely published in the official sources.Once again, the human factor had to be considered as a major element in causing the accident. INSAG notes that both the operating regulations and staff handled the disabling of the reactor protection easily enough: witness the length of time for which the ECCS was out of service while the reactor was operated at half power. INSAG's view is that it was the operating crew's deviation from the test programme that was mostly to blame. ""Most reprehensibly, unapproved changes in the test procedure were deliberately made on the spot, although the plant was known to be in a very different condition from that intended for the test.""As in the previously released report INSAG-1, close attention is paid in report INSAG-7 to the inadequate (at the moment of the accident) ""culture of safety"" at all levels. Deficiency in the safety culture was inherent not only at the operational stage but also, and to no lesser extent, during activities at other stages in the lifetime of nuclear power plants (including design, engineering, construction, manufacture, and regulation). The poor quality of operating procedures and instructions, and their conflicting character, put a heavy burden on the operating crew, including the chief engineer. ""The accident can be said to have flowed from a deficient safety culture, not only at the Chernobyl plant, but throughout the Soviet design, operating and regulatory organizations for nuclear power that existed at that time.""Although no informing comparisons can be made between the accident and a strictly air burst-fuzed nuclear detonation, it has still been approximated that about four hundred times more radioactive material was released from Chernobyl than by the atomic bombing of Hiroshima and Nagasaki. By contrast the Chernobyl accident released about one hundredth to one thousandth of the total amount of radioactivity released during the era of nuclear weapons testing at the height of the Cold War, 1950 – 1960s, with the 1/100 to 1/1000 variance due to trying to make comparisons with different spectrums of isotopes released. Approximately 100,000 km² of land was significantly contaminated with fallout, with the worst hit regions being in Belarus, Ukraine and Russia. Slighter levels of contamination were detected over all of Europe except for the Iberian Peninsula.The initial evidence that a major release of radioactive material was affecting other countries came not from Soviet sources, but from Sweden. On the morning of 28 April workers at the Forsmark Nuclear Power Plant (approximately 1,100 km (680 mi) from the Chernobyl site) were found to have radioactive particles on their clothes.It was Sweden's search for the source of radioactivity, after they had determined there was no leak at the Swedish plant, that at noon on 28 April led to the first hint of a serious nuclear problem in the western Soviet Union. Hence the evacuation of Pripyat on 27 April 36 hours after the initial explosions, was silently completed before the disaster became known outside the Soviet Union. The rise in radiation levels had at that time already been measured in Finland, but a civil service strike delayed the response and publication.Contamination from the Chernobyl accident was scattered irregularly depending on weather conditions, much of it deposited on mountainous regions such as the Alps, the Welsh mountains and the Scottish Highlands, where adiabatic cooling caused radioactive rainfall. The resulting patches of contamination were often highly localized, and water-flows across the ground contributed further to large variations in radioactivity over small areas. Sweden and Norway also received heavy fallout when the contaminated air collided with a cold front, bringing rain.Rain was purposely seeded over 10,000 km2 of the Belorussian SSR by the Soviet air force to remove radioactive particles from clouds heading toward highly populated areas. Heavy, black-coloured rain fell on the city of Gomel. Reports from Soviet and Western scientists indicate that Belarus received about 60% of the contamination that fell on the former Soviet Union. However, the 2006 TORCH report stated that half of the volatile particles had landed outside Ukraine, Belarus, and Russia. A large area in Russia south of Bryansk was also contaminated, as were parts of northwestern Ukraine. Studies in surrounding countries indicate that over one million people could have been affected by radiation.Recently published data from a long-term monitoring program (The Korma Report II) shows a decrease in internal radiation exposure of the inhabitants of a region in Belarus close to Gomel. Resettlement may even be possible in prohibited areas provided that people comply with appropriate dietary rules.In Western Europe, precautionary measures taken in response to the radiation included seemingly arbitrary regulations banning the importation of certain foods but not others. In France some officials stated that the Chernobyl accident had no adverse effects. Official figures in southern Bavaria in Germany indicated that some wild plant species contained substantial levels of caesium, which were believed to have been passed onto them during their consumption by wild boars, a significant number of which already contained radioactive particles above the allowed level.Like many other releases of radioactivity into the environment, the Chernobyl release was controlled by the physical and chemical properties of the radioactive elements in the core. Particularly dangerous are the highly radioactive fission products, those with high nuclear decay rates that accumulate in the food chain, such as some of the isotopes of iodine, caesium and strontium. Iodine-131 and caesium-137 are responsible for most of the radiation exposure received by the general population.Detailed reports on the release of radioisotopes from the site were published in 1989 and 1995, with the latter report updated in 2002.At different times after the accident, different isotopes were responsible for the majority of the external dose. The remaining quantity of any radioisotope, and therefore the activity of that isotope, after 7 decay half-lives have passed, is less than 1% of its initial magnitude, and it continues to reduce beyond 0.78% after 7 half-lives to 0.098% remaining after 10 half-lives have passed and so on. (Some radionuclides have decay products that are likewise radioactive, which is not accounted for here.) The release of radioisotopes from the nuclear fuel was largely controlled by their boiling points, and the majority of the radioactivity present in the core was retained in the reactor.All of the noble gases, including krypton and xenon, contained within the reactor were released immediately into the atmosphere by the first steam explosion. The atmospheric release of xenon-133, with a half-life of 5 days, is estimated at 5200 PBq.50 to 60% of all core radioiodine in the reactor, about 1760 PBq (1760×1015 becquerels), or about 0.4 kg, was released, as a mixture of sublimed vapour, solid particles, and organic iodine compounds. Iodine-131 has a half-life of 8 days.20 to 40% of all core caesium-137 was released, 85 PBq in all. Caesium was released in aerosol form caesium-137, along with isotopes of strontium, are the two primary elements preventing the Chernobyl exclusion zone being re-inhabited. 8.5×1016 Bq equals 24 kilograms of caesium-137. Cs-137 has a half-life of 30 years.Tellurium-132, half-life 78 hours, an estimated 1150 PBq was released.An early estimate for total nuclear fuel material released to the environment was 3±1.5% this was later revised to 3.5±0.5%. This corresponds to the atmospheric emission of 6 t of fragmented fuel.Two sizes of particles were released: small particles of 0.3 to 1.5 micrometres (aerodynamic diameter) and large particles of 10 micrometres. The large particles contained about 80% to 90% of the released nonvolatile radioisotopes zirconium-95, niobium-95, lanthanum-140, cerium-144 and the transuranic elements, including neptunium, plutonium and the minor actinides, embedded in a uranium oxide matrix.The dose that was calculated is the relative external gamma dose rate for a person standing in the open. The exact dose to a person in the real world who would spend most of their time sleeping indoors in a shelter and then venturing out to consume an internal dose from the inhalation or ingestion of a radioisotope, requires a personnel specific radiation dose reconstruction analysis.The Chernobyl nuclear power plant is located next to the Pripyat River, which feeds into the Dnieper reservoir system, one of the largest surface water systems in Europe, which at the time supplied water to Kiev's 2.4 million residents, and was still in spring flood when the accident occurred. The radioactive contamination of aquatic systems therefore became a major problem in the immediate aftermath of the accident. In the most affected areas of Ukraine, levels of radioactivity (particularly from radionuclides 131I, 137Cs and 90Sr) in drinking water caused concern during the weeks and months after the accident, though officially it was stated that all contaminants had settled to the bottom ""in an insoluble phase"" and would not dissolve for 800–1000 years. Guidelines for levels of radioiodine in drinking water were temporarily raised to 3,700 Bq/L, allowing most water to be reported as safe, and a year after the accident it was announced that even the water of the Chernobyl plant's cooling pond was within acceptable norms. Despite this, two months after the disaster the Kiev water supply was abruptly switched from the Dnieper to the Desna River. Meanwhile, massive silt traps were constructed, along with an enormous 30m-deep underground barrier to prevent groundwater from the destroyed reactor entering the Pripyat River.Bio-accumulation of radioactivity in fish resulted in concentrations (both in western Europe and in the former Soviet Union) that in many cases were significantly above guideline maximum levels for consumption. Guideline maximum levels for radiocaesium in fish vary from country to country but are approximately 1000 Bq/kg in the European Union. In the Kiev Reservoir in Ukraine, concentrations in fish were several thousand Bq/kg during the years after the accident.In small ""closed"" lakes in Belarus and the Bryansk region of Russia, concentrations in a number of fish species varied from 100 to 60,000 Bq/kg during the period 1990–92. The contamination of fish caused short-term concern in parts of the UK and Germany and in the long term (years rather than months) in the affected areas of Ukraine, Belarus, and Russia as well as in parts of Scandinavia.Groundwater was not badly affected by the Chernobyl accident since radionuclides with short half-lives decayed away long before they could affect groundwater supplies, and longer-lived radionuclides such as radiocaesium and radiostrontium were adsorbed to surface soils before they could transfer to groundwater. However, significant transfers of radionuclides to groundwater have occurred from waste disposal sites in the 30 km (19 mi) exclusion zone around Chernobyl. Although there is a potential for transfer of radionuclides from these disposal sites off-site (i.e. out of the 30 km (19 mi) exclusion zone), the IAEA Chernobyl Report argues that this is not significant in comparison to current levels of washout of surface-deposited radioactivity.After the disaster, four square kilometres of pine forest directly downwind of the reactor turned reddish-brown and died, earning the name of the ""Red Forest"". Some animals in the worst-hit areas also died or stopped reproducing. Most domestic animals were removed from the exclusion zone, but horses left on an island in the Pripyat River 6 km (4 mi) from the power plant died when their thyroid glands were destroyed by radiation doses of 150–200 Sv. Some cattle on the same island died and those that survived were stunted because of thyroid damage. The next generation appeared to be normal.A robot sent into the reactor itself has returned with samples of black, melanin-rich radiotrophic fungi that are growing on the reactor's walls.Of the 440,350 wild boar killed in the 2010 hunting season in Germany, over 1000 were found to be contaminated with levels of radiation above the permitted limit of 600 becquerels per kilogram, due to residual radioactivity from Chernobyl.The Norwegian Agricultural Authority reported that in 2009 a total of 18,000 livestock in Norway needed to be given uncontaminated feed for a period of time before slaughter in order to ensure that their meat was safe for human consumption. This was due to residual radioactivity from Chernobyl in the plants they graze on in the wild during the summer. 1,914 sheep needed to be given uncontaminated feed for a period of time before slaughter during 2012, and these sheep were located in just 18 of Norway's municipalities, a decrease of 17 from the 35 municipalities affected animals were located in during 2011 (117 municipalities were affected during 1986).The after-effects of Chernobyl were expected to be seen for a further 100 years, although the severity of the effects would decline over that period. Scientists report this is due to radioactive caesium-137 isotopes being taken up by fungi such as Cortinarius caperatus which is in turn eaten by sheep whilst grazing.The United Kingdom was forced to restrict the movement of sheep from upland areas when radioactive caesium-137 fell across parts of Northern Ireland, Wales, Scotland and northern England. In the immediate aftermath of the disaster in 1986, a total of 4,225,000 sheep had their movement restricted across a total of 9,700 farms, in order to prevent contaminated meat entering the human food chain. The number of sheep and the number of farms affected has decreased since 1986, Northern Ireland was released from all restrictions in 2000 and by 2009 369 farms containing around 190,000 sheep remained under the restrictions in Wales, Cumbria and northern Scotland. The restrictions applying in Scotland were lifted in 2010, whilst those applying to Wales and Cumbria were lifted during 2012, meaning no farms in the UK remain restricted because of Chernobyl fallout.The legislation used to control sheep movement and compensate farmers (farmers were latterly compensated per animal to cover additional costs in holding animals prior to radiation monitoring) was revoked during October and November 2012 by the relevant authorities in the UK.In the aftermath of the accident, 237 people suffered from acute radiation sickness, of whom 31 died within the first three months.In 2005 the Chernobyl Forum, composed of the IAEA, other UN organizations and the governments of Belarus, Russia and Ukraine, published a report on the radiological environmental and health consequences of the Chernobyl accident.On the death toll of the accident, the report states that 28 emergency workers (""liquidators"") died from acute radiation syndrome, including beta burns, and 15 patients died from thyroid cancer in the following years, and it roughly estimated that cancer deaths caused by Chernobyl may reach a total of about 4,000 among the 5 million persons residing in the contaminated areas.  The report projected cancer mortality ""increases of less than one per cent"" (~0.3%) on a time span of 80 years, cautioning that this estimate was ""speculative"" since at this time only a few cancer deaths are linked to the Chernobyl disaster. The report says it is impossible to reliably predict the number of fatal cancers arising from the incident as small differences in assumptions can result in large differences in the estimated health costs. The report says it represents the consensus view of the eight UN organizations.Of all 66,000 Belarusian emergency workers, by the mid-1990s only 150 (roughly 0.2%) were reported by their government as having died. In contrast, 5,722 casualties were reported among Ukrainian clean-up workers up to the year 1995, by the National Committee for Radiation Protection of the Ukrainian Population.The four most harmful radionuclides spread from Chernobyl were iodine-131, caesium-134, caesium-137 and strontium-90, with half-lives of 8.02 days, 2.07 years, 30.2 years and 28.8 years respectively. The iodine was initially viewed with less alarm than the other isotopes, because of its short half-life, but it is highly volatile, and now appears to have travelled furthest and caused the most severe health problems in the short term. Strontium, on the other hand, is the least volatile of the four, and of main concern in the areas near Chernobyl itself. Iodine tends to become concentrated in thyroid and milk glands, leading, among other things, to increased incidence of thyroid cancers. Caesium tends to accumulate in vital organs such as the heart, while strontium accumulates in bones, and may thus be a risk to bone-marrow and lymphocytes. Radiation is most damaging to cells that are actively dividing. In adult mammals cell division is slow, except in hair follicles, skin, bone marrow and the gastrointestinal tract, which is why vomiting and hair loss are common symptoms of acute radiation sickness.By the year 2000, the number of Ukrainians claiming to be radiation 'sufferers' (poterpili) and receiving state benefits had jumped to 3.5 million, or 5% of the population. Many of these are populations resettled from contaminated zones, or former or current Chernobyl plant workers. According to IAEA-affiliated scientific bodies, these apparent increases of ill health result partly from economic strains on these countries and poor health-care and nutrition also, they suggest that increased medical vigilance following the accident has meant that many cases that would previously have gone unnoticed (especially of cancer) are now being registered.The World Health Organization states, ""children conceived before or after their father's exposure showed no statistically significant differences in mutation frequencies"". This statistically insignificant increase was also seen by independent researchers analyzing the children of the Chernobyl liquidators.On farms in Narodychi Raion of Ukraine it is claimed that in the first four years of the disaster nearly 350 animals were born with gross deformities such as missing or extra limbs, missing eyes, heads or ribs, or deformed skulls in comparison, only three abnormal births had been registered in the five years prior. The two primary individuals involved with the attempt to suggest that the mutation rate amongst animals was, and continues to be, higher in the Chernobyl zone, are the Anders Moller and Timothy Mousseau group. Apart from continuing to publish experimentally unrepeatable and discredited papers, Mousseau routinely gives talks at the Helen Caldicott organized symposiums for ""Physicians for Social Responsibility"", an anti-nuclear advocacy group, devoted to bring about a ""nuclear free planet"". Moreover, in years past Moller was previously caught and reprimanded for publishing papers that crossed the scientific ""misconduct""/""fraud"" line. The duo have more recently attempted to publish meta-analyses in which the primary references they weigh-up, analyze and draw their conclusions from is their own prior papers along with the discredited book Chernobyl: Consequences of the Catastrophe for People and the Environment.In 1996, geneticist colleagues Ronald Chesser and Robert Baker published a paper on the thriving vole population within the exclusion zone, in which the central conclusion of their work was essentially that ""The mutation rate in these animals is hundreds and probably thousands of times greater than normal"", this claim occurred after they had done a comparison of the mitochondrial DNA of the ""Chernobyl voles"" with that of a control group of voles from outside the region. These alarming conclusions led the paper to appear on the front cover of the prestigious journal Nature, however not long after publication Chesser & Baker discovered a fundamental error in their research in which they had incorrectly classified the species of vole, and therefore were comparing the genetics of two entirely different vole species to start with.Following the accident, journalists mistrusted many medical professionals (such as the spokesman from the UK National Radiological Protection Board), and in turn encouraged the public to mistrust them. Throughout the European continent, due to this media-driven framing of the slight contamination and in nations where abortion is legal, many requests for induced abortions, of otherwise normal pregnancies, were obtained out of fears of radiation from Chernobyl, including an excess number of abortions in Denmark in the months following the accident. In Greece, following the accident many obstetricians were unable to resist requests from worried pregnant mothers over fears of radiation. Although it was determined that the effective dose to Greeks would not exceed 1 mSv (100 mrem), a dose much lower than that which could induce embryonic abnormalities or other non-stochastic effects, there was an observed 2500 excess of otherwise wanted pregnancies being terminated, probably out of fear in the mother of radiation risk. A ""slightly"" above the expected number of requested induced abortions occurred in Italy.Worldwide, an estimated excess of about 150,000 elective abortions may have been performed on otherwise healthy pregnancies out of unfounded fears of radiation from Chernobyl, according to Dr Robert Baker and ultimately a 1987 article published by Linda E. Ketchum in the Journal of Nuclear Medicine which mentions but does not reference an IAEA source on the matter.The available statistical data excludes the Soviet/Ukraine/Belarus abortion rates, as they are presently unavailable. From the available data, an increase in the number of abortions in what were healthy developing human offspring in Denmark occurred in the months following the accident, at a rate of about 400 cases. In Greece, there was an observed 2500 excess of otherwise wanted pregnancies being terminated. In Italy, a ""slightly"" above the expected number of induced abortions occurred, approximately 100.As the increase in radiation in Denmark was so low...the public debate and anxiety among the pregnant women and their husbands ""caused"" more fetal deaths in Denmark than the accident. This underlines the importance of public debate, the role of the mass media and of the way in which National Health authorities participate in this debate.No evidence of changes in the prevalence of human deformities/birth congenital anomalies which might be associated with the accident, are apparent in Belarus or the Ukraine, the two republics which had the highest exposure to fallout. In Sweden, and Finland where no increase in abortion rates occurred, it was likewise determined that ""no association between the temporal and spatial variations in radioactivity and variable incidence of congenital malformations [was found]."" A similar null increase in the abortion rate and a healthy baseline situation of no increase in birth defects was determined by assessing the Hungarian Congenital Abnormality Registry, Findings also mirrored in Austria. Larger, ""mainly western European"" data sets approaching a million births in the EUROCAT database, divided into ""exposed"" and control groups were assessed in 1999. As no Chernobyl impacts were detected, the researchers conclude ""in retrospect the widespread fear in the population about the possible effects of exposure on the unborn fetus was not justified"". Despite studies from Germany and Turkey, the only robust evidence of negative pregnancy outcomes that transpired after the accident were these elective abortion indirect effects, in Greece, Denmark, Italy etc., due to the anxieties created.In very high doses, it was known at the time that radiation can cause a physiological increase in the rate of pregnancy anomalies, but unlike the dominant linear-no threshold model of radiation and cancer rate increases, it was known, by researchers familiar with both the prior human exposure data and animal testing, that the ""Malformation of organs appears to be a deterministic effect with a threshold dose"" below which, no rate increase is observed. This teratology (birth defects) issue was discussed by Frank Castronovo of the Harvard Medical School in 1999, publishing a detailed review of dose reconstructions and the available pregnancy data following the Chernobyl accident, inclusive of data from Kiev's two largest obstetrics hospitals. Castronovo concludes that ""the lay press with newspaper reporters playing up anecdotal stories of children with birth defects"" is, together with dubious studies that show selection bias, the two primary factors causing the persistent belief that Chernobyl increased the background rate of birth defects. When the vast amount of pregnancy data does not support this perception as no women took part in the most radioactive liquidator operations, no in-utero individuals would have been expected to have received a threshold dose.In one small behavioral study in 1998, with low statistical power and limited Multivariate analysis which akin to the widely published Hiroshima and Nagasaki studies, investigated and selected the children who were in utero during the rapidly dividing and therefore radiosensitive phase of neurogenesis(8 to 16 weeks of gestation), and whose mothers were evacuated from some of the more energetic hot-spot parts of the Chernobyl exclusion zone following the accident. From a random selection of 50 individuals in late-childhood in 1998, a low quality statistically-significant increase in the rate of severe IQ reduction was found, with a threshold of a suggested ~ 0.30 Sv(300 mSv) as a thyroid dose to the developing human head, for the beginning emergence of cerebral disorder.The Chernobyl liquidators, essentially an all-male civil defense emergency workforce, would go on to father normal children, without an increase in developmental anomalies or a statistically significant increase in the frequencies of germline mutations in their progeny. This normality is similarly seen in the children of the survivors of the Goiana accident.Due in largest part from the ingestion of contaminated dairy products along with the inhalation of the short-lived and therefore highly radioactive isotope, Iodine-131, the 2005 UN collaborative Chernobyl Forum revealed thyroid cancer among children to be one of the main health impacts from the Chernobyl accident. In that publication more than 4000 cases were reported, and that there was no evidence of an increase in solid cancers or leukemia. It said that there was an increase in psychological problems among the affected population. Dr Michael Repacholi, manager of WHO's Radiation Program reported that the 4000 cases of thyroid cancer resulted in nine deaths.According to the United Nations Scientific Committee on the Effects of Atomic Radiation, up to the year 2005, an excess of over 6000 cases of thyroid cancer have been reported. That is, over the estimated pre-accident baseline thyroid cancer rate, more than 6000 casual cases of thyroid cancer have been reported in children and adolescents exposed at the time of the accident, a number that is expected to increase. They concluded that there is no other evidence of major health impacts from the radiation exposure.Well-differentiated thyroid cancers are generally treatable, and when treated the five-year survival rate of thyroid cancer is 96%, and 92% after 30 years. the United Nations Scientific Committee on the Effects of Atomic Radiation had reported 15 deaths from thyroid cancer in 2011. The International Atomic Energy Agency (IAEA) also states that there has been no increase in the rate of birth defects or abnormalities, or solid cancers (such as lung cancer) corroborating the assessments by the UN committee. UNSCEAR raised the possibility of long term genetic defects, pointing to a doubling of radiation-induced minisatellite mutations among children born in 1994. However, the risk of thyroid cancer associated with the Chernobyl accident is still high according to published studies.The German affiliate of the ultra-anti-nuclear energy organization, the International Physicians for the Prevention of Nuclear War suggest that 10,000 people are affected by thyroid cancer as of 2006 and that 50,000 cases are expected in the future.Fred Mettler, a radiation expert at the University of New Mexico, puts the number of worldwide cancer deaths outside the highly contaminated zone at ""perhaps"" 5000, for a total of 9000 Chernobyl-associated fatal cancers, saying ""the number is small (representing a few percent) relative to the normal spontaneous risk of cancer, but the numbers are large in absolute terms"". The same report outlined studies based in data found in the Russian Registry from 1991 to 1998 that suggested that ""of 61,000 Russian workers exposed to an average dose of 107 mSv about 5% of all fatalities that occurred may have been due to radiation exposure.""The report went into depth about the risks to mental health of exaggerated fears about the effects of radiation. According to the IAEA the ""designation of the affected population as ""victims"" rather than ""survivors"" has led them to perceive themselves as helpless, weak and lacking control over their future"". The IAEA says that this may have led to behaviour that has caused further health effects.Fred Mettler commented that 20 years later: ""The population remains largely unsure of what the effects of radiation actually are and retain a sense of foreboding. A number of adolescents and young adults who have been exposed to modest or small amounts of radiation feel that they are somehow fatally flawed and there is no downside to using illicit drugs or having unprotected sex. To reverse such attitudes and behaviours will likely take years although some youth groups have begun programs that have promise."" In addition, disadvantaged children around Chernobyl suffer from health problems that are attributable not only to the Chernobyl accident, but also to the poor state of post-Soviet health systems.The United Nations Scientific Committee on the Effects of Atomic Radiation (UNSCEAR), part of the Chernobyl Forum, have produced their own assessments of the radiation effects. UNSCEAR was set up as a collaboration between various United Nation bodies, including the World Health Organization, after the atomic bomb attacks on Hiroshima and Nagasaki, to assess the long-term effects of radiation on human health.The number of potential deaths arising from the Chernobyl disaster is heavily debated. The WHO's prediction of 4000 future cancer deaths in surrounding countries is based on the Linear no-threshold model (LNT), which assumes that the damage inflicted by radiation at low doses is directly proportional to the dose. Radiation epidemiologist Roy Shore contends that estimating health effects in a population from the LNT model ""is not wise because of the uncertainties"".According to the Union of Concerned Scientists the number of excess cancer deaths worldwide (including all contaminated areas) is approximately 27,000 based on the same LNT.Another study critical of the Chernobyl Forum report was commissioned by Greenpeace, which asserted that the most recently published figures indicate that in Belarus, Russia and Ukraine the accident could have resulted in 10,000–200,000 additional deaths in the period between 1990 and 2004. The Scientific Secretary of the Chernobyl Forum criticized the report's reliance on non-peer-reviewed locally produced studies. Although most of the study's sources were from peer-reviewed journals, including many Western medical journals, the higher mortality estimates were from non-peer-reviewed sources, while Gregory Härtl (spokesman for the WHO) suggested that the conclusions were motivated by ideology.Chernobyl: Consequences of the Catastrophe for People and the Environment is a 2007 Russian publication that concludes that there were 985,000 premature deaths as a result of the radioactivity released. The results were criticized by M. I. Balonov of the Institute of Radiation Hygiene in St. Petersburg, who described them as biased, drawing from sources which were difficult to independently verify and lacking a proper scientific base. Balanov expressed his opinion that ""the authors unfortunately did not appropriately analyze the content of the Russian-language publications, for example, to separate them into those that contain scientific evidence and those based on hasty impressions and ignorant conclusions"".According to Kenneth Mossman, a Professor of Health Physics and member of the U.S. Nuclear Regulatory Commission advisory committee, the ""LNT philosophy is overly conservative, and low-level radiation may be less dangerous than commonly believed"". Yoshihisa Matsumoto, a radiation biologist at the Tokyo Institute of Technology, cites laboratory experiments on animals to suggest there must be a threshold dose below which DNA repair mechanisms can completely repair any radiation damage. Mossman suggests that the proponents of the current model believe that being conservative is justified due to the uncertainties surrounding low level doses and it is better to have a ""prudent public health policy"".Another significant issue is establishing consistent data on which to base the analysis of the impact of the Chernobyl accident. Since 1991 large social and political changes have occurred within the affected regions and these changes have had significant impact on the administration of health care, on socio-economic stability, and the manner in which statistical data is collected. Ronald Chesser, a radiation biologist at Texas Tech University, says that ""the subsequent Soviet collapse, scarce funding, imprecise dosimetry, and difficulties tracking people over the years have limited the number of studies and their reliability"".It is difficult to establish the total economic cost of the disaster. According to Mikhail Gorbachev, the Soviet Union spent 18 billion rubles (the equivalent of US$18 billion at that time) on containment and decontamination, virtually bankrupting itself. In Belarus the total cost over 30 years is estimated at US$235 billion (in 2005 dollars). Ongoing costs are well known in their 2003–2005 report, The Chernobyl Forum stated that between 5% and 7% of government spending in Ukraine is still related to Chernobyl, while in Belarus over $13 billion is thought to have been spent between 1991 and 2003, with 22% of national budget having been Chernobyl-related in 1991, falling to 6% by 2002. Much of the current cost relates to the payment of Chernobyl-related social benefits to some 7 million people across the 3 countries.A significant economic impact at the time was the removal of 784,320 ha (1,938,100 acres) of agricultural land and 694,200 ha (1,715,000 acres) of forest from production. While much of this has been returned to use, agricultural production costs have risen due to the need for special cultivation techniques, fertilizers and additives.Politically, the accident gave great significance to the new Soviet policy of glasnost, and helped forge closer Soviet–US relations at the end of the Cold War, through bioscientific cooperation. The disaster also became a key factor in the Union's eventual 1991 dissolution, and a major influence in shaping the new Eastern Europe.Both Ukraine and Belarus, in their first months of independence, lowered legal radiation thresholds from the Soviet Union's previous, elevated thresholds (from 35 rems per lifetime under the USSR to 7 rems per lifetime in Ukraine and 0.1 rems per year in Belarus). This required an expansion of territories that were considered contaminated. In Ukraine, over 500,000 people have now been resettled, many of whom have become applicants for medical and other welfare. Ukraine also maintains the destroyed reactor, for which it employs a very large workforce in order to keep individual exposure times low. Many of these workers have since registered disabilities and enrolled for welfare. In Ukraine, the Chernobyl disaster was an icon of the nationalist movement, symbolic of all that was wrong with the Soviet Union, and welfare became a key platform for winning independence. Ukraine has since developed a massive and burdensome welfare system that has become increasingly corrupt and ineffective. It has presented its greatly increased welfare demands since 1991 as a demonstration of its own moral legitimacy, and as an argument for needing foreign aid. Belarus, on the other hand, was politically weak when it gained independence, and looked to Moscow for guidance in many ways it has returned to the old Soviet policy of secrecy and denial.Following the accident, questions arose about the future of the plant and its eventual fate. All work on the unfinished reactors 5 and 6 was halted three years later. However, the trouble at the Chernobyl plant did not end with the disaster in reactor 4. The damaged reactor was sealed off and 200 cubic meters (260 cu yd) of concrete was placed between the disaster site and the operational buildings. The work was managed by Grigoriy Mihaylovich Naginskiy, the deputy chief engineer of Installation and Construction Directorate – 90. The Ukrainian government continued to let the three remaining reactors operate because of an energy shortage in the country.In October 1991, a fire broke out in the turbine building of reactor 2 the authorities subsequently declared the reactor damaged beyond repair, and it was taken offline. Reactor 1 was decommissioned in November 1996 as part of a deal between the Ukrainian government and international organizations such as the IAEA to end operations at the plant. On 15 December 2000, then-President Leonid Kuchma personally turned off Reactor 3 in an official ceremony, shutting down the entire site.The Chernobyl reactor is now enclosed in a large concrete sarcophagus, which was built quickly to allow continuing operation of the other reactors at the plant.A New Safe Confinement was to have been built by the end of 2005 however, it has suffered ongoing delays and as of  2010, when construction finally began, was expected to be completed in 2013. This was delayed again to 2016, the end of the 30-year lifespan of the sarcophagus. The structure was built adjacent to the existing shelter and in November 2016 was slid into place on rails. It is a metal arch 105 metres (344 ft) high and spanning 257 metres (843 ft), to cover both unit 4 and the hastily built 1986 structure. The Chernobyl Shelter Fund, set up in 1997, has received €810 million from international donors and projects to cover this project and previous work. It and the Nuclear Safety Account, also applied to Chernobyl decommissioning, are managed by the European Bank for Reconstruction and Development (EBRD).As of 29 November 2016, Reactor No. 4 has been covered by the New Safe Confinement that covers the reactor and the unstable “sarcophagus”. The huge steel arch was moved into place over several weeks, and the completion of this procedure was celebrated with a ceremony at the site, attended by the Ukrainian president, Petro Poroshenko, diplomats and site workers.  Unlike the original sarcophagus, the New Safe Confinement is designed to allow the reactor to be safely dismantled using remotely operated equipment.By 2002, roughly 15,000 Ukrainian workers were still working within the Zone of Exclusion, maintaining the plant and performing other containment- and research-related tasks, often in dangerous conditions.A handful of Ukrainian scientists work inside the sarcophagus, but outsiders are rarely granted access. In 2006 an Australian 60 Minutes team led by reporter Richard Carleton and producer Stephen Rice were allowed to enter the sarcophagus for 15 minutes and film inside the control room.On 12 February 2013, a 600 m2 (6,500 sq ft) section of the roof of the turbine-building, adjacent to the sarcophagus, collapsed. At first it was assumed that the roof collapsed because of the weight of snow on it. However the amount of snow was not exceptional, and the report of a Ukrainian fact-finding panel concluded that the part collapse of the turbine-building was the result of sloppy repair work and aging of the structure. The report mentioned the possibility that the repaired part of the turbine-building added a larger strain on the total structure than expected, and the braces in the roof were damaged by corrosion and sloppy welding. Experts such as Valentin Kupny, former deputy director of the nuclear plant, did warn that the complex was on the verge of a collapse, leaving the building in an extremely dangerous condition. A proposed reinforcement in 2005 was cancelled by a superior official. After the 12 February incident, radioactivity levels were up to 19 becquerels per cubic meter of air: 12 times normal. The report assumed radioactive materials from inside the structure spread to the surroundings after the roof collapsed. All 225 workers employed by the Chernobyl complex and the French company that is building the new shelter were evacuated shortly after the collapse. According to the managers of the complex, radiation levels around the plant were at normal levels (between 5 and 6 µSv/h) and should not affect workers' health. According to Kupny the situation was underestimated by the Chernobyl nuclear complex managers, and information was kept secret.As of  2006, some fuel remained in the reactors at units 1 through 3, most of it in each unit's spent fuel pool, as well as some material in a small spent fuel interim storage facility pond (ISF-1).In 1999 a contract was signed for construction of a radioactive waste management facility to store 25,000 used fuel assemblies from units 1–3 and other operational wastes, as well as material from decommissioning units 1–3 (which will be the first RBMK units decommissioned anywhere). The contract included a processing facility able to cut the RBMK fuel assemblies and to put the material in canisters, which were to be filled with inert gas and welded shut.The canisters were to be transported to dry storage vaults, where the fuel containers would be enclosed for up to 100 years. This facility, treating 2500 fuel assemblies per year, would be the first of its kind for RBMK fuel. However, after a significant part of the storage structures had been built, technical deficiencies in the concept emerged, and the contract was terminated in 2007. The interim spent fuel storage facility (ISF-2) will now be completed by others by mid-2013.Another contract has been let for a liquid radioactive waste treatment plant, to handle some 35,000 cubic meters of low- and intermediate-level liquid wastes at the site. This will need to be solidified and eventually buried along with solid wastes on site.In January 2008, the Ukrainian government announced a 4-stage decommissioning plan that incorporates the above waste activities and progresses towards a cleared site.According to official estimates, about 95% of the fuel in Reactor 4 at the time of the accident (about 180 metric tons) remains inside the shelter, with a total radioactivity of nearly 18 million curies (670 PBq). The radioactive material consists of core fragments, dust, and lava-like ""fuel containing materials"" (FCM)—also called ""corium""—that flowed through the wrecked reactor building before hardening into a ceramic form.Three different lavas are present in the basement of the reactor building: black, brown, and a porous ceramic. The lava materials are silicate glasses with inclusions of other materials within them. The porous lava is brown lava that dropped into water and thus cooled rapidly.It is unclear how long the ceramic form will retard the release of radioactivity. From 1997 to 2002 a series of published papers suggested that the self-irradiation of the lava would convert all 1,200 metric tons into a submicrometre and mobile powder within a few weeks. But it has been reported that the degradation of the lava is likely to be a slow and gradual process rather than sudden and rapid. The same paper states that the loss of uranium from the wrecked reactor is only 10 kg (22 lb) per year this low rate of uranium leaching suggests that the lava is resisting its environment. The paper also states that when the shelter is improved, the leaching rate of the lava will decrease.Some of the surfaces of the lava flows have started to show new uranium minerals such as čejkaite (Na4(UO2)(CO3)3) and uranyl carbonate. However, the level of radioactivity is such that during 100 years, the lava's self irradiation (2×1016 α decays per gram and 2 to 5×105 Gy of β or γ) will fall short of the level required to greatly change the properties of glass (1018 α decays per gram and 108 to 109 Gy of β or γ). Also the lava's rate of dissolution in water is very low (10−7 g·cm−2·day−1), suggesting that the lava is unlikely to dissolve in water.An area originally extending 30 kilometres (19 mi) in all directions from the plant is officially called the ""zone of alienation"". It is largely uninhabited, except for about 300 residents who have refused to leave. The area has largely reverted to forest, and has been overrun by wildlife because of a lack of competition with humans for space and resources. Even today, radiation levels are so high that the workers responsible for rebuilding the sarcophagus are only allowed to work five hours a day for one month before taking 15 days of rest. Ukrainian officials estimated the area would not be safe for human life again for another 20,000 years (although by 2016, 187 local Ukrainians had returned and were living permanently in the zone).In 2011 Ukraine opened up the sealed zone around the Chernobyl reactor to tourists who wish to learn more about the tragedy that occurred in 1986. Sergii Mirnyi, a radiation reconnaissance officer at the time of the accident, and now an academic at National University of Kyiv-Mohyla Academy in Kiev, Ukraine, has written about the psychological and physical effects on survivors and visitors, and worked as an advisor to Chernobyl tourism groups.If the forests that have been contaminated by radioactive material catch on fire, they will spread the radioactive material further outwards in the smoke.The Chernobyl Shelter Fund was established in 1997 at the Denver 23rd G8 summit to finance the Shelter Implementation Plan (SIP). The plan calls for transforming the site into an ecologically safe condition by means of stabilization of the sarcophagus followed by construction of a New Safe Confinement (NSC). While the original cost estimate for the SIP was US$768 million, the 2006 estimate was $1.2 billion. The SIP is being managed by a consortium of Bechtel, Battelle, and Électricité de France, and conceptual design for the NSC consists of a movable arch, constructed away from the shelter to avoid high radiation, to be slid over the sarcophagus. The NSC was moved into position in November 2016 and is expected to be completed in late 2017, and is the largest movable structure ever built.Dimensions:Span: 270 m (886 ft)Height: 100 m (330 ft)Length: 150 m (492 ft)The United Nations Development Programme has launched in 2003 a specific project called the Chernobyl Recovery and Development Programme (CRDP) for the recovery of the affected areas. The programme was initiated in February 2002 based on the recommendations in the report on Human Consequences of the Chernobyl Nuclear Accident. The main goal of the CRDP's activities is supporting the Government of Ukraine in mitigating long-term social, economic, and ecological consequences of the Chernobyl catastrophe. CRDP works in the four most Chernobyl-affected areas in Ukraine: Kyivska, Zhytomyrska, Chernihivska and Rivnenska.The International Project on the Health Effects of the Chernobyl Accident (IPEHCA) was created and received US $20 million, mainly from Japan, in hopes of discovering the main cause of health problems due to 131I radiation. These funds were divided among Ukraine, Belarus, and Russia, the three main affected countries, for further investigation of health effects. As there was significant corruption in former Soviet countries, most of the foreign aid was given to Russia, and no positive outcome from this money has been demonstrated.Chernobyl Children International (CCI) is a United Nations-accredited, non-profit, international development, medical, and humanitarian organization that works with children, families and communities that continue to be affected by the economic outcome of the Chernobyl accident. The organization's founder and chief executive is Adi Roche. The CCI was founded in 1991 in response to an appeal from Ukrainian and Belarusian doctors for aid. Roche then began organizing 'rest and recuperation' holidays for a few Chernobyl children. Recruiting Irish families who would welcome and care for them, CCI expanded into the United States in 2001.It works closely with the Belarusian government, the United Nations, and many thousands of volunteers worldwide to deliver a broad range of economic supports to the children and the wider community. It also acts as an advocate for the rights of those affected by the Chernobyl explosion, and engages in research and outreach activities to encourage the rest of the world to remember the victims and understand the long-term impact on their lives.The Front Veranda (1986), a lithograph by Susan Dorothea White in the National Gallery of Australia, exemplifies worldwide awareness of the event. Heavy Water: A Film for Chernobyl was released by Seventh Art in 2006 to commemorate the disaster through poetry and first-hand accounts. The film secured the Best Short Documentary at Cinequest Film Festival as well as the Rhode Island ""best score"" award along with a screening at Tate Modern.Chernobyl Way is an annual rally run on 26 April by the opposition in Belarus as a remembrance of the Chernobyl disaster.The Chernobyl accident attracted a great deal of interest. Because of the distrust that many people (both within and outside the USSR) had in the Soviet authorities, a great deal of debate about the situation at the site occurred in the First World during the early days of the event. Because of defective intelligence based on photographs taken from space, it was thought that unit number three had also suffered a dire accident.Journalists mistrusted many professionals (such as the spokesman from the UK NRPB), and they in turn encouraged the public to mistrust them.In Italy, the Chernobyl accident was reflected in the outcome of the 1987 referendum. As a result of that referendum, Italy began phasing out its nuclear power plants in 1988, a decision that was effectively reversed in 2008. A referendum in 2011 reiterated Italians' strong objections to nuclear power, thus abrogating the government's decision of 2008.In Germany, the Chernobyl accident led to the creation of a federal environment ministry, after several states had already created such a post. The minister was given the authority over reactor safety as well, which the current minister still holds as of 2015. The events are also credited with strengthening the anti-nuclear power movement, which culminated in the decision to end the use of nuclear power that was made by the 1998–2005 Schröder government.Explanatory notesCitationsSourcesThe source documents relating to the emergency, published in unofficial sources:Technological Regulations on operation of 3 and 4 power units of Chernobyl NPP (in force at the moment of emergency)Tables and graphs of some parameters variation of the unit before the emergencyOfficial UN Chernobyl siteInternational Chernobyl Portal chernobyl.info, UN Inter-Agency Project ICRINFrequently Asked Chernobyl Questions, by the IAEAChernobyl Recovery and Development Programme (United Nations Development Programme)Photographs from inside the zone of alienation and City of Prypyat (2010)Photographs from inside the Chernobyl Reactor and City of PrypyatPhotographs of those affected by the Chernobyl DisasterPhotographs from the City of Pripyat, and of those affected by the disasterEnglishRussia Photos of a RBMK-based power plant, showing details of the reactor hall, pumps, and the control roomPost-Soviet Pollution: Effects of Chernobyl from the Dean Peter Krogh Foreign Affairs Digital Archives";environmental disaster;Chernobyl disaster;0
"Collapse: How Societies Choose to Fail or Succeed (titled Collapse: How Societies Choose to Fail or Survive for the British edition) is a 2005 book by academic and popular science author Jared Diamond, in which Diamond first defines collapse:  ""a drastic decrease in human population size and/or political/economic/social complexity, over a considerable area, for an extended time.""  He then reviews the causes of historical and pre-historical instances of societal collapse — particularly those involving significant influences from environmental changes, the effects of climate change, hostile neighbors, trade partners, and the society's response to the foregoing four challenges— and considers the success or failure different societies have had in coping with such threats.While the bulk of the book is concerned with the demise of these historical civilizations, Diamond also argues that humanity collectively faces, on a much larger scale, many of the same issues, with possibly catastrophic near-future consequences to many of the world's populations.In the prologue, Jared Diamond summarizes his methodology in one paragraph:This book employs the comparative method to understand societal collapses to which environmental problems contribute. My previous book (Guns, Germs, and Steel: The Fates of Human Societies), had applied the comparative method to the opposite problem: the differing rates of buildup of human societies on different continents over the last 13,000 years. In the present book focusing on collapses rather than buildups, I compare many past and present societies that differed with respect to environmental fragility, relations with neighbors, political institutions, and other ""input"" variables postulated to influence a society's stability. The ""output"" variables that I examine are collapse or survival, and form of the collapse if collapse does occur. By relating output variables to input variables, I aim to tease out the influence of possible input variables on collapses.Diamond identifies five factors that contribute to collapse: climate change, hostile neighbours, collapse of essential trading partners, environmental problems, and the society's response to the forgoing four factors.The root problem in all but one of Diamond's factors leading to collapse is overpopulation relative to the practicable (as opposed to the ideal theoretical) carrying capacity of the environment. One environmental problem not related to overpopulation is the harmful effect of accidental or intentional introduction of non-native species to a region.Diamond also writes about cultural factors (values), such as the apparent reluctance of the Greenland Norse to eat fish. Diamond also states that ""it would be absurd to claim that environmental damage must be a major factor in all collapses: the collapse of the Soviet Union is a modern counter-example, and the destruction of Carthage by Rome in 146 BC is an ancient one. It's obviously true that military or economic factors alone may suffice"".He also lists twelve environmental problems facing humankind today. The first eight have historically contributed to the collapse of past societies:Deforestation and habitat destructionSoil problems (erosion, salinization, and soil fertility losses)Water management problemsOverhuntingOverfishingEffects of introduced species on native speciesOverpopulationIncreased per-capita impact of peopleFurther, he says four new factors may contribute to the weakening and collapse of present and future societies: Anthropogenic climate change Buildup of toxins in the environment Energy shortages Full human use of the Earth’s photosynthetic capacityIn the last chapter, he discusses environmental problems facing modern societies and addresses objections that are often given to dismiss the importance of environmental problems (section ""One-liner objections""). In the ""Further readings"" section, he gives suggestions to people who ask ""What can I do as an individual?"". He also draws conclusions, such as:In fact, one of the main lesson to be learned from the collapses of the Maya, Anasazi, Easter Islanders, and those other past societies [...] is that a society's steep decline may begin only a decade or two after the society reaches its peak numbers, wealth, and power. [...] The reason is simple: maximum population, wealth, resource consumption, and waste production mean maximum environmental impact, approaching the limit where impact outstrips resources.Finally, he answers the question, ""What are the choices that we must make if we are to succeed, and not to fail?"" by identifying two crucial choices distinguishing the past societies that failed from those that survived:Long-term planning: ""[...] the courage to practice long-term thinking, and to make bold, courageous, anticipatory decisions at a time when problems have become perceptible but before they have reached crisis proportions.""Willingness to reconsider core values: ""[...] the courage to make painful decisions about values. Which of the values that formerly served a society well can continue to be maintained under new changed circumstances? Which of these treasured values must instead be jettisoned and replaced with different approaches?""Collapse is divided into four parts.Part One describes the environment of the US state of Montana, focusing on the lives of several individuals to put a human face on the interplay between society and the environment.Part Two describes past societies that have collapsed. Diamond uses a ""framework"" when considering the collapse of a society, consisting of five ""sets of factors"" that may affect what happens to a society: environmental damage, climate change, hostile neighbors, loss of trading partners, and the society's  responses to its environmental problems. A recurrent problem in collapsing societies is a structure that creates ""a conflict between the short-term interests of those in power, and the long-term interests of the society as a whole.""The societies Diamond describes are:The Greenland Norse (cf. Hvalsey Church) (climate change, environmental damage, loss of trading partners,  hostile neighbors, irrational reluctance to eat fish, chiefs looking after their short-term interests).Easter Island (a society that collapsed entirely due to environmental damage)The Polynesians of Pitcairn Island (environmental damage and loss of trading partners)The Anasazi of southwestern North America (environmental damage and climate change)The Maya of Central America (environmental damage, climate change, and hostile neighbors)Finally, Diamond discusses three past success stories:The tiny egalitarian Pacific island of TikopiaThe agricultural success of egalitarian central New GuineaThe forest management in stratified Japan of the Tokugawa-era, and in Germany.Part Three examines modern societies, including:The collapse into genocide of Rwanda, caused in part by overpopulationThe failure of Haiti compared with the relative success of its neighbor on Hispaniola, the Dominican RepublicThe problems facing a developing nation, ChinaThe problems facing a First World nation, AustraliaPart Four concludes the study by considering such subjects as business and globalization, and ""extracts practical lessons for us today"" (pp. 22–23). Specific attention is given to the polder model as a way Dutch society has addressed its challenges and the ""top-down"" and most importantly ""bottom-up"" approaches that we must take now that ""our world society is presently on a non-sustainable course"" (p. 498) in order to avoid the ""12 problems of non-sustainability"" that he expounds throughout the book, and reviews in the final chapter. The results of this survey are perhaps why Diamond sees ""signs of hope"" nevertheless and arrives at a position of ""cautious optimism"" for all our futures.Tim Flannery gave Collapse the highest praise in Science, writing:While he planned the book, Diamond at first thought that it would deal only with human impacts on the environment. Instead, what has emerged is arguably the most incisive study of senescing human civilizations ever written. [...] the fact that one of the world's most original thinkers has chosen to pen this mammoth work when his career is at his apogee is itself a persuasive argument that Collapse must be taken seriously. It is probably the most important book you will ever read.The Economist's review was generally favorable, although the reviewer had two disagreements. First, the reviewer felt Diamond was not optimistic enough about the future. Secondly, the reviewer claimed Collapse contains some erroneous statistics: for instance, Diamond purportedly overstated the number of starving people in the world. University of British Columbia professor of ecological planning William Rees wrote that Collapse's most important lesson is that societies most able to avoid collapse are the ones that are most agile, able to adopt practices favorable to their own survival and avoid unfavorable ones. Moreover, Rees wrote that Collapse is ""a necessary antidote"" to followers of Julian Simon, such as Bjørn Lomborg who authored The Skeptical Environmentalist. Rees explained this assertion as follows:Human behaviour towards the ecosphere has become dysfunctional and now arguably threatens our own long-term security. The real problem is that the modern world remains in the sway of a dangerously illusory cultural myth. Like Lomborg, most governments and international agencies seem to believe that the human enterprise is somehow 'decoupling' from the environment, and so is poised for unlimited expansion. Jared Diamond's new book, Collapse, confronts this contradiction head-on.Jennifer Marohasy of the think-tank, Institute of Public Affairs wrote a critical review in Energy & Environment, in particular its chapter on Australia's environmental degradation. Marohasy claims that Diamond reflects a popular view that is reinforced by environmental campaigning in Australia, but is not supported by evidence, and argues that many of his claims are easily disproved.In his review in The New Yorker, Malcolm Gladwell highlights the way Diamond's approach differs from traditional historians by focusing on environmental issues rather than cultural questions.Diamond's distinction between social and biological survival is a critical one, because too often we blur the two, or assume that biological survival is contingent on the strength of our civilizational values... The fact is, though, that we can be law-abiding and peace-loving and tolerant and inventive and committed to freedom and true to our own values and still behave in ways that are biologically suicidal.While Diamond does not reject the approach of traditional historians, his book, according to Gladwell, vividly illustrates the limitations of that approach. Gladwell demonstrates this with his own example of a recent ballot initiative in Oregon, where questions of property rights and other freedoms were subject to a free and healthy debate, but serious ecological questions were given scant attention.In 2006 the book was shortlisted for The Aventis Prizes for Science Books award, eventually losing out to David Bodanis' Electric Universe.Jared Diamond's thesis that Easter Island society collapsed in isolation entirely due to environmental damage and cultural inflexibility is contested by some ethnographers and archaeologists, who argue that the introduction of diseases carried by European colonizers and slave raiding, which devastated the population in the 19th century, had a much greater social impact than environmental decline, and that introduced animals—first rats and then sheep—were greatly responsible for the island's loss of native flora, which came closest to deforestation as late as 1930–1960.The book Questioning Collapse (Cambridge University Press, 2010) is a collection of essays by anthropologists criticizing various aspects of Diamond's books Collapse and Guns, Germs and Steel: A short history of everybody for the last 13,000 years.In 2010, National Geographic released the documentary film Collapse based on Diamond's book.List of important publications in anthropologyList of environmental booksComplexityCreeping normalityDecline of the Roman EmpireDeforestation during the Roman periodGuns, Germs, and Steel: The Fates of Human Societies by Jared DiamondEcocideEnvironmental disasterGlobal catastrophic riskGlobal warmingHuman impact on the environmentList of environmental issuesSocietal collapseSustainabilityTwilight of the Elites: America After MeritocracyWhy do societies collapse? (2003), a TED talk by Jared Diamond""Metacritic – collection of reviews of book"". Archived from the original on 21 July 2006. The first chapterTokugawa Shoguns vs. Consumer Democracy: Diamond interview on the subjects raised in the book with NPQ, Spring 2005, concentrating on the intersection of politics and environmentalism.How Societies Fail – And Sometimes Succeed, video of a seminar given in June 2005 at the Long Now Foundation.Learning from Past Societies: The Sustainability Lessons Are There, If Only We Can Find Them – This is an assessment of the process maturity used in Collapse and a similar book, Treading Lightly, to answer their driving questions. The assessment sheds light on the process maturity of any similar effort to solve difficult complex social system problems, particularly the sustainability problem.COLLAPSE? – museum exhibit developed by the Natural History Museum of Los Angeles County in collaboration with Jared Diamond (pdf archive)Environmental-issues – A public annotated bibliography containing print and online sources discussing the 12 most serious environmental problems that Diamond discusses in Collapse.";environmental disaster;Collapse: How Societies Choose to Fail or Succeed;0
"The Deepwater Horizon oil spill (also referred to as the BP oil spill/leak, the BP oil disaster, the Gulf of Mexico oil spill, and the Macondo blowout) is an industrial disaster that began on 20 April 2010, in the Gulf of Mexico on the BP-operated Macondo Prospect, considered to be the largest marine oil spill in the history of the petroleum industry and estimated to be 8% to 31% larger in volume than the previous largest, the Ixtoc I oil spill. The U.S. government estimated the total discharge at 4.9 million barrels (210 million US gal 780,000 m3). After several failed efforts to contain the flow, the well was declared sealed on 19 September 2010. Reports in early 2012 indicated that the well site was still leaking.A massive response ensued to protect beaches, wetlands and estuaries from the spreading oil utilizing skimmer ships, floating booms, controlled burns and 1.84 million US gallons (7,000 m3) of oil dispersant. Due to the months-long spill, along with adverse effects from the response and cleanup activities, extensive damage to marine and wildlife habitats and fishing and tourism industries was reported.   In Louisiana, 4,900,000 pounds (2,200 t) of oily material was removed from the beaches in 2013, over double the amount collected in 2012. Oil cleanup crews worked four days a week on 55 miles (89 km) of Louisiana shoreline throughout 2013. Oil continued to be found as far from the Macondo site as the waters off the Florida Panhandle and Tampa Bay, where scientists said the oil and dispersant mixture is embedded in the sand.  In April 2013, it was reported that dolphins and other marine life continued to die in record numbers with infant dolphins dying at six times the normal rate.  One study released in 2014 reported that tuna and amberjack that were exposed to oil from the spill developed deformities of the heart and other organs that would be expected to be fatal or at least life-shortening and another study found that cardiotoxicity might have been widespread in animal life exposed to the spill.Numerous investigations explored the causes of the explosion and record-setting spill.  The U.S. government September 2011 report pointed to defective cement on the well, faulting mostly BP, but also rig operator Transocean and contractor Halliburton. Earlier in 2011, a White House commission likewise blamed BP and its partners for a series of cost-cutting decisions and an inadequate safety system, but also concluded that the spill resulted from ""systemic"" root causes and ""absent significant reform in both industry practices and government policies, might well recur"".In November 2012, BP and the United States Department of Justice settled federal criminal charges with BP pleading guilty to 11 counts of manslaughter, two misdemeanors, and a felony count of lying to Congress.  BP also agreed to four years of government monitoring of its safety practices and ethics, and the Environmental Protection Agency announced that BP would be temporarily banned from new contracts with the US government.  BP and the Department of Justice agreed to a record-setting $4.525 billion in fines and other payments.  As of February 2013, criminal and civil settlements and payments to a trust fund had cost the company $42.2 billion.In September 2014, a U.S. District Court judge ruled that BP was primarily responsible for the oil spill because of its gross negligence and reckless conduct. In July 2015, BP agreed to pay $18.7 billion in fines, the largest corporate settlement in U.S. history.The Deepwater Horizon was a 10-year-old semi-submersible, mobile, floating, dynamically positioned drilling rig that could operate in waters up to 10,000 feet (3,000 m) deep. Built by South Korean company Hyundai Heavy Industries and owned by Transocean, the rig operated under the Marshallese flag of convenience, and was chartered to BP from March 2008 to September 2013. It was drilling a deep exploratory well, 18,360 feet (5,600 m) below sea level, in approximately 5,100 feet (1,600 m) of water. The well is situated in the Macondo Prospect in Mississippi Canyon Block 252 (MC252) of the Gulf of Mexico, in the United States' exclusive economic zone. The Macondo well is found roughly 41 miles (66 km) off the Louisiana coast. BP was the operator and principal developer of the Macondo Prospect with a 65% share, while 25% was owned by Anadarko Petroleum Corporation, and 10% by MOEX Offshore 2007, a unit of Mitsui.At approximately 9:45 pm CDT, on 20 April 2010, high-pressure methane gas from the well expanded into the drilling riser and rose into the drilling rig, where it ignited and exploded, engulfing the platform. At the time, 126 crew members were on board: seven BP employees, 79 of Transocean, and employees of various other companies.  Eleven missing workers were never found despite a three-day U.S. Coast Guard (USCG) search operation and are believed to have died in the explosion. Ninety-four crew members were rescued by lifeboat or helicopter, 17 of whom were treated for injuries. The Deepwater Horizon sank on the morning of 22 April 2010.The oil leak was discovered on the afternoon of 22 April 2010 when a large oil slick began to spread at the former rig site. The oil flowed for 87 days. BP originally estimated a flow rate of 1,000 to 5,000 barrels per day (160 to 790 m3/d). The Flow Rate Technical Group (FRTG) estimated the initial flow rate was 62,000 barrels per day (9,900 m3/d). The total estimated volume of leaked oil approximated 4.9 million barrels (210 million US gal 780,000 m3) with plus or minus 10% uncertainty, including oil that was collected, making it the world’s largest accidental spill. BP challenged the higher figure, saying that the government overestimated the volume. Internal emails released in 2013 showed that one BP employee had estimates that matched those of the FRTG, and shared the data with supervisors, but BP continued with their lower number. The company argued that government figures do not reflect over 810,000 barrels (34 million US gal 129,000 m3) of oil that was collected or burned before it could enter the Gulf waters.According to the satellite images, the spill directly impacted 68,000 square miles (180,000 km2) of ocean, which is comparable to the size of Oklahoma. By early June 2010, oil had washed up on 125 miles (201 km) of Louisiana's coast and along the Mississippi, Florida, and Alabama coastlines. Oil sludge appeared in the Intracoastal Waterway and on Pensacola Beach and the Gulf Islands National Seashore. In late June, oil reached Gulf Park Estates, its first appearance in Mississippi. In July, tar balls reached Grand Isle and the shores of Lake Pontchartrain. In September a new wave of oil suddenly coated 16 miles (26 km) of Louisiana coastline and marshes west of the Mississippi River in Plaquemines Parish. In October, weathered oil reached Texas. As of July 2011, about 491 miles (790 km) of coastline in Louisiana, Mississippi, Alabama and Florida were contaminated by oil and a total of 1,074 miles (1,728 km) had been oiled since the spill began. As of December 2012, 339 miles (546 km) of coastline remain subject to evaluation and/or cleanup operations.Concerns were raised about the appearance of underwater, horizontally extended plumes of dissolved oil. Researchers concluded that deep plumes of dissolved oil and gas would likely remain confined to the northern Gulf of Mexico and that the peak impact on dissolved oxygen would be delayed and long lasting.Two weeks after the wellhead was capped on 15 July 2010, the surface oil appeared to have dissipated, while an unknown amount of subsurface oil remained. Estimates of the residual ranged from a 2010 NOAA report that claimed about half of the oil remained below the surface to independent estimates of up to 75%.That means that over 100 million US gallons (380 Ml) (2.4 million barrels) remained in the Gulf. As of January 2011, tar balls, oil sheen trails, fouled wetlands marsh grass and coastal sands were still evident. Subsurface oil remained offshore and in fine silts. In April 2012, oil was still found along as much as 200 miles (320 km) of Louisiana coastline and tar balls continued to wash up on the barrier islands. In 2013, some scientists at the Gulf of Mexico Oil Spill and Ecosystem Science Conference said that as much as one-third of the oil may have mixed with deep ocean sediments, where it risks damage to ecosystems and commercial fisheries.In 2013, more than 4,600,000 pounds (2,100 t) of ""oiled material"" was removed from the Louisiana coast. Although only ""minute"" quantities of oil continued to wash up in 2013, patches of tar balls were still being reported almost every day from Alabama and Florida Panhandle beaches. Regular cleanup patrols were no longer considered justified but cleanup was being conducted on an as-needed basis, in response to public reports.It was first thought that oil had not reached as far as Tampa Bay, Florida however, a study done in 2013 found that one of the plumes of dispersant-treated oil had reached a shelf 80 miles (130 km) off the Tampa Bay region.  According to researchers, there is ""some evidence it may have caused lesions in fish caught in that area"".First BP unsuccessfully attempted to close the blowout preventer valves on the wellhead with remotely operated underwater vehicles. Next it placed a 125-tonne (280,000 lb) containment dome over the largest leak and piped the oil to a storage vessel. While this technique had worked in shallower water, it failed here when gas combined with cold water to form methane hydrate crystals that blocked the opening at the top of the dome. Pumping heavy drilling fluids into the blowout preventer to restrict the flow of oil before sealing it permanently with cement (""top kill"") also failed.BP then inserted a riser insertion tube into the pipe and a stopper-like washer around the tube plugged at the end of the riser and diverted the flow into the insertion tube. The collected gas was flared and oil stored on board the drillship Discoverer Enterprise. Before the tube was removed, it collected 924,000 US gallons (22,000 bbl 3,500 m3) of oil. On 3 June 2010, BP removed the damaged drilling riser from the top of the blowout preventer and covered the pipe by the cap which connected it to another riser. On 16 June a second containment system connected directly to the blowout preventer began carrying oil and gas to service vessels, where it was consumed in a clean-burning system. The United States government's estimates suggested the cap and other equipment were capturing less than half of the leaking oil. On 10 July the containment cap was removed to replace it with a better-fitting cap (""Top Hat Number 10""). Mud and cement were later pumped in through the top of the well to reduce the pressure inside it which didn't work either. A final device was created to attach a chamber of larger diameter than the flowing pipe with a flange that bolted to the top of the blowout preventer and a manual valve set to close off the flow once attached. On 15 July the device was secured and time was taken closing the valves to ensure the attachment under increasing pressure until the valves were closed completing the temporary measures.In mid-May, United States Secretary of Energy Steven Chu assembled a team of nuclear physicists, including hydrogen bomb designer Richard Garwin and Sandia National Laboratories director Tom Hunter. Oil expert Matthew Simmons maintained that a nuclear explosion was the only way BP could permanently seal the well and cited successful Soviet attempts to seal off runaway gas wells with nuclear blasts. A spokesperson for the U.S. Energy Department said that ""neither Energy Secretary Steven Chu nor anyone else"" ever considered this option. On 24 May BP ruled out conventional explosives, claiming that if blasts failed to clog the well, ""we would have denied ourselves all other options.""Transocean's Development Driller III started drilling a first relief well on 2 May 2010. GSF Development Driller II started drilling a second relief on 16 May 2010. On 3 August 2010, first test oil and then drilling mud was pumped at a slow rate of approximately 2 barrels (320 L) per minute into the well-head. Pumping continued for eight hours, at the end of which time the well was declared to be ""in a static condition."" On 4 August 2010, BP began pumping cement from the top, sealing that part of the flow channel permanently.On 3 September 2010, the 300-ton failed blowout preventer was removed from the well and a replacement blowout preventer was installed. On 16 September 2010, the relief well reached its destination and pumping of cement to seal the well began. On 19 September 2010, National Incident Commander Thad Allen declared the well ""effectively dead"" and said that it posed no further threat to the Gulf.In May 2010, BP admitted they had ""discovered things that were broken in the sub-surface"" during the ""top kill"" effort.Oil slicks were reported in March and August 2011, in March and October 2012, and in January 2013. Repeated scientific analyses confirmed that the sheen was a chemical match for oil from Macondo well.The USCG initially said the oil was too dispersed to recover and posed no threat to the coastline, but later warned BP and Transocean that they might be held financially responsible for cleaning up the new oil. USGS director Marcia McNutt stated that the riser pipe could hold at most 1,000 barrels (160 m3) because it is open on both ends, making it unlikely to hold the amount of oil being observed.In October 2012, BP reported that they had found and plugged leaking oil from the failed containment dome, now abandoned about 1,500 feet (460 m) from the main well. In December 2012, the USCG conducted a subsea survey no oil coming from the wells or the wreckage was found and its source remains unknown. In addition, white, milky substance was observed seeping from the wreckage. According to BP and the USCG, it is ""not oil and it's not harmful.""In January 2013, BP said that they were continuing to investigate possible sources of the oil sheen. Chemical data implied that the substance might be residual oil leaking from the wreckage. If that proves to be the case, the sheen can be expected to eventually disappear. Another possibility is that it is formation oil escaping from the subsurface, using the Macondo well casing as flow conduit, possibly intersecting a naturally occurring fault, and then following that to escape at the surface some distance from the wellhead. If it proves to be oil from the subsurface, then that could indicate the possibility of an indefinite release of oil. The oil slick was comparable in size to naturally occurring oil seeps and was not large enough to pose an immediate threat to wildlife.The fundamental strategies for addressing the spill were containment, dispersal and removal.  In summer 2010, approximately 47,000 people and 7,000 vessels were involved in the project. By 3 October 2012, federal response costs amounted to $850 million, mostly reimbursed by BP.  As of January 2013, 935 personnel were still involved. By that time cleanup had cost BP over $14 billion.It was estimated with plus-or-minus 10% uncertainty that 4.9 million barrels (780,000 m3) of oil was released from the well 4.1 million barrels (650,000 m3) of oil went into the Gulf. The report led by the Department of the Interior and the NOAA said that ""75% [of oil] has been cleaned up by Man or Mother Nature"" however, only about 25% of released oil was collected or removed while about 75% of oil remained in the environment in one form or another. In 2012, Markus Huettel, a benthic ecologist at Florida State University, maintained that while much of BP's oil was degraded or evaporated, at least 60% remains unaccounted for.In May 2010, a local native set up a network for people to volunteer their assistance in cleaning up beaches. Boat captains were given the opportunity to offer the use of their boat to help clean and prevent the oil from further spreading. To assist with the efforts the captains had to register their ships with the Vessels of Opportunity, however an issue arose when more boats registered than actually participated in the clean up efforts - only a third of the registered boats. Many local supporters were disappointed with BP's slow response, prompting the formation of The Florida Key Environmental Coalition.  This coalition gained significant influence in the clean up of the oil spill to try and gain some control over the situation. Containment booms stretching over 4,200,000 feet (1,300 km) were deployed, either to corral the oil or as barriers to protect marshes, mangroves, shrimp/crab/oyster ranches or other ecologically sensitive areas. Booms extend 18–48 inches (0.46–1.22 m) above and below the water surface and were effective only in relatively calm and slow-moving waters. Including one-time use sorbent booms, a total of 13,300,000 feet (4,100 km) of booms were deployed. Booms were criticized for washing up on the shore with the oil, allowing oil to escape above or below the boom, and for ineffectiveness in more than three to four-foot (90–120 cm) waves.The Louisiana barrier island plan was developed to construct barrier islands to protect the coast of Louisiana. The plan was criticised for its expense and poor results. Critics allege that the decision to pursue the project was political with little scientific input. The EPA expressed concern that the booms would threaten wildlife.For a time, a group called Matter of Trust, citing insufficient availability of manufactured oil absorption booms, campaigned to encourage hair salons, dog groomers and sheep farmers to donate hair, fur and wool clippings, stuffed in pantyhose or tights, to help contain oil near impacted shores, a technique dating back to the Exxon Valdez disater.The spill was also notable for the volume of Corexit oil dispersant used and for application methods that were ""purely experimental."" Altogether, 1.84 million US gallons (7,000 m3) of dispersants were used of this 771,000 US gallons (2,920 m3) were released at the wellhead. Subsea injection had never previously been tried but due to the spill's unprecedented nature BP together with USCG and EPA decided to use it. Over 400 sorties were flown to release the product. Although usage of dispersants was described as ""the most effective and fast moving tool for minimizing shoreline impact"", the approach continues to be investigated.A 2011 analysis conducted by Earthjustice and Toxipedia showed that the dispersant could contain cancer-causing agents, hazardous toxins and endocrine-disrupting chemicals. Environmental scientists expressed concerns that the dispersants add to the toxicity of a spill, increasing the threat to sea turtles and bluefin tuna. The dangers are even greater when poured into the source of a spill, because they are picked up by the current and wash through the Gulf. According to BP and federal officials, dispersant use stopped after the cap was in place however, marine toxicologist Riki Ott wrote in an open letter to the EPA that Corexit use continued after that date and a GAP investigation stated that ""[a] majority of GAP witnesses cited indications that Corexit was used after [July 2010].""According to a NALCO manual obtained by GAP, Corexit 9527 is an “eye and skin irritant. Repeated or excessive exposure ... may cause injury to red blood cells (hemolysis), kidney or the liver.” The manual adds: “Excessive exposure may cause central nervous system effects, nausea, vomiting, anesthetic or narcotic effects.” It advises, “Do not get in eyes, on skin, on clothing,” and “Wear suitable protective clothing.” For Corexit 9500 the manual advised, “Do not get in eyes, on skin, on clothing,” “Avoid breathing vapor,” and “Wear suitable protective clothing.” According to FOIA requests obtained by GAP, neither the protective gear nor the manual were distributed to Gulf oil spill cleanup workers.Corexit EC9500A and Corexit EC9527A were the principal variants. The two formulations are neither the least toxic, nor the most effective, among EPA's approved dispersants, but BP said it chose to use Corexit because it was available the week of the rig explosion. On 19 May, the EPA gave BP 24 hours to choose less toxic alternatives to Corexit from the National Contingency Plan Product Schedule, and begin applying them within 72 hours of EPA approval or provide a detailed reasoning why no approved products met the standards. On 20 May, BP determined that none of the alternative products met all three criteria of availability, non-toxicity and effectiveness. On 24 May, EPA Administrator Lisa P. Jackson ordered EPA to conduct its own evaluation of alternatives and ordered BP to reduce dispersant use by 75%. BP reduced Corexit use by 25,689 to 23,250 US gallons (97,240 to 88,010 L) per day, a 9% decline. On 2 August 2010, the EPA said dispersants did no more harm to the environment than the oil and that they stopped a large amount of oil from reaching the coast by breaking it down faster. However, some independent scientists and EPA's own experts continue to voice concerns about the approach.Underwater injection of Corexit into the leak may have created the oil plumes which were discovered below the surface. Because the dispersants were applied at depth, much of the oil never rose to the surface. One plume was 22 miles (35 km) long, more than 1 mile (1,600 m) wide and 650 feet (200 m) deep. In a major study on the plume, experts were most concerned about the slow pace at which the oil was breaking down in the cold, 40 °F (4 °C) water at depths of 3,000 feet (900 m).In late 2012, a study from Georgia Tech and Universidad Autonoma de Aguascalientes in Environmental Pollution journal reported that Corexit used during the BP oil spill had increased the toxicity of the oil by 52 times.  The scientists concluded that ""Mixing oil with dispersant increased toxicity to ecosystems"" and made the gulf oil spill worse.""The three basic approaches for removing the oil from the water were: combustion, offshore filtration, and collection for later processing. USCG said 33 million US gallons (120,000 m3) of tainted water was recovered, including 5 million US gallons (19,000 m3) of oil. BP said 826,800 barrels (131,450 m3) had been recovered or flared. It is calculated that about 5% of leaked oil was burned at the surface and 3% was skimmed. On the most demanding day 47,849 people were assigned on the response works.From April to mid-July 2010, 411 controlled in-situ fires remediated approximately 265,000 barrels (11.1 million US gal 42,100 m3). The fires released small amounts of toxins, including cancer-causing dioxins. According to EPA's report, the released amount is not enough to pose an added cancer risk to workers and coastal residents, while a second research team concluded that there was only a small added risk.Oil was collected from water by using skimmers. In total 2,063 various skimmers were used. For offshore, more than 60 open-water skimmers were deployed, including 12 purpose-built vehicles. EPA regulations prohibited skimmers that left more than 15 parts per million (ppm) of oil in the water. Many large-scale skimmers exceeded the limit. Due to use of Corexit the oil was too dispersed to collect, according to a spokesperson for shipowner TMT. In mid-June 2010, BP ordered 32 machines that separate oil and water, with each machine capable of extracting up to 2,000 barrels per day (320 m3/d). After one week of testing, BP began to proceed and by 28 June, had removed 890,000 barrels (141,000 m3).After the well was captured, the cleanup of shore became the main task of the response works. Two main types of affected coast were sandy beaches and marshes. On beaches the main techniques were sifting sand, removing tar balls, and digging out tar mats manually or by using mechanical devices. For marshes, techniques such as vacuum and pumping, low-pressure flush, vegetation cutting, and bioremediation were used.Dispersants are said to facilitate the digestion of the oil by microbes. Mixing dispersants with oil at the wellhead would keep some oil below the surface and in theory, allowing microbes to digest the oil before it reached the surface. Various risks were identified and evaluated, in particular that an increase in microbial activity might reduce subsea oxygen levels, threatening fish and other animals.Several studies suggest that microbes successfully consumed part of the oil. By mid-September, other research claimed that microbes mainly digested natural gas rather than oil. David L. Valentine, a professor of microbial geochemistry at UC Santa Barbara, said that the capability of microbes to break down the leaked oil had been greatly exaggerated.  However, biogeochemist Chris Reddy, said natural microorganisms are a big reason why the oil spill in the Gulf of Mexico was not far worse.Genetically modified Alcanivorax borkumensis was added to the waters to speed digestion. The delivery method of microbes to oil patches was proposed by the Russian Research and Development Institute of Ecology and the Sustainable Use of Natural Resources.On 18 May 2010, BP was designated the lead ""Responsible Party"" under the Oil Pollution Act of 1990, which meant that BP had operational authority in coordinating the response.The first video images were released on 12 May, and further video images were released by members of Congress who had been given access to them by BP.During the spill response operations, at the request of the Coast Guard, the Federal Aviation Administration (FAA) implemented a 900-square-mile (2,300 km2) temporary flight restriction zone over the operations area. Restrictions were to prevent civilian air traffic from interfering with aircraft assisting the response effort. All flights in the operations' area were prohibited except flight authorized by air traffic control routine flights supporting offshore oil operations federal, state, local and military flight operations supporting spill response and air ambulance and law enforcement operations. Exceptions for these restrictions were granted on a case-by-case basis dependent on safety issues, operational requirements, weather conditions, and traffic volume. No flights, except aircraft conducting aerial chemical dispersing operations, or for landing and takeoff, were allowed below 1,000 metres (3,300 ft). Notwithstanding restrictions, there were 800 to 1,000 flights per day during the operations.Local and federal authorities citing BP's authority denied access to members of the press attempting to document the spill from the air, from boats, and on the ground, blocking access to areas that were open to the public. In some cases photographers were granted access only with BP officials escorting them on BP-contracted boats and aircraft. In one example, the U.S. Coast Guard stopped Jean-Michel Cousteau's boat and allowed it to proceed only after the Coast Guard was assured that no journalists were on board. In another example, a CBS News crew was denied access to the oil-covered beaches of the spill area. The CBS crew was told by the authorities: ""this is BP's rules, not ours,"" when trying to film the area. Some members of Congress criticized the restrictions placed on access by journalists.The FAA denied that BP employees or contractors made decisions on flights and access, saying those decisions were made by the FAA and Coast Guard. The FAA acknowledged that media access was limited to hired planes or helicopters, but was arranged through the Coast Guard. The Coast Guard and BP denied having a policy of restricting journalists they noted that members of the media had been embedded with the authorities and allowed to cover response efforts since the beginning of the effort, with more than 400 embeds aboard boats and aircraft to date. They also said that they wanted to provide access to the information while maintaining safety.On 15 April 2014, BP claimed that cleanup along the coast was substantially complete, but the United States Coast Guard responded by stating that a lot of work remained. Using physical barriers such as floating booms, cleanup workers’ objective was to keep the oil from spreading any further. They used skimmer boats to remove a majority of the oil and they used sorbents to absorb any remnant of oil like a sponge. Although that method did not remove the oil completely, chemicals called dispersants are used to hasten the oil’s degradation to prevent the oil from doing further damage to the marine habitats below the surface water. For the Deep Horizon oil spill, cleanup workers used 1,400,000 US gallons (5,300,000 l 1,200,000 imp gal) of various chemical dispersants to further breakdown the oil.The State of Louisiana was funded by BP to do regular testing of fish, shellfish, water, and sand. Initial testing regularly showed detectable levels of dioctyl sodium sulfosuccinate, a chemical used in the clean up. Testing over the past year reported by GulfSource.org, for the pollutants tested have not produced results.The spill area hosts 8,332 species, including more than 1,270 fish, 604 polychaetes, 218 birds, 1,456 mollusks, 1,503 crustaceans, 4 sea turtles and 29 marine mammals. Between May and June 2010, the spill waters contained 40 times more polycyclic aromatic hydrocarbons (PAHs) than before the spill. PAHs are often linked to oil spills and include carcinogens and chemicals that pose various health risks to humans and marine life. The PAHs were most concentrated near the Louisiana Coast, but levels also jumped 2–3 fold in areas off Alabama, Mississippi and Florida. PAHs can harm marine species directly and microbes used to consume the oil can reduce marine oxygen levels.  The oil contained approximately 40% methane by weight, compared to about 5% found in typical oil deposits. Methane can potentially suffocate marine life and create ""dead zones"" where oxygen is depleted.A 2014 study of the effects of the oil spill on bluefin tuna funded by National Oceanic and Atmospheric Administration (NOAA), Stanford University, and the Monterey Bay Aquarium and published in the journal Science, found that the toxins from oil spills can cause irregular heartbeats leading to cardiac arrest.  Calling the vicinity of the spill ""one of the most productive ocean ecosystems in the world"", the study found that even at very low concentrations ""PAH cardiotoxicity was potentially a common form of injury among a broad range of species in the vicinity of the oil."" Another peer-reviewed study, released in March 2014 and conducted by 17 scientists from the United States and Australia and published in the Proceedings of the National Academy of Sciences, found that tuna and amberjack that were exposed to oil from the spill developed deformities of the heart and other organs that would be expected to be fatal or at least life-shortening.The scientists said that their findings would most likely apply to other large predator fish and ""even to humans, whose developing hearts are in many ways similar.""  BP responded that the concentrations of oil in the study were a level rarely seen in the Gulf, but The New York Times reported that the BP statement was contradicted by the study.The oil dispersant Corexit, previously only used as a surface application, was released underwater in unprecedented amounts, with the intent of making it more easily biodegraded by naturally occurring microbes.  Thus, oil that would normally rise to the surface of the water was emulsified into tiny droplets and remained suspended in the water and on the sea floor.  The oil and dispersant mixture permeated the food chain through zooplankton. Signs of an oil-and-dispersant mix were found under the shells of tiny blue crab larvae.A study of insect populations in the coastal marshes affected by the spill also found a significant impact. Chemicals from the spill were found in migratory birds as far away as Minnesota. Pelican eggs contained ""petroleum compounds and Corexit"". Dispersant and PAHs from oil are believed to have caused ""disturbing numbers"" of mutated fish that scientists and commercial fishers saw in 2012, including 50% of shrimp found lacking eyes and eye sockets. Fish with oozing sores and lesions were first noted by fishermen in November 2010. Prior to the spill, approximately 0.1% of Gulf fish had lesions or sores. A report from the University of Florida said that many locations showed 20% of fish with lesions, while later estimates reached 50%. In October 2013, Al Jazeera reported that the gulf ecosystem was ""in crisis"", citing a decline in seafood catches, as well as deformities and lesions found in fish.In July 2010 it was reported that the spill was ""already having a 'devastating' effect on marine life in the Gulf"".  Damage to the ocean floor especially endangered the Louisiana pancake batfish whose range is entirely contained within the spill-affected area.  In March 2012, a definitive link was found between the death of a Gulf coral community and the spill. According to NOAA, a cetacean Unusual Mortality Event (UME) has been recognized since before the spill began, NOAA is investigating possible contributing factors to the ongoing UME from the Deepwater Horizon spill, with the possibility of eventual criminal charges being filed if the spill is shown to be connected. Some estimates are that only 2% of the carcasses of killed mammals have been recovered. In the first birthing season for dolphins after the spill, dead baby dolphins washed up along Mississippi and Alabama shorelines at about 10 times the normal number. A peer-reviewed NOAA/BP study disclosed that nearly half the bottlenose dolphins tested in mid-2011 in Barataria Bay, a heavily oiled area, were in “guarded or worse” condition, ""including 17 percent that were not expected to survive"". BP officials deny that the disease conditions are related to the spill, saying that dolphin deaths actually began being reported before the BP oil spill. By 2013, over 650 dolphins had been found stranded in the oil spill area, a four-fold increase over the historical average.  The National Wildlife Federation (NWF) reports that sea turtles, mostly endangered Kemp’s ridley sea turtles, have been stranding at a high rate.  Before the spill there were an average of 100 strandings per year since the spill the number has jumped to roughly 500.NWF senior scientist Doug Inkley notes that the marine death rates are unprecedented and occurring high in the food chain, strongly suggesting there is ""something amiss with the Gulf ecosystem"". In December 2013, the journal Environmental Science & Technology published a study finding that of 32 dolphins briefly captured from 24-km stretch near southeastern Louisiana, half were seriously ill or dying. BP said the report was “inconclusive as to any causation associated with the spill”.In 2012, tar balls continued to wash up along the Gulf coast and in 2013, tar balls could still be found in on the Mississippi and Louisiana coasts, along with oil sheens in marshes and signs of severe erosion of coastal islands, brought about by the death of trees and marsh grass from exposure to the oil.  In 2013, former NASA physicist Bonny Schumaker noted a ""dearth of marine life"" in a radius 30 to 50 miles (48 to 80 km) around the well, after flying over the area numerous times since May 2010.In 2013, researchers found that oil on the bottom of the seafloor did not seem to be degrading,  and observed a phenomenon called a ""dirty blizzard"": oil in the water column began clumping around suspended sediments, and falling to the ocean floor in an ""underwater rain of oily particles."" The result could have long-term effects because oil could remain in the food chain for generations.A 2014 bluefin tuna study in Science found that oil already broken down by wave action and chemical dispersants was more toxic than fresh oil. A 2015 study of the relative toxicity of oil and dispersants to coral also found that the dispersants were more toxic than the oil.A 2015 study by the National Oceanic and Atmospheric Administration, published in PLOS ONE, links the sharp increase in dolphin deaths to the Deepwater Horizon oil spill.On 12 April 2016, a research team reported that 88 percent of about 360 baby or stillborn dolphins within the spill area ""had abnormal or under-developed lungs"", compared to 15 percent in other areas. The study was published in the April 2016  Diseases of Aquatic Organisms.By June 2010, 143 spill-exposure cases had been reported to the Louisiana Department of Health and Hospitals 108 of those involved workers in the clean-up efforts, while 35 were reported by residents. Chemicals from the oil and dispersant are believed to be the cause it is believed that the addition of dispersants made the oil more toxic.The United States Department of Health and Human Services set up the GuLF Study in June 2010 in response to these reports. The study is run by the National Institute of Environmental Health Sciences, and will last at least five years.Mike Robicheux, a Louisiana physician, described the situation as ""the biggest public health crisis from a chemical poisoning in the history of this country."" In July, after testing the blood of BP cleanup workers and residents in Louisiana, Mississippi, Alabama, and Florida for volatile organic compounds, environmental scientist Wilma Subra said she was ""finding amounts 5 to 10 times in excess of the 95th percentile"" she said that ""the presence of these chemicals in the blood indicates exposure."" Riki Ott, a marine toxicologist with experience of the Exxon Valdez oil spill, advised families to evacuate the Gulf. She said that workers from the Valdez spill had suffered long-term health consequences.Following the 26 May 2010 hospitalization of seven fishermen that were working in the cleanup crew, BP requested that the National Institute for Occupational Safety and Health perform a Health Hazard Evaluation. This was to cover all offshore cleanup activities, BP later requested a second NIOSH investigation of onshore cleanup operations. Tests for chemical exposure in the seven fishermen were negative NIOSH concluded that the hospitalizations were most likely a result of heat, fatigue, and terpenes that were being used to clean the decks. Review of 10 later hospitalizations found that heat exposure and dehydration were consistent findings but could not establish chemical exposure. NIOSH personnel performed air monitoring around cleanup workers at sea, on land, and during the application of Corexit. Air concentrations of volatile organic compounds and PAHs never exceeded permissible exposure levels. A limitation of their methodology was that some VOCs may have already evaporated from the oil before they began their investigation. In their report, they suggest the possibility that respiratory symptoms might have been caused by high levels of ozone or reactive aldehydes in the air, possibly produced from photochemical reactions in the oil. NIOSH did note that many of the personnel involved were not donning personal protective equipment (gloves and impermeable coveralls) as they had been instructed to and emphasized that this was important protection against transdermal absorption of chemicals from the oil. Heat stress was found to be the most pressing safety concern.Workers reported that they were not allowed to use respirators, and that their jobs were threatened if they did. OSHA said ""cleanup workers are receiving ""minimal"" exposure to airborne toxins...OSHA will require that BP provide certain protective clothing, but not respirators."" ProPublica reported that workers were being photographed while working with no protective clothing. An independent investigation for Newsweek showed that BP did not hand out the legally required safety manual for use with Corexit, and were not provided with safety training or protective gear.A 2012 survey of the health effects of the spill on cleanup workers reported ""eye, nose and throat irritation respiratory problems blood in urine, vomit and rectal bleeding seizures nausea and violent vomiting episodes that last for hours skin irritation, burning and lesions short-term memory loss and confusion liver and kidney damage central nervous system effects and nervous system damage hypertension and miscarriages"". Dr. James Diaz, writing for the American Journal of Disaster Medicine, said these ailments appearing in the Gulf reflected those reported after previous oil spills, like the Exxon Valdez. Diaz warned that ""chronic adverse health effects, including cancers, liver and kidney disease, mental health disorders, birth defects and developmental disorders should be anticipated among sensitive populations and those most heavily exposed"". Diaz also believes neurological disorders should be expected.Two years after the spill, a study initiated by the National Institute for Occupational Safety and Health found biomarkers matching the oil from the spill in the bodies of cleanup workers. Other studies have reported a variety of mental health issues, skin problems, breathing issues, coughing, and headaches. In 2013, during the three-day ""Gulf of Mexico Oil Spill & Ecosystem Science Conference"", findings discussed included a '""significant percentage"" of Gulf residents reporting mental health problems like anxiety, depression and PTSD. These studies also showed that the bodies of former spill cleanup workers carry biomarkers of ""many chemicals contained in the oil"".A study that investigated the health effects among children in Louisiana and Florida living less than 10 miles from the coast found that more than a third of the parents reported physical or mental health symptoms among their children. The parents reported ""unexplained symptoms among their children, including bleeding ears, nose bleeds, and the early start of menstruation among girls,"" according to David Abramson, director of Columbia University's National Center for Disaster Preparedness.A cohort study of almost 2200 Louisiana women found ""high physical/environmental exposure was significantly associated with all 13 of the physical health symptoms surveyed, with the strongest associations for burning in nose, throat or lungs  sore throat dizziness and wheezing. Women who suffered a high degree of economic disruption as a result of spill were significantly more likely to report wheezing headaches watery, burning, itchy eyes and stuffy, itchy, runny nose.The spill had a strong economic impact to BP  and also the Gulf Coast's economy sectors such as offshore drilling, fishing and tourism. Estimates of lost tourism dollars were projected to cost the Gulf coastal economy up to 22.7 billion through 2013. In addition, Louisiana reported that lost visitor spending through the end of 2010 totaled $32 million, and losses through 2013 were expected to total $153 million in this state alone. The Gulf of Mexico commercial fishing industry was estimated to have lost $247 million as a result of postspill fisheries closures. One study projects that the overall impact of lost or degraded commercial, recreational, and mariculture fisheries in the Gulf could be $8.7 billion by 2020, with a potential loss of 22,000 jobs over the same time frame. BP's expenditures on the spill included the cost of the spill response, containment, relief well drilling, grants to the Gulf states, claims paid, and federal costs, including fines and penalties. As of March 2012, BP estimated the company's total spill-related expenses do not exceed $37.2 billion. However, by some estimations penalties that BP may be required to pay have reached as high as $90 billion. In addition, in November 2012 the EPA announced that BP will be temporarily banned from seeking new contracts with the US government. Due to the loss of the market value, BP had dropped from the second to the fourth largest of the four major oil companies by 2013. During the crisis, BP gas stations in the United States reported a sales drop of between 10 and 40% due to backlash against the company.Local officials in Louisiana expressed concern that the offshore drilling moratorium imposed in response to the spill would further harm the economies of coastal communities as the oil industry directly or indirectly employs about 318,000 Louisiana residents (17% of all jobs in the state). NOAA had closed 86,985 square miles (225,290 km2), or approximately 36% of Federal waters in the Gulf of Mexico, for commercial fishing causing $2.5 billion cost for the fishing industry. The U.S. Travel Association estimated that the economic impact of the oil spill on tourism across the Gulf Coast over a three-year period could exceed approximately $23 billion, in a region that supports over 400,000 travel industry jobs generating $34 billion in revenue annually.On 30 April 2010 President Barack Obama ordered the federal government to hold the issuing of new offshore drilling leases and authorized investigation of 29 oil rigs in the Gulf in an effort to determine the cause of the disaster. Later a six-month offshore drilling (below 500 feet (150 m) of water) moratorium was enforced by the United States Department of the Interior. The moratorium suspended work on 33 rigs, and a group of affected companies formed the Back to Work Coalition. On 22 June, a United States federal judge on the United States District Court for the Eastern District of Louisiana Martin Leach-Cross Feldman when ruling in the case Hornbeck Offshore Services LLC v. Salazar, lifted the moratorium finding it too broad, arbitrary and not adequately justified. The ban was lifted in October 2010.On 28 April 2010, the National Energy Board of Canada, which regulates offshore drilling in the Canadian Arctic and along the British Columbia Coast, issued a letter to oil companies asking them to explain their argument against safety rules which require same-season relief wells. On 3 May California Governor Arnold Schwarzenegger withdrew his support for a proposed plan to allow expanded offshore drilling projects in California. On 8 July, Florida Governor Charlie Crist called for a special session of the state legislature to draft an amendment to the state constitution banning offshore drilling in state waters, which the legislature rejected on 20 July.In October 2011, the United States Department of the Interior's Minerals Management Service was dissolved after it was determined it had exercised poor oversight over the drilling industry.  Three new agencies replaced it, separating the regulation, leasing, and revenue collection responsibilities respectively, among the Bureau of Safety and Environmental Enforcement, the Bureau of Ocean Energy Management, and Office of Natural Resources Revenue.In March 2014, BP was again allowed to bid for oil and gas leases.On 30 April President Obama dispatched the Secretaries of the Department of Interior and Homeland Security, as well as the EPA Administrator and NOAA to the Gulf Coast to assess the disaster. In his 15 June speech, Obama said, ""This oil spill is the worst environmental disaster America has ever faced... Make no mistake: we will fight this spill with everything we've got for as long as it takes. We will make BP pay for the damage their company has caused. And we will do whatever's necessary to help the Gulf Coast and its people recover from this tragedy.""  Interior Secretary Ken Salazar stated, ""Our job basically is to keep the boot on the neck of British Petroleum."" Some observers suggested that the Obama administration was being overly aggressive in its criticisms, which some BP investors saw as an attempt to deflect criticism of his own handling of the crisis. Rand Paul accused President Obama of being anti-business and ""un-American"".Public opinion polls in the U.S. were generally critical of the way President Obama and the federal government handled the disaster and they were extremely critical of BPs response. Across the US, thousands participated in dozens of protests at BP gas stations and other locations, reducing sales at some stations by 10% to 40%.Industry claimed that disasters are infrequent and that this spill was an isolated incident and rejected claims of a loss of industry credibility. The American Petroleum Institute (API) stated that the offshore drilling industry is important to job creation and economic growth. CEOs from the top five oil companies all agreed to work harder at improving safety.  API announced the creation of an offshore safety institute, separate from API's lobbying operation.The Organization for International Investment, a Washington D.C.-based advocate for overseas investment in the United States, warned that the heated rhetoric was potentially damaging the reputation of British companies with operations in the United States and could spark a wave of U.S. protectionism that would restrict British firms from government contracts, political donations and lobbying.In the UK, there was anger at the American press and news outlets for the misuse of the term ""British Petroleum"" for the company – a name which has not been used since British Petroleum merged with the American company Amoco in 1998 to form BP. It was said that the U.S. was 'dumping' the blame onto the British people and there were calls for British Prime Minister David Cameron to protect British interests in the United States.  British pension fund managers (who have large holdings of BP shares and rely upon its dividends) accepted that while BP had to pay compensation for the spill and the environmental damage, they argued that the cost to the company's market value from President Obama's criticism was far outweighing the direct clean-up costs.Initially BP downplayed the incident its CEO Tony Hayward called the amount of oil and dispersant ""relatively tiny"" in comparison with the ""very big ocean."" Later, he drew an outpouring of criticism when he said that the spill was a disruption to Gulf Coast residents and himself adding, ""You know, I'd like my life back."" BP's chief operating officer Doug Suttles contradicted the underwater plume discussion noting, ""It may be down to how you define what a plume is here… The oil that has been found is in very minute quantities."" In June, BP launched a PR campaign and successfully bid for several search terms related to the spill on Google and other search engines so that the first sponsored search result linked directly to the company's website. On 26 July 2010, it was announced that CEO Tony Hayward was to resign and would be replaced by Bob Dudley, who is an American citizen and previously worked for Amoco.Hayward's involvement in Deepwater Horizon has left him a highly controversial public figure. In May 2013, he was honored as a ""distinguished leader"" by the University of Birmingham, but his award ceremony was stopped on multiple occasions by jeers and walk-outs and the focus of a protest from People & Planet members.In July 2013, Hayward was awarded an honorary degree from Robert Gordon University. This was described as ""a very serious error of judgement"" by Friends of the Earth Scotland, and ""a sick joke"" by the university's Student President.The U.S. government rejected offers of cleanup help from Canada, Croatia, France, Germany, Ireland, Mexico, the Netherlands, Norway, Romania, South Korea, Spain, Sweden, the United Kingdom, and the United Nations. The U.S. State Department listed 70 assistance offers from 23 countries, all being initially declined, but later, 8 had been accepted. The USCG actively requested skimming boats and equipment from several countries.In the United States the Deepwater Horizon investigation included several investigations and commissions, including reports by the USCG National Incident Commander, Admiral Thad Allen, the National Commission on the BP Deepwater Horizon Oil Spill and Offshore Drilling, Bureau of Ocean Energy Management, Regulation and Enforcement (BOEMRE), National Academy of Engineering, National Research Council, Government Accountability Office, National Oil Spill Commission, and Chemical Safety and Hazard Investigation Board. The Republic of the Marshall Islands Maritime Administrator conducted a separate investigation on the marine casualty. BP conducted its internal investigation.An investigation of the possible causes of the explosion was launched on 22 April 2010 by the USCG and the Minerals Management Service. On 11 May the United States administration requested the National Academy of Engineering conduct an independent technical investigation. The National Commission on the BP Deepwater Horizon Oil Spill and Offshore Drilling was established on 22 May to ""consider the root causes of the disaster and offer options on safety and environmental precautions."" The investigation by United States Attorney General Eric Holder was announced on 1 June 2010. Also the United States House Committee on Energy and Commerce conducted a number of hearings, including hearings of Tony Hayward and heads of Anadarko and Mitsui's exploration unit. According to the US Congressional investigation, the rig's blowout preventer, built by Cameron International Corporation, had a hydraulic leak and a failed battery, and therefore failed.On 8 September 2010, BP released a 193-page report on its web site. The report places some of the blame for the accident on BP but also on Halliburton and Transocean. The report found that on 20 April 2010, managers misread pressure data and gave their approval for rig workers to replace drilling fluid in the well with seawater, which was not heavy enough to prevent gas that had been leaking into the well from firing up the pipe to the rig, causing the explosion. The conclusion was that BP was partly to blame, as was Transocean, which owned the rig. Responding to the report, Transocean and Halliburton placed all blame on BP.On 9 November 2010, a report by the Oil Spill Commission said that there had been ""a rush to completion"" on the well and criticised poor management decisions. ""There was not a culture of safety on that rig,"" the co-chair said.The National Commission on the BP Deepwater Horizon Oil Spill and Offshore Drilling released a final report on 5 January 2011. The panel found that BP, Halliburton, and Transocean had attempted to work more cheaply and thus helped to trigger the explosion and ensuing leakage. The report stated that ""whether purposeful or not, many of the decisions that BP, Halliburton, and Transocean made that increased the risk of the Macondo blowout clearly saved those companies significant time (and money)."" BP released a statement in response to this, saying, that ""even prior to the conclusion of the commission's investigation, BP instituted significant changes designed to further strengthen safety and risk management."" Transocean, however, blamed BP for making the decisions before the actual explosion occurred and government officials for permitting those decisions. Halliburton stated that it was acting only upon the orders of BP when it injected the cement into the wall of the well. It criticized BP for its failure to run a cement bond log test. In the report, BP was accused of nine faults. One was that it had not used a diagnostic tool to test the strength of the cement. Another was ignoring a pressure test that had failed. Still another was for not plugging the pipe with cement. The study did not, however, place the blame on any one of these events. Rather, it concluded that ""notwithstanding these inherent risks, the accident of April 20 was avoidable"" and that ""it resulted from clear mistakes made in the first instance by BP, Halliburton and Transocean, and by government officials who, relying too much on industry's assertions of the safety of their operations, failed to create and apply a program of regulatory oversight that would have properly minimized the risk of deepwater drilling."" The panel also noted that the government regulators did not have sufficient knowledge or authority to notice these cost-cutting decisions.On 23 March 2011, BOEMRE (former MMS) and the USCG published a forensic examination report on the blowout preventer, prepared by Det Norske Veritas. The report concluded that the primary cause of failure was that the blind shear rams failed to fully close and seal due to a portion of drill pipe buckling between the shearing blocks.The US government report issued in September 2011 stated that BP is ultimately responsible for the spill, and that Halliburton and Transocean share some of the blame. The report states that the main cause was the defective cement job, and Halliburton, BP and Transocean were, in different ways, responsible for the accident. The report stated that, although the events leading to the sinking of Deepwater Horizon were set into motion by the failure to prevent a well blowout, the investigation revealed numerous systems deficiencies, and acts and omissions by Transocean and its Deepwater Horizon crew, that had an adverse impact on the ability to prevent or limit the magnitude of the disaster.  The report also states that a central cause of the blowout was failure of a cement barrier allowing hydrocarbons to flow up the wellbore, through the riser and onto the rig, resulting in the blowout.  The loss of life and the subsequent pollution of the Gulf of Mexico were the result of poor risk management, last‐minute changes to plans, failure to observe and respond to critical indicators, inadequate well control response, and insufficient emergency bridge response training by companies and individuals responsible for drilling at the Macondo well and for the operation of the drilling platform.On 16 June 2010, after BP executives met with President Obama, BP announced and established the Gulf Coast Claims Facility (GCCF), a $20 billion fund to settle claims arising from the Deepwater Horizon spill. This fund was set aside for natural resource damages, state and local response costs, and individual compensation, but could not be used for fines or penalties. Prior to establishing the GCCF, emergency compensation was paid by BP from an initial facility.The GCCF was administrated by attorney Kenneth Feinberg. The facility began accepting claims on 23 August 2010. On 8 March 2012, after BP and a team of plaintiffs' attorneys agreed to a class-action settlement, a court-supervised administrator Patrick Juneau took over administration. Until this more than one million claims of 220,000 individual and business claimants were processed and more than $6.2 billion was paid out from the fund. 97% of payments were made to claimants in the Gulf States. In June 2012, the settlement of claims through the GCCF was replaced by the court supervised settlement program.  During this transition period additional $404 million in claims were paid.The GCCF and its administrator Feinberg had been criticized about the amount and speed of payments as well as a lack of transparency. An independent audit of the GCCF, announced by Attorney General Eric Holder, was approved by Senate on 21 October 2011. An auditor BDO Consulting found that 7,300 claimants were wrongly denied or underpaid. As a result, about $64 million of additional payments was made.The Mississippi Center for Justice provided pro bono assistance to 10,000 people to help them ""navigate the complex claims process."" In a New York Times opinion piece, Stephen Teague, staff attorney at the Mississippi Center for Justice, argued that BP had become ""increasingly brazen"" in ""stonewalling payments."" ""But tens of thousands of gulf residents still haven't been fully compensated for their losses, and many are struggling to make ends meet. Many low-wage workers in the fishing and service industries, for example, have been seeking compensation for lost wages and jobs for three years.""In July 2013 BP made a motion in court to freeze payments on tens of thousands of claims, arguing inter alia that a staff attorney from the Deepwater Horizon Court-Supervised Settlement Program, the program responsible for evaluating compensation claims, had improperly profited from claims filed by a New Orleans law firm. The attorney is said to have received portions of settlement claims for clients he referred to the firm. The federal judge assigned to the case, Judge Barbier, refused to halt the settlement program, saying he had not seen evidence of widespread fraud, adding that he was ""offended by what he saw as attempts to smear the lawyer administering the claims.""By 26 May 2010, over 130 lawsuits relating to the spill had been filed against one or more of BP, Transocean, Cameron International Corporation, and Halliburton Energy Services, although it was considered likely by observers that these would be combined into one court as a multidistrict litigation. On 21 April 2011, BP issued $40 billion worth of lawsuits against rig owner Transocean, cementer Halliburton and blowout preventer manufacturer Cameron. The oil firm alleged failed safety systems and irresponsible behaviour of contractors had led to the explosion, including claims that Halliburton failed to properly use modelling software to analyze safe drilling conditions. The firms deny the allegations.On 2 March 2012, BP and plaintiffs agreed to settle their lawsuits. The deal would settle roughly 100,000 claims filled by individuals and businesses affected by the spill. On 13 August, BP asked US District Judge Carl Barbier to approve the settlement, saying its actions ""did not constitute gross negligence or willful misconduct"". On 13 January 2013, Judge Barbier approved a medical-benefits portion of BP's proposed $7.8 billion partial settlement. People living for at least 60 days along oil-impacted shores or involved in the clean-up who can document one or more specific health conditions caused by the oil or dispersants are eligible for benefits, as are those injured during clean-up. BP also agreed to spend $105 million over five years to set up a Gulf Coast health outreach program and pay for medical examinations. According to a group presenting the plaintiffs, the deal has no specific cap. BP says that it has $9.5 billion in assets set aside in a trust to pay the claims, and the settlement will not increase the $37.2 billion the company budgeted for spill-related expenses. BP originally expected to spend $7.8 billion. By October 2013 it had increased its projection to $9.2 billion, saying it could be ""significantly higher.""On 31 August 2012, the US Department of Justice (DOJ) filed papers in federal court in New Orleans blaming BP for the Gulf oil spill, describing the spill as an example of ""gross negligence and willful misconduct."" In their statement the DOJ said that some of BP's arguments were ""plainly misleading"" and that the court should ignore BP'sargument that the Gulf region is ""undergoing a robust recovery"".  BP rejected the charges saying ""BP believes it was not grossly negligent and looks forward to presenting evidence on this issue at trial in January."" The DOJ also said Transocean, the owner and operator of the Deepwater Horizon rig, was guilty of gross negligence as well.On 14 November 2012, BP and the US Department of Justice reached a settlement. BP will pay $4.5 billion in fines and other payments, the largest of its kind in US history. In addition, the U.S. government temporarily banned BP from new federal contracts over its ""lack of business integrity"".  The plea was accepted by Judge Sarah Vance of the United States District Court for the Eastern District of Louisiana on 31 January 2013. The settlement includes payments of $2.394 billion to the National Fish and Wildlife Foundation, $1.15 billion to the Oil Spill Liability Trust Fund, $350 million to the National Academy of Sciences for oil spill prevention and response research, $100 million to the North America Wetland Conservation Fund, $6 million to General Treasury and $525 million to the Securities and Exchange Commission. Oil sector analysts at London-based investment bank Canaccord Genuity noted that a settlement along the lines disclosed would only be a partial resolution of the many claims against BP.On 3 January 2013 the US Justice Department announced ""Transocean Deepwater Inc. has agreed to plead guilty to violating the Clean Water Act and to pay a total of $1.4 billion in civil and criminal fines and penalties"".  $800 million goes to Gulf Coast restoration Trust Fund, $300 million to the Oil Spill Liability Trust Fund, $150 million to the National Wild Turkey Federation and $150 million to the National Academy of Sciences. MOEX Offshore 2007 agreed to pay $45 million to the Oil Spill Liability Trust Fund, $25 million to five Gulf state and $20 million to supplemental environmental projects.On 25 July 2013 Halliburton pleaded guilty to destruction of critical evidence after the oil spill and said it would pay the maximum allowable fine of $200,000 and will be subject to three years of probation.On 9 July 2013 Alaska inventor and oil field veteran Chris McIntyre filed suit against BP, alleging that the company used his design to cap the Macondo Well without compensation. McIntyre sent BP the design for the capping device on 14 May 2010. BP subsequently used McIntyre's design (or one very similar) to shut in the well on 15 July 2010. BP maintains that its employees first conceived of the design some days before McIntyre. Both parties agree that the device did not exist prior to 20 April 2010. The case, Christopher McIntyre v. BP Exploration & Production is currently on appeal with the United States Court for the Ninth Circuit in San Francisco. McIntyre seeks remand to the District Court of Alaska for a jury trial.In January 2014, a panel of the U.S. Fifth Circuit Court of Appeals rejected an effort by BP to curb payment of what it described as ""fictitious"" and ""absurd"" claims to a settlement fund for businesses and persons affected by the oil spill. BP said administration of the 2012 settlement was marred by the fact that people without actual damages could file a claim. The court ruled that BP hadn't explained ""how this court or the district court should identify or even discern the existence of 'claimants that have suffered no cognizable injury.'"" The Court then went further, calling BP's position ""nonsensical."" The Supreme Court of the United States later refused to hear BP's appeal after victims and claimants, along with numerous Gulf coast area chambers of commerce, objected to the oil major's efforts to renege on the Settlement Agreement.In September 2014, Halliburton agreed to settle a large percentage of legal claims against it by paying $1.1 billion into a trust by way of three installments over two years.BP and its partners in the oil well, Transocean and Halliburton, went on trial on 25 February 2013 in the United States District Court for the Eastern District of Louisiana in New Orleans to determine payouts and fines under the Clean Water Act and the Natural Resources Damage Assessment. The plaintiffs included the U.S. Justice Department, Gulf states and private individuals. Tens of billions of dollars in liability and fines were at stake. A finding of gross negligence would result in a four-fold increase in the fines BP would have to pay for violating the federal Clean Water Act, and leave the company liable for punitive damages for private claims.The trial's first phase was to determine the liability of BP, Transocean, Halliburton, and other companies, and if they acted with gross negligence and willful misconduct. The second phase scheduled in September 2013 focused on the flow rate of the oil and the third phase scheduled in 2014 was to consider damages. According to the plaintiffs' lawyers the major cause of an explosion was the mishandling of a rig safety test, while inadequate training of the staff, poor maintenance of the equipment and substandard cement were also mentioned as things leading to the disaster. According to The Wall Street Journal the U.S. government and Gulf Coast states had prepared an offer to BP for a $16 billion settlement. However, it was not clear if this deal had been officially proposed to BP and if BP has accepted it.On 4 September 2014, U.S. District Judge Carl Barbier ruled BP was guilty of gross negligence and willful misconduct. He described BP's actions as ""reckless."" He said Transocean's and Halliburton's actions were ""negligent."" He apportioned 67% of the blame for the spill to BP, 30% to Transocean, and 3% to Halliburton. Fines would be apportioned commensurate with the degree of negligence of the parties, measured against the number of barrels of oil spilled. Under the Clean Water Act fines can be based on a cost per barrel of up to $4,300, at the discretion of the judge. The number of barrels was in dispute at the conclusion of the trial with BP arguing 2.5 million barrels were spilled over the 87 days the spill lasted, while the court contends 4.2 million barrels were spilled. BP issued a statement strongly disagreeing with the finding, and saying the court's decision would be appealed.Barbier ruled that BP had acted with “conscious disregard of known risks"" and rejected BP’s assertion that other parties were equally responsible for the oil spill. His ruling stated that BP ""employees took risks that led to the largest environmental disaster in U.S. history,” that the company was “reckless,” and determined that several crucial BP decisions were “primarily driven by a desire to save time and money, rather than ensuring that the well was secure.” The ruling means that BP, which had already spent more than $28 billion on cleanup costs and damage claims, may be liable for another $18 billion in damages, four times the Clean Water Act maximum penalties and many times more than the $3.5 billion BP had already allotted. BP strongly disagreed with the ruling and filed an immediate appeal. The size of the ruling ""casts a cloud over BP’s future,"" The New York Times reported.On 2 July 2015, BP, the U.S. Justice Department and five gulf states announced that the company agreed to pay a record settlement of $18.7 billion. To date BP’s cost for the clean-up, environmental and economic damages and penalties has reached $54 billion.In addition to the private lawsuits and civil governmental actions, the federal government charged multiple companies and five individuals with federal crimes.In the November 2012 resolution of the federal charges against it, BP agreed to plead guilty to 11 felony counts related to the deaths of the 11 workers and paid a $4 billion fine. Transocean's plead guilty to a misdemeanor charge as part of its $1.4 billion fine.In April 2012, the Justice Department filed the first criminal charge against Kurt Mix, a BP engineer, for obstructing justice by deleting messages showing that BP knew the flow rate was three times higher than initial claims by the company, and knew that ""Top Kill"" was unlikely to succeed, but claimed otherwise. Three more BP employees were charged in November 2012. Site managers Donald Vidrine and Robert Kaluza were charged with manslaughter for acting negligently in their supervision of key safety tests performed on the rig prior to the explosion, and failure to alert onshore engineers of problems in the drilling operation. David Rainey, BP's former vice-president for exploration in the Gulf of Mexico, was charged with obstructing Congress by misrepresenting the rate that oil was flowing out of the well. Lastly, Anthony Badalamenti, a Halliburton manager, was charged with instructing two employees to delete data related to Halliburton's cementing job on the oil well.None of the charges against individuals resulted in any prison time, and no charges were levied against upper level executives. Anthony Badalementi was sentenced to one year probation, Donald Vidrine paid a $50,000 fine and received 10 months probation, Kurt Mix received 6 months probation, and David Rainey and Robert Kaluza were acquitted.Deepwater Horizon is a 2016 film based on the explosion, directed by Peter Berg and starring Mark Wahlberg. The 2015 film The Runner, directed by Austin Stark and starring Nicolas Cage, is a fictional story of a politician and his family set in the aftermath of the Deepwater Horizon disaster.""We Just Decided To"", the pilot of the HBO series The Newsroom, featured its characters covering the Deepwater Horizon story.Timeline of the Deepwater Horizon oil spillList of industrial disastersList of oil spillsOffshore oil and gas in the US Gulf of MexicoKhatchadourian, Raffi (11 March 2011). ""A Reporter at Large: The Gulf war"". The New Yorker. 87 (04): 36–59. Retrieved 15 December 2013. Liu, Yonggang MacFadyen, Amy Ji, Zhen-Gang Weisberg, Robert H. (2011). Monitoring and Modeling the Deepwater Horizon Oil Spill: A Record-Breaking Enterprise. Geophysical Monograph Series. 195. Bibcode:2011GMS...195.....L. doi:10.1029/GM195. Marghany, Maged (15 December 2014). ""Utilization of a genetic algorithm for the automatic detection of oil spill from RADARSAT-2 SAR satellite data"". Marine Pollution Bulletin. 89 (1–2): 20–29. doi:10.1016/j.marpolbul.2014.10.041. Deepwater BP Oil Spill at the Wayback Machine (archive index) – at Whitehouse.govDeepwater Horizon Incident, Gulf of Mexico from the National Oceanic and Atmospheric Administration (NOAA)RestoreTheGulf.gov official U.S. Government Web site, taking over content and functions from Deepwater Horizon Response siteSmithsonian's Ocean PortalScience in a Time of Crisis: WHOI's response to the Deepwater Horizon Oil Spill a multimedia presentation from Woods Hole Oceanographic Institution""Approaches for Ecosystem Services Valuation for the Gulf of Mexico After the Deepwater Horizon Oil Spill: Interim Report by the National Academy of Sciences""Erik Stokstad (8 February 2013). ""BP Research Dollars Yield Signs of Cautious Hope"". Sciencemag.org. Retrieved 25 February 2013. CDC – Oil Spill Response Resources – NIOSH Workplace Safety and Health TopicDaniel Kaniewski James Carafano (9 August 2010). ""Critical Lessons from the Federal Response to the Gulf Oil Spill"". The Heritage Foundation. Retrieved 31 July 2015. The Role of BP in the Deepwater Horizon Explosion and Oil Spill: Hearing before the Subcommittee on Oversight and Investigations of the Committee on Energy and Commerce, House of Representatives, One Hundred Eleventh Congress, Second Session, June 17, 2010Florida Department of Environmental Protection (DEP)Mississippi DEQState of Florida Oil Spill Academic Task ForceFull coverage from The New York TimesFull coverage from The Times-Picayune (New Orleans)ScientificAmerican.com 2015-04020 BP Gulf Oil Spill: 5 Years Later Indepth ReportDeepwater Horizon oil spill caused lasting damage, report saysBP Oil Spill, NPRGulf Oil Spill Tracker interactive map and form for citizen reporting (SkyTruth.org)Map and Estimates of the Oil Spilled (New York Times)Where Oil Has Made Landfall (New York Times)Rig fire at Deepwater Horizon 4/21/10, video at CNN iReportGOES-13 satellite images on the CIMSS Satellite BlogUnderwater Video Examines Multiple Leak Points Causing BP Oil SpillThe Big Fix. Documentary about the oil spillGulf of Mexico Oil Spill Interactive: Smithsonian Ocean PortalBBC News – interactive animation to the disaster and blocking effortsNew York Times exploded view diagrams on the methods used to stop the oil spillGraphic: Where the oil and gas went";environmental disaster;Deepwater Horizon oil spill;0
"Deforestation, clearance, or clearing is the removal of a forest or stand of trees where the land is thereafter converted to a non-forest use. Examples of deforestation include conversion of forestland to farms, ranches, or urban use. The most concentrated deforestation occurs in tropical rainforests. About 30 percent of Earth's land surface is covered by forests.Deforestation occurs for multiple reasons: trees are cut down to be used for building or sold as fuel (sometimes in the form of charcoal or timber), while cleared land is used as pasture for livestock and plantation. The removal of trees without sufficient reforestation has resulted in habitat damage, biodiversity loss, and aridity. It has adverse impacts on biosequestration of atmospheric carbon dioxide. Deforestation has also been used in war to deprive the enemy of vital resources and cover for its forces. Modern examples of this were the use of Agent Orange by the British military in Malaya during the Malayan Emergency and the United States military in Vietnam during the Vietnam War. As of 2005, net deforestation rates have ceased to increase in countries with a per capita GDP of at least US$4,600. Deforested regions typically incur significant adverse soil erosion and frequently degrade into wasteland.Disregard of ascribed value, lax forest management, and deficient environmental laws are some of the factors that allow deforestation to occur on a large scale. In many countries, deforestation–both naturally occurring and human-induced–is an ongoing issue. Deforestation causes extinction, changes to climatic conditions, desertification, and displacement of populations as observed by current conditions and in the past through the fossil record. More than half of all plant and land animal species in the world live in tropical forests.Between 2000 and 2012, 2.3 million square kilometres (890,000 sq mi) of forests around the world were cut down. As a result of deforestation, only 6.2 million square kilometres (2.4 million square miles) remain of the original 16 million square kilometres (6 million square miles) of forest that formerly covered the Earth. An area the size of a football pitch is cleared from the Amazon rainforest every minute, with 136 million acres (55 million hectares) of rainforest cleared for animal agriculture overall.According to the United Nations Framework Convention on Climate Change (UNFCCC) secretariat, the overwhelming direct cause of deforestation is agriculture. Subsistence farming is responsible for 48% of deforestation commercial agriculture is responsible for 32% logging is responsible for 14%, and fuel wood removals make up 5%.Experts do not agree on whether industrial logging is an important contributor to global deforestation. Some argue that poor people are more likely to clear forest because they have no alternatives, others that the poor lack the ability to pay for the materials and labour needed to clear forest. One study found that population increases due to high fertility rates were a primary driver of tropical deforestation in only 8% of cases.Other causes of contemporary deforestation may include corruption of government institutions, the inequitable distribution of wealth and power, population growth and overpopulation, and urbanization. Globalization is often viewed as another root cause of deforestation, though there are cases in which the impacts of globalization (new flows of labor, capital, commodities, and ideas) have promoted localized forest recovery.In 2000 the United Nations Food and Agriculture Organization (FAO) found that ""the role of population dynamics in a local setting may vary from decisive to negligible"", and that deforestation can result from ""a combination of population pressure and stagnating economic, social and technological conditions"".The degradation of forest ecosystems has also been traced to economic incentives that make forest conversion appear more profitable than forest conservation. Many important forest functions have no markets, and hence, no economic value that is readily apparent to the forests' owners or the communities that rely on forests for their well-being. From the perspective of the developing world, the benefits of forest as carbon sinks or biodiversity reserves go primarily to richer developed nations and there is insufficient compensation for these services. Developing countries feel that some countries in the developed world, such as the United States of America, cut down their forests centuries ago and benefited economically from this deforestation, and that it is hypocritical to deny developing countries the same opportunities, i.e. that the poor shouldn't have to bear the cost of preservation when the rich created the problem.Some commentators have noted a shift in the drivers of deforestation over the past 30 years. Whereas deforestation was primarily driven by subsistence activities and government-sponsored development projects like transmigration in countries like Indonesia and colonization in Latin America, India, Java, and so on, during the late 19th century and the earlier half of the 20th century, by the 1990s the majority of deforestation was caused by industrial factors, including extractive industries, large-scale cattle ranching, and extensive agriculture.Deforestation is ongoing and is shaping climate and geography.Deforestation is a contributor to global warming, and is often cited as one of the major causes of the enhanced greenhouse effect. Tropical deforestation is responsible for approximately 20% of world greenhouse gas emissions. According to the Intergovernmental Panel on Climate Change deforestation, mainly in tropical areas, could account for up to one-third of total anthropogenic carbon dioxide emissions. But recent calculations suggest that carbon dioxide emissions from deforestation and forest degradation (excluding peatland emissions) contribute about 12% of total anthropogenic carbon dioxide emissions with a range from 6 to 17%. Deforestation causes carbon dioxide to linger in the atmosphere. As carbon dioxide accrues, it produces a layer in the atmosphere that traps radiation from the sun. The radiation converts to heat which causes global warming, which is better known as the greenhouse effect. Plants remove carbon in the form of carbon dioxide from the atmosphere during the process of photosynthesis, but release some carbon dioxide back into the atmosphere during normal respiration. Only when actively growing can a tree or forest remove carbon, by storing it in plant tissues. Both the decay and burning of wood releases much of this stored carbon back to the atmosphere. Although an accumulation of wood is generally necessary for carbon sequestration, in some forests the network of symbiotic fungi that surround the trees' roots can store a significant amount of carbon, storing it underground even if the tree which supplied it dies and decays, or is harvested and burned.  Another way carbon can be sequestered by forests is for the wood to be harvested and turned into long-lived products, with new young trees replacing them. Deforestation may also cause carbon stores held in soil to be released. Forests can be either sinks or sources depending upon environmental circumstances. Mature forests alternate between being net sinks and net sources of carbon dioxide (see carbon dioxide sink and carbon cycle).In deforested areas, the land heats up faster and reaches a higher temperature, leading to localized upward motions that enhance the formation of clouds and ultimately produce more rainfall. However, according to the Geophysical Fluid Dynamics Laboratory, the models used to investigate remote responses to tropical deforestation showed a broad but mild temperature increase all through the tropical atmosphere. The model predicted <0.2 °C warming for upper air at 700 mb and 500 mb. However, the model shows no significant changes in other areas besides the Tropics. Though the model showed no significant changes to the climate in areas other than the Tropics, this may not be the case since the model has possible errors and the results are never absolutely definite.  Deforestation affects wind flows,water vapour flows and absorption of solar energy thus clearly influencing local and global climate.Reducing emissions from deforestation and forest degradation (REDD) in developing countries has emerged as a new potential to complement ongoing climate policies. The idea consists in providing financial compensations for the reduction of greenhouse gas (GHG) emissions from deforestation and forest degradation"".Rainforests are widely believed by laymen to contribute a significant amount of the world's oxygen, although it is now accepted by scientists that rainforests contribute little net oxygen to the atmosphere and deforestation has only a minor effect on atmospheric oxygen levels. However, the incineration and burning of forest plants to clear land releases large amounts of CO2, which contributes to global warming. Scientists also state that tropical deforestation releases 1.5 billion tons of carbon each year into the atmosphere.The water cycle is also affected by deforestation. Trees extract groundwater through their roots and release it into the atmosphere. When part of a forest is removed, the trees no longer transpire this water, resulting in a much drier climate. Deforestation reduces the content of water in the soil and groundwater as well as atmospheric moisture. The dry soil leads to lower water intake for the trees to extract. Deforestation reduces soil cohesion, so that erosion, flooding and landslides ensue.Shrinking forest cover lessens the landscape's capacity to intercept, retain and transpire precipitation. Instead of trapping precipitation, which then percolates to groundwater systems, deforested areas become sources of surface water runoff, which moves much faster than subsurface flows. Forests return most of the water that falls as precipitation to the atmosphere by transpiration. In contrast, when an area is deforested, almost all precipitation is lost as run-off. That quicker transport of surface water can translate into flash flooding and more localized floods than would occur with the forest cover. Deforestation also contributes to decreased evapotranspiration, which lessens atmospheric moisture which in some cases affects precipitation levels downwind from the deforested area, as water is not recycled to downwind forests, but is lost in runoff and returns directly to the oceans. According to one study, in deforested north and northwest China, the average annual precipitation decreased by one third between the 1950s and the 1980s.Trees, and plants in general, affect the water cycle significantly:their canopies intercept a proportion of precipitation, which is then evaporated back to the atmosphere (canopy interception)their litter, stems and trunks slow down surface runofftheir roots create macropores – large conduits – in the soil that increase infiltration of waterthey contribute to terrestrial evaporation and reduce soil moisture via transpirationtheir litter and other organic residue change soil properties that affect the capacity of soil to store water.their leaves control the humidity of the atmosphere by transpiring. 99% of the water absorbed by the roots moves up to the leaves and is transpired.As a result, the presence or absence of trees can change the quantity of water on the surface, in the soil or groundwater, or in the atmosphere. This in turn changes erosion rates and the availability of water for either ecosystem functions or human services. Deforestation on lowland plains moves cloud formation and rainfall to higher elevations.The forest may have little impact on flooding in the case of large rainfall events, which overwhelm the storage capacity of forest soil if the soils are at or close to saturation.Tropical rainforests produce about 30% of our planet's fresh water.Deforestation disrupts normal weather patterns creating hotter and drier weather thus increasing drought, desertification, crop failures, melting of the polar ice caps, coastal flooding and displacement of major vegetation regimes.Due to surface plant litter, forests that are undisturbed have a minimal rate of erosion. The rate of erosion occurs from deforestation, because it decreases the amount of litter cover, which provides protection from surface runoff. The rate of erosion is around 2 metric tons per square kilometre. This can be an advantage in excessively leached tropical rain forest soils. Forestry operations themselves also increase erosion through the development of (forest) roads and the use of mechanized equipment.Deforestation in China's Loess Plateau many years ago has led to soil erosion this erosion has led to valleys opening up. The increase of soil in the runoff causes the Yellow River to flood and makes it yellow colored.Greater erosion is not always a consequence of deforestation, as observed in the southwestern regions of the US. In these areas, the loss of grass due to the presence of trees and other shrubbery leads to more erosion than when trees are removed.Soils are reinforced by the presence of trees, which secure the soil by binding their roots to soil bedrock. Due to deforestation, the removal of trees causes sloped lands to be more susceptible to landslides.Deforestation on a human scale results in decline in biodiversity, and on a natural global scale is known to cause the extinction of many species. The removal or destruction of areas of forest cover has resulted in a degraded environment with reduced biodiversity. Forests support biodiversity, providing habitat for wildlife moreover, forests foster medicinal conservation. With forest biotopes being irreplaceable source of new drugs (such as taxol), deforestation can destroy genetic variations (such as crop resistance) irretrievably.Since the tropical rainforests are the most diverse ecosystems on Earth and about 80% of the world's known biodiversity could be found in tropical rainforests, removal or destruction of significant areas of forest cover has resulted in a degraded environment with reduced biodiversity. A study in Rondônia, Brazil, has shown that deforestation also removes the microbial community which is involved in the recycling of nutrients, the production of clean water and the removal of pollutants.It has been estimated that we are losing 137 plant, animal and insect species every single day due to rainforest deforestation, which equates to 50,000 species a year. Others state that tropical rainforest deforestation is contributing to the ongoing Holocene mass extinction. The known extinction rates from deforestation rates are very low, approximately 1 species per year from mammals and birds which extrapolates to approximately 23,000 species per year for all species. Predictions have been made that more than 40% of the animal and plant species in Southeast Asia could be wiped out in the 21st century. Such predictions were called into question by 1995 data that show that within regions of Southeast Asia much of the original forest has been converted to monospecific plantations, but that potentially endangered species are few and tree flora remains widespread and stable.Scientific understanding of the process of extinction is insufficient to accurately make predictions about the impact of deforestation on biodiversity. Most predictions of forestry related biodiversity loss are based on species-area models, with an underlying assumption that as the forest declines species diversity will decline similarly. However, many such models have been proven to be wrong and loss of habitat does not necessarily lead to large scale loss of species. Species-area models are known to overpredict the number of species known to be threatened in areas where actual deforestation is ongoing, and greatly overpredict the number of threatened species that are widespread.A recent study of the Brazilian Amazon predicts that despite a lack of extinctions thus far,  up to 90 percent of predicted extinctions will finally occur in the next 40 years.Damage to forests and other aspects of nature could halve living standards for the world's poor and reduce global GDP by about 7% by 2050, a report concluded at the Convention on Biological Diversity (CBD) meeting in Bonn in 2008. Historically, utilization of forest products, including timber and fuel wood, has played a key role in human societies, comparable to the roles of water and cultivable land. Today, developed countries continue to utilize timber for building houses, and wood pulp for paper. In developing countries almost three billion people rely on wood for heating and cooking.The forest products industry is a large part of the economy in both developed and developing countries. Short-term economic gains made by conversion of forest to agriculture, or over-exploitation of wood products, typically leads to loss of long-term income and long-term biological productivity. West Africa, Madagascar, Southeast Asia and many other regions have experienced lower revenue because of declining timber harvests. Illegal logging causes billions of dollars of losses to national economies annually.The new procedures to get amounts of wood are causing more harm to the economy and overpower the amount of money spent by people employed in logging. According to a study, ""in most areas studied, the various ventures that prompted deforestation rarely generated more than US$5 for every ton of carbon they released and frequently returned far less than US$1"". The price on the European market for an offset tied to a one-ton reduction in carbon is 23 euro (about US$35).Rapidly growing economies also have an effect on deforestation. Most pressure will come from the world's developing countries, which have the fastest-growing populations and most rapid economic (industrial) growth. In 1995, economic growth in developing countries reached nearly 6%, compared with the 2% growth rate for developed countries.” As our human population grows, new homes, communities, and expansions of cities will occur. Connecting all of the new expansions will be roads, a very important part in our daily life. Rural roads promote economic development but also facilitate deforestation. About 90% of the deforestation has occurred within 100 km of roads in most parts of the Amazon.The European Union is one of the largest importer of products made from illegal deforestation.The forest area change may follow a pattern suggested by the forest transition (FT) theory, whereby at early stages in its development a country is characterized by high forest cover and low deforestation rates (HFLD countries).Then deforestation rates accelerate (HFHD, high forest cover – high deforestation rate), and forest cover is reduced (LFHD, low forest cover – high deforestation rate), before the deforestation rate slows (LFLD, low forest cover – low deforestation rate), after which forest cover stabilizes and eventually starts recovering. FT is not a ""law of nature"", and the pattern is influenced by national context (for example, human population density, stage of development, structure of the economy), global economic forces, and government policies. A country may reach very low levels of forest cover before it stabilizes, or it might through good policies be able to “bridge” the forest transition.FT depicts a broad trend, and an extrapolation of historical rates therefore tends to underestimate future BAU deforestation for counties at the early stages in the transition (HFLD), while it tends to overestimate BAU deforestation for countries at the later stages (LFHD and LFLD).Countries with high forest cover can be expected to be at early stages of the FT. GDP per capita captures the stage in a country’s economic development, which is linked to the pattern of natural resource use, including forests. The choice of forest cover and GDP per capita also fits well with the two key scenarios in the FT:(i) a forest scarcity path, where forest scarcity triggers forces (for example, higher prices of forest products) that lead to forest cover stabilization and(ii) an economic development path, where new and better off-farm employment opportunities associated with economic growth (= increasing GDP per capita) reduce profitability of frontier agriculture and slows deforestation.The Carboniferous Rainforest Collapse was an event that occurred 300 million years ago. Climate change devastated tropical rainforests causing the extinction of many plant and animal species. The change was abrupt, specifically, at this time climate became cooler and drier, conditions that are not favourable to the growth of rainforests and much of the biodiversity within them. Rainforests were fragmented forming shrinking 'islands' further and further apart. Populations such as the sub class Lissamphibia were devastated, whereas Reptilia survived the collapse. The surviving organisms were better adapted to the drier environment left behind and served as legacies in succession after the collapse.Rainforests once covered 14% of the earth's land surface now they cover a mere 6% and experts estimate that the last remaining rainforests could be consumed in less than 40 years.Small scale deforestation was practiced by some societies for tens of thousands of years before the beginnings of civilization. The first evidence of deforestation appears in the Mesolithic period. It was probably used to convert closed forests into more open ecosystems favourable to game animals. With the advent of agriculture, larger areas began to be deforested, and fire became the prime tool to clear land for crops. In Europe there is little solid evidence before 7000 BC. Mesolithic foragers used fire to create openings for red deer and wild boar. In Great Britain, shade-tolerant species such as oak and ash are replaced in the pollen record by hazels, brambles, grasses and nettles. Removal of the forests led to decreased transpiration, resulting in the formation of upland peat bogs. Widespread decrease in elm pollen across Europe between 8400–8300 BC and 7200–7000 BC, starting in southern Europe and gradually moving north to Great Britain, may represent land clearing by fire at the onset of Neolithic agriculture.The Neolithic period saw extensive deforestation for farming land. Stone axes were being made from about 3000 BC not just from flint, but from a wide variety of hard rocks from across Britain and North America as well. They include the noted Langdale axe industry in the English Lake District, quarries developed at Penmaenmawr in North Wales and numerous other locations. Rough-outs were made locally near the quarries, and some were polished locally to give a fine finish. This step not only increased the mechanical strength of the axe, but also made penetration of wood easier. Flint was still used from sources such as Grimes Graves but from many other mines across Europe.Evidence of deforestation has been found in Minoan Crete for example the environs of the Palace of Knossos were severely deforested in the Bronze Age.Throughout prehistory, humans were hunter gatherers who hunted within forests. In most areas, such as the Amazon, the tropics, Central America, and the Caribbean, only after shortages of wood and other forest products occur are policies implemented to ensure forest resources are used in a sustainable manner.Three regional studies of historic erosion and alluviation in ancient Greece found that, wherever adequate evidence exists, a major phase of erosion follows the introduction of farming in the various regions of Greece by about 500-1,000 years, ranging from the later Neolithic to the Early Bronze Age. The thousand years following the mid-first millennium BC saw serious, intermittent pulses of soil erosion in numerous places. The historic silting of ports along the southern coasts of Asia Minor (e.g. Clarus, and the examples of Ephesus, Priene and Miletus, where harbors had to be abandoned because of the silt deposited by the Meander) and in coastal Syria during the last centuries BC.Easter Island has suffered from heavy soil erosion in recent centuries, aggravated by agriculture and deforestation. Jared Diamond gives an extensive look into the collapse of the ancient Easter Islanders in his book Collapse. The disappearance of the island's trees seems to coincide with a decline of its civilization around the 17th and 18th century. He attributed the collapse to deforestation and over-exploitation of all resources.The famous silting up of the harbor for Bruges, which moved port commerce to Antwerp, also followed a period of increased settlement growth (and apparently of deforestation) in the upper river basins. In early medieval Riez in upper Provence, alluvial silt from two small rivers raised the riverbeds and widened the floodplain, which slowly buried the Roman settlement in alluvium and gradually moved new construction to higher ground concurrently the headwater valleys above Riez were being opened to pasturage.A typical progress trap was that cities were often built in a forested area, which would provide wood for some industry (for example, construction, shipbuilding, pottery). When deforestation occurs without proper replanting, however local wood supplies become difficult to obtain near enough to remain competitive, leading to the city's abandonment, as happened repeatedly in Ancient Asia Minor. Because of fuel needs, mining and metallurgy often led to deforestation and city abandonment.With most of the population remaining active in (or indirectly dependent on) the agricultural sector, the main pressure in most areas remained land clearing for crop and cattle farming. Enough wild green was usually left standing (and partially used, for example, to collect firewood, timber and fruits, or to graze pigs) for wildlife to remain viable. The elite's (nobility and higher clergy) protection of their own hunting privileges and game often protected significant woodland.Major parts in the spread (and thus more durable growth) of the population were played by monastical 'pioneering' (especially by the Benedictine and Commercial orders) and some feudal lords' recruiting farmers to settle (and become tax payers) by offering relatively good legal and fiscal conditions. Even when speculators sought to encourage towns, settlers needed an agricultural belt around or sometimes within defensive walls. When populations were quickly decreased by causes such as the Black Death or devastating warfare (for example, Genghis Khan's Mongol hordes in eastern and central Europe, Thirty Years' War in Germany), this could lead to settlements being abandoned. The land was reclaimed by nature, but the secondary forests usually lacked the original biodiversity.From 1100 to 1500 AD, significant deforestation took place in Western Europe as a result of the expanding human population. The large-scale building of wooden sailing ships by European (coastal) naval owners since the 15th century for exploration, colonisation, slave trade–and other trade on the high seas consumed many forest resources. Piracy also contributed to the over harvesting of forests, as in Spain. This led to a weakening of the domestic economy after Columbus' discovery of America, as the economy became dependent on colonial activities (plundering, mining, cattle, plantations, trade, etc.)In Changes In the Land (1983), William Cronon analyzed and documented 17th-century English colonists' reports of increased seasonal flooding in New England during the period when new settlers initially cleared the forests for agriculture. They believed flooding was linked to widespread forest clearing upstream.The massive use of charcoal on an industrial scale in Early Modern Europe was a new type of consumption of western forests even in Stuart England, the relatively primitive production of charcoal has already reached an impressive level. Stuart England was so widely deforested that it depended on the Baltic trade for ship timbers, and looked to the untapped forests of New England to supply the need. Each of Nelson's Royal Navy war ships at Trafalgar (1805) required 6,000 mature oaks for its construction. In France, Colbert planted oak forests to supply the French navy in the future. When the oak plantations matured in the mid-19th century, the masts were no longer required because shipping had changed.Norman F. Cantor's summary of the effects of late medieval deforestation applies equally well to Early Modern Europe:Europeans had lived in the midst of vast forests throughout the earlier medieval centuries. After 1250 they became so skilled at deforestation that by 1500 they were running short of wood for heating and cooking. They were faced with a nutritional decline because of the elimination of the generous supply of wild game that had inhabited the now-disappearing forests, which throughout medieval times had provided the staple of their carnivorous high-protein diet. By 1500 Europe was on the edge of a fuel and nutritional disaster [from] which it was saved in the sixteenth century only by the burning of soft coal and the cultivation of potatoes and maize.In the 19th century, introduction of steamboats in the United States was the cause of deforestation of banks of major rivers, such as the Mississippi River, with increased and more severe flooding one of the environmental results. The steamboat crews cut wood every day from the riverbanks to fuel the steam engines. Between St. Louis and the confluence with the Ohio River to the south, the Mississippi became more wide and shallow, and changed its channel laterally. Attempts to improve navigation by the use of snag pullers often resulted in crews' clearing large trees 100 to 200 feet (61 m) back from the banks. Several French colonial towns of the Illinois Country, such as Kaskaskia, Cahokia and St. Philippe, Illinois, were flooded and abandoned in the late 19th century, with a loss to the cultural record of their archeology.The wholescale clearance of woodland to create agricultural land can be seen in many parts of the world, such as the Central forest-grasslands transition and other areas of the Great Plains of the United States. Specific parallels are seen in the 20th-century deforestation occurring in many developing nations.Global deforestation sharply accelerated around 1852. It has been estimated that about half of the Earth's mature tropical forests—between 7.5 million and 8 million km2 (2.9 million to 3 million sq mi) of the original 15 million to 16 million km2 (5.8 million to 6.2 million sq mi) that until 1947 covered the planet—have now been destroyed. Some scientists have predicted that unless significant measures (such as seeking out and protecting old growth forests that have not been disturbed) are taken on a worldwide basis, by 2030 there will only be 10% remaining, with another 10% in a degraded condition. 80% will have been lost, and with them hundreds of thousands of irreplaceable species.  Some cartographers have attempted to illustrate the sheer scale of deforestation by country using a cartogram.Estimates vary widely as to the extent of tropical deforestation. Over a 50-year period, percentage of land cover by tropical rainforests has decreased by 50%. Where total land coverage by tropical rainforests decreased from 14% to 6%. A large contribution to this loss can be identified between 1960 and 1990, when 20% of all tropical rainforests were destroyed. At this rate, extinction of such forests is projected to occur by the mid 21st century.A 2002 analysis of satellite imagery suggested that the rate of deforestation in the humid tropics (approximately 5.8 million hectares per year) was roughly 23% lower than the most commonly quoted rates. Conversely, a newer analysis of satellite images reveals that deforestation of the Amazon rainforest is twice as fast as scientists previously estimated.Some have argued that deforestation trends may follow a Kuznets curve, which if true would nonetheless fail to eliminate the risk of irreversible loss of non-economic forest values (for example, the extinction of species).A 2005 report by the United Nations Food and Agriculture Organization (FAO) estimated that although the Earth's total forest area continued to decrease at about 13 million hectares per year, the global rate of deforestation has recently been slowing. The 2016 report by the FAO reports from 2010 to 2015 there was a worldwide decrease in forest area of 3.3 million ha per year. During this five-year period, the biggest forest area loss occurred in the tropics, particularly in South America and Africa. Per capita forest area decline was also greatest in the tropics and subtropics but is occurring in every climatic domain (except in the temperate) as populations increase.Others claim that rainforests are being destroyed at an ever-quickening pace. The London-based Rainforest Foundation notes that ""the UN figure is based on a definition of forest as being an area with as little as 10% actual tree cover, which would therefore include areas that are actually savannah-like ecosystems and badly damaged forests."" Other critics of the FAO data point out that they do not distinguish between forest types, and that they are based largely on reporting from forestry departments of individual countries, which do not take into account unofficial activities like illegal logging.Despite these uncertainties, there is agreement that destruction of rainforests remains a significant environmental problem. Up to 90% of West Africa's coastal rainforests have disappeared since 1900.In South Asia, about 88% of the rainforests have been lost. Much of what remains of the world's rainforests is in the Amazon basin, where the Amazon Rainforest covers approximately 4 million square kilometres. The regions with the highest tropical deforestation rate between 2000 and 2005 were Central America—which lost 1.3% of its forests each year—and tropical Asia. In Central America, two-thirds of lowland tropical forests have been turned into pasture since 1950 and 40% of all the rainforests have been lost in the last 40 years. Brazil has lost 90–95% of its Mata Atlântica forest.Paraguay was losing its natural semi humid forests in the country’s western regions at a rate of 15.000 hectares at a randomly studied 2-month period in 2010, Paraguay’s parliament refused in 2009 to pass a law that would have stopped cutting of natural forests altogether.Madagascar has lost 90% of its eastern rainforests. As of 2007, less than 50% of Haiti's forests remained. Mexico, India, the Philippines, Indonesia, Thailand, Burma, Malaysia, Bangladesh, China, Sri Lanka, Laos, Nigeria, the Democratic Republic of the Congo, Liberia, Guinea, Ghana and the Ivory Coast, have lost large areas of their rainforest. Several countries, notably Brazil, have declared their deforestation a national emergency. The World Wildlife Fund's ecoregion project catalogues habitat types throughout the world, including habitat loss such as deforestation, showing for example that even in the rich forests of parts of Canada such as the Mid-Continental Canadian forests of the prairie provinces half of the forest cover has been lost or altered.Rates of deforestation vary around the world.In 2011 Conservation International listed the top 10 most endangered forests, characterized by having all lost 90% or more of their original habitat, and each harboring at least 1500 endemic plant species (species found nowhere else in the world).Table source:Main international organizations including the United Nations and the World Bank, have begun to develop programs aimed at curbing deforestation. The blanket term Reducing Emissions from Deforestation and Forest Degradation (REDD) describes these sorts of programs, which use direct monetary or other incentives to encourage developing countries to limit and/or roll back deforestation. Funding has been an issue, but at the UN Framework Convention on Climate Change (UNFCCC) Conference of the Parties-15 (COP-15) in Copenhagen in December 2009, an accord was reached with a collective commitment by developed countries for new and additional resources, including forestry and investments through international institutions, that will approach USD 30 billion for the period 2010–2012. Significant work is underway on tools for use in monitoring developing country adherence to their agreed REDD targets. These tools, which rely on remote forest monitoring using satellite imagery and other data sources, include the Center for Global Development's FORMA (Forest Monitoring for Action) initiative and the Group on Earth Observations' Forest Carbon Tracking Portal. Methodological guidance for forest monitoring was also emphasized at COP-15. The environmental organization Avoided Deforestation Partners leads the campaign for development of REDD through funding from the U.S. government. In 2014, the Food and Agriculture Organization of the United Nations and partners launched Open Foris – a set of open-source software tools that assist countries in gathering, producing and disseminating information on the state of forest resources. The tools support the inventory lifecycle, from needs assessment, design, planning, field data collection and management, estimation analysis, and dissemination. Remote sensing image processing tools are included, as well as tools for international reporting for Reducing emissions from deforestation and forest degradation (REDD) and MRV (Measurement, Reporting and Verification) and FAO's Global Forest Resource Assessments.In evaluating implications of overall emissions reductions, countries of greatest concern are those categorized as High Forest Cover with High Rates of Deforestation (HFHD) and Low Forest Cover with High Rates of Deforestation (LFHD). Afghanistan, Benin, Botswana, Burma, Burundi, Cameroon, Chad, Ecuador, El Salvador, Ethiopia, Ghana, Guatemala, Guinea, Haiti, Honduras, Indonesia, Liberia, Malawi, Mali, Mauritania, Mongolia, Namibia, Nepal, Nicaragua, Niger, Nigeria, Pakistan, Paraguay, Philippines, Senegal, Sierra Leone, Sri Lanka, Sudan, Togo, Uganda, United Republic of Tanzania, Zimbabwe are listed as having Low Forest Cover with High Rates of Deforestation (LFHD). Brazil, Cambodia, Democratic Peoples Republic of Korea, Equatorial Guinea, Malaysia, Solomon Islands, Timor-Leste, Venezuela, Zambia are listed as High Forest Cover with High Rates of Deforestation (HFHD).In Bolivia, deforestation in upper river basins has caused environmental problems, including soil erosion and declining water quality. An innovative project to try and remedy this situation involves landholders in upstream areas being paid by downstream water users to conserve forests. The landholders receive US$20 to conserve the trees, avoid polluting livestock practices, and enhance the biodiversity and forest carbon on their land. They also receive US$30, which purchases a beehive, to compensate for conservation for two hectares of water-sustaining forest for five years. Honey revenue per hectare of forest is US$5 per year, so within five years, the landholder has sold US$50 of honey. The project is being conducted by Fundación Natura Bolivia and Rare Conservation, with support from the Climate & Development Knowledge Network.Transferring rights over land from public domain to its indigenous inhabitants is argued to be a cost effective strategy to conserve forests. This includes the protection of such rights entitled in existing laws, such as India’s Forest Rights Act. The transferring of such rights in China, perhaps the largest land reform in modern times, has been argued to have increased forest cover. In Brazil, forested areas given tenure to indigenous groups have even lower rates of clearing than national parks.New methods are being developed to farm more intensively, such as high-yield hybrid crops, greenhouse, autonomous building gardens, and hydroponics. These methods are often dependent on chemical inputs to maintain necessary yields. In cyclic agriculture, cattle are grazed on farm land that is resting and rejuvenating. Cyclic agriculture actually increases the fertility of the soil. Intensive farming can also decrease soil nutrients by consuming at an accelerated rate the trace minerals needed for crop growth. The most promising approach, however, is the concept of food forests in permaculture, which consists of agroforestal systems carefully designed to mimic natural forests, with an emphasis on plant and animal species of interest for food, timber and other uses. These systems have low dependence on fossil fuels and agro-chemicals, are highly self-maintaining, highly productive, and with strong positive impact on soil and water quality, and biodiversity.There are multiple methods that are appropriate and reliable for reducing and monitoring deforestation. One method is the “visual interpretation of aerial photos or satellite imagery that is labor-intensive but does not require high-level training in computer image processing or extensive computational resources”. Another method includes hot-spot analysis (that is, locations of rapid change) using expert opinion or coarse resolution satellite data to identify locations for detailed digital analysis with high resolution satellite images. Deforestation is typically assessed by quantifying the amount of area deforested, measured at the present time.From an environmental point of view, quantifying the damage and its possible consequences is a more important task, while conservation efforts are more focused on forested land protection and development of land-use alternatives to avoid continued deforestation. Deforestation rate and total area deforested, have been widely used for monitoring deforestation in many regions, including the Brazilian Amazon deforestation monitoring by INPE. A global satellite view is available.Efforts to stop or slow deforestation have been attempted for many centuries because it has long been known that deforestation can cause environmental damage sufficient in some cases to cause societies to collapse. In Tonga, paramount rulers developed policies designed to prevent conflicts between short-term gains from converting forest to farmland and long-term problems forest loss would cause, while during the 17th and 18th centuries in Tokugawa, Japan, the shoguns developed a highly sophisticated system of long-term planning to stop and even reverse deforestation of the preceding centuries through substituting timber by other products and more efficient use of land that had been farmed for many centuries. In 16th-century Germany, landowners also developed silviculture to deal with the problem of deforestation. However, these policies tend to be limited to environments with good rainfall, no dry season and very young soils (through volcanism or glaciation). This is because on older and less fertile soils trees grow too slowly for silviculture to be economic, whilst in areas with a strong dry season there is always a risk of forest fires destroying a tree crop before it matures.In the areas where ""slash-and-burn"" is practiced, switching to ""slash-and-char"" would prevent the rapid deforestation and subsequent degradation of soils. The biochar thus created, given back to the soil, is not only a durable carbon sequestration method, but it also is an extremely beneficial amendment to the soil. Mixed with biomass it brings the creation of terra preta, one of the richest soils on the planet and the only one known to regenerate itself.Certification, as provided by global certification systems such as Programme for the Endorsement of Forest Certification and Forest Stewardship Council, contributes to tackling deforestation by creating market demand for timber from sustainably managed forests. According to the United Nations Food and Agriculture Organization (FAO), ""A major condition for the adoption of sustainable forest management is a demand for products that are produced sustainably and consumer willingness to pay for the higher costs entailed. Certification represents a shift from regulatory approaches to market incentives to promote sustainable forest management. By promoting the positive attributes of forest products from sustainably managed forests, certification focuses on the demand side of environmental conservation."" Rainforest Rescue argues that the standards of organizations like FSC are too closely connected to timber industry interests and therefore do not guarantee environmentally and socially responsible forest management. In reality, monitoring systems are inadequate and various cases of fraud have been documented worldwide.Some nations have taken steps to help increase the number of trees on Earth. In 1981, China created National Tree Planting Day Forest and forest coverage had now reached 16.55% of China's land mass, as against only 12% two decades ago.Using fuel from bamboo rather than wood results in cleaner burning, and since bamboo matures much faster than wood, deforestation is reduced as supply can be replenished faster.In many parts of the world, especially in East Asian countries, reforestation and afforestation are increasing the area of forested lands. The amount of woodland has increased in 22 of the world's 50 most forested nations. Asia as a whole gained 1 million hectares of forest between 2000 and 2005. Tropical forest in El Salvador expanded more than 20% between 1992 and 2001. Based on these trends, one study projects that global forest will increase by 10%—an area the size of India—by 2050.In the People's Republic of China, where large scale destruction of forests has occurred, the government has in the past required that every able-bodied citizen between the ages of 11 and 60 plant three to five trees per year or do the equivalent amount of work in other forest services. The government claims that at least 1 billion trees have been planted in China every year since 1982. This is no longer required today, but 12 March of every year in China is the Planting Holiday. Also, it has introduced the Green Wall of China project, which aims to halt the expansion of the Gobi desert through the planting of trees. However, due to the large percentage of trees dying off after planting (up to 75%), the project is not very successful. There has been a 47-million-hectare increase in forest area in China since the 1970s. The total number of trees amounted to be about 35 billion and 4.55% of China's land mass increased in forest coverage. The forest coverage was 12% two decades ago and now is 16.55%.An ambitious proposal for China is the Aerially Delivered Re-forestation and Erosion Control System and the proposed Sahara Forest Project coupled with the Seawater Greenhouse.In Western countries, increasing consumer demand for wood products that have been produced and harvested in a sustainable manner is causing forest landowners and forest industries to become increasingly accountable for their forest management and timber harvesting practices.The Arbor Day Foundation's Rain Forest Rescue program is a charity that helps to prevent deforestation. The charity uses donated money to buy up and preserve rainforest land before the lumber companies can buy it. The Arbor Day Foundation then protects the land from deforestation. This also locks in the way of life of the primitive tribes living on the forest land. Organizations such as Community Forestry International, Cool Earth, The Nature Conservancy, World Wide Fund for Nature, Conservation International, African Conservation Foundation and Greenpeace also focus on preserving forest habitats. Greenpeace in particular has also mapped out the forests that are still intact and published this information on the internet. World Resources Institute in turn has made a simpler thematic map showing the amount of forests present just before the age of man (8000 years ago) and the current (reduced) levels of forest. These maps mark the amount of afforestation required to repair the damage caused by people.In order to acquire the world’s demand for wood, it is suggested that high yielding forest plantations are suitable according to forest writers Botkins and Sedjo.  Plantations that yield 10 cubic meters per hectare a year would supply enough wood for trading of 5% of the world’s existing forestland. By contrast, natural forests produce about 1–2 cubic meters per hectare therefore, 5–10 times more forestland would be required to meet demand. Forester Chad Oliver has suggested a forest mosaic with high-yield forest lands interspersed with conservation land.Globally, planted forests increased from 4.1% to 7.0% of the total forest area between 1990 and 2015.   Plantation forests made up 280 million ha in 2015, an increase of about 40 million ha in the last ten years. Globally, planted forests consist of about 18% exotic or introduced species while the rest are species native to the country where they are planted. In South America, Oceania, and East and Southern Africa, planted forests are dominated by introduced species: 88%, 75% and 65%, respectively. In North America, West and Central Asia, and Europe the proportions of introduced species in plantations are much lower at 1%, 3% and 8% of the total area planted, respectively.In the country of Senegal, on the western coast of Africa, a movement headed by youths has helped to plant over 6 million mangrove trees. The trees will protect local villages from storm damages and will provide a habitat for local wildlife. The project started in 2008, and already the Senegalese government has been asked to establish rules and regulations that would protect the new mangrove forests.While demands for agricultural and urban use for the human population cause the preponderance of deforestation, military causes can also intrude. One example of deliberate deforestation played out in the U.S. zone of occupation in Germany after World War II ended in 1945. Before the onset of the Cold War, defeated Germany was still considered a potential future threat rather than a potential future ally. To address this threat, the victorious  Allies made attempts to lower German industrial potential, of which forests were deemed an element. Sources in the U.S. government admitted that the purpose of this was that the ""ultimate destruction of the war potential of German forests"". As a consequence of the practice of clear-felling, deforestation resulted which could ""be replaced only by long forestry development over perhaps a century"".Operations in war can also cause deforestation. For example, in the 1945 Battle of Okinawa, bombardment and other combat operations reduced a lush tropical landscape into ""a vast field of mud, lead, decay and maggots"".Deforestation can also result from the intentional  tactics of military forces. Clearing forest became an element in the Russian Empire's successful  conquest of the Caucasus in the mid-19th century.The British (during the Malayan Emergency) and the United States (in the Korean War and in the Vietnam War) used defoliants (like Agent Orange or others).Deforestation eliminates a great number of species of plants and animals which also often results in an increase in disease. Loss of native species allows new species to come to dominance. Often the destruction of predatory species can result in an increase in rodent populations which can carry plague. Additionally, erosion can produce pools of stagnant water that are perfect breeding grounds for mosquitos, well known vectors of malaria, yellow fever, nipah virus, and more. Deforestation can also create a path for non-native species to flourish such as certain types of snails, which have been correlated with an increase in schistosomiasis cases.Deforestation is occurring all over the world and has been coupled with an increase in the occurrence of disease outbreaks. In Malaysia, thousands of acres of forest have been cleared for pig farms. This has resulted in an increase in the zoonosis the Nipah virus. In Kenya, deforestation has led to an increase in malaria cases which is now the leading cause of morbidity and mortality the country. A 2017 study in the American Economic Review found that deforestation substantially increased the incidence of malaria in Nigeria.Another pathway through which deforestation affects disease is the relocation and dispersion of disease-carrying hosts. This disease emergence pathway can be called ""range expansion"", whereby the host’s range (and thereby the range of pathogens) expands to new geographic areas. Through deforestation, hosts and reservoir species are forced into neighboring habitats. Accompanying the reservoir species are pathogens that have the ability to find new hosts in previously unexposed regions. As these pathogens and species come into closer contact with humans, they are infected both directly and indirectly.A catastrophic example of range expansion is the 1998 outbreak of Nipah virus in Malaysia. For a number of years, deforestation, drought, and subsequent fires led to a dramatic geographic shift and density of fruit bats, a reservoir for Nipah virus. Deforestation reduced the available fruiting trees in the bats’ habitat, and they encroached on surrounding orchards which also happened to be the location of a large number of pigsties. The bats, through proximity spread the Nipah to pigs. While the virus infected the pigs, mortality was much lower than among humans, making the pigs a virulent host leading to the transmission of the virus to humans. This resulted in 265 reported cases of encephalitis, of which 105 resulted in death. This example provides an important lesson for the impact deforestation can have on human health.Another example of range expansion due to deforestation and other anthropogenic habitat impacts includes the Capybara rodent in Paraguay. This rodent is the host of a number of zoonotic diseases and, while there has not yet been a human-borne outbreak due to the movement of this rodent into new regions, it offers an example of how habitat destruction through deforestation and subsequent movements of species is occurring regularly.A now well-developed theory is that the spread of HIV it is at least partially due deforestation. Rising populations created a food demand and with deforestation opening up new areas of the forest the hunters harvested a great deal of primate bushmeat, which is believed to be the origin of HIV.NotesGeneral referencesEthiopia deforestation referencesGlobal map of deforestation based on Landsat dataJICA-JAXA Forest Early Warning System in the Tropics: JJ-FAST (FOREST GOVERNANCE INITIATIVE)- JICA-JAXAOld-growth forest zones within the remaining world forestsEIA forest reports: Investigations into illegal logging.EIA in the USA Reports and info.Cocaine destroys 4 m2 of rainforest per gram The Guardian""Avoided Deforestation"" Plan Gains Support – Worldwatch InstituteOneWorld Tropical Forests GuideSome Background Info to Deforestation and REDD+General info on deforestation effectsDeforestation and Climate ChangeIn the media14 March 2007, Independent Online: Destruction of forests in developing world 'out of control'Pappas, S. (14 November 2013). ""Vanishing Forests: New Map Details Global Deforestation"". LiveScience.com. TechMediaNetwork. Retrieved 16 November 2013. 31 August 2017, Independent Online: New Amazonian species discovered every two days while the rainforest is trashed by 'relentless deforestation'Films onlineWatch the National Film Board of Canada documentaries Battle for the Trees & Forest in CrisisVideo on Illegal Deforestation In Paraguay";environmental disaster;Deforestation;0
"Climate change has brought about possibly permanent alterations to Earth's geological, biological and ecological systems. These changes have led to the emergence of a not so large-scale environmental hazards to human health, such as extreme weather, ozone depletion, increased danger of wildland fires, loss of biodiversity, stresses to food-producing systems and the global spread of infectious diseases. The World Health Organization (WHO) estimates that 160,000 deaths, since 1950, are directly attributable to climate change.To date, a neglected aspect of the climate change debate, much less research has been conducted on the impacts of climate change on health, food supply, economic growth, migration, security, societal change, and public goods, such as drinking water, than on the geophysical changes related to global warming. Human impacts can be both negative and positive. Climatic changes in Siberia, for instance, are expected to improve food production and local economic activity, at least in the short to medium term. Numerous studies suggest, however, that the current and future impacts of climate change on human society are and will continue to be overwhelmingly negative.The majority of the adverse effects of climate change are experienced by poor and low-income communities around the world, who have much higher levels of vulnerability to environmental determinants of health, wealth and other factors, and much lower levels of capacity available for coping with environmental change. A report on the global human impact of climate change published by the Global Humanitarian Forum in 2009, estimated more than 300,000 deaths and about $125 billion in economic losses each year, and indicating that most climate change induced mortality is due to worsening floods and droughts in developing countries.Most of the key vulnerabilities to climate change are related to climate phenomena that exceed thresholds for adaptation such as extreme weather events or abrupt climate change, as well as limited access to resources (financial, technical, human, institutional) to cope. In 2007, the IPCC published a report of key vulnerabilities of industry, settlements, and society to climate change. This assessment included a level of confidence for each key vulnerability:Very high confidence: Interactions between climate change and urbanization: this is most notable in developing countries, where urbanization is often focused in vulnerable coastal areas.High confidence:Interactions between climate change and global economic growth: Stresses due to climate change are not only linked to the impacts of climate change, but also to the impacts of climate change policies. For example, these policies might affect development paths by requiring high cost fuel choices.Fixed physical infrastructures that are important in meeting human needs: These include infrastructures that are susceptible to damage from extreme weather events or sea level rise, and infrastructures that are already close to being inadequate.Medium confidence: Interactions with governmental and social cultural structures that already face other pressures, e.g., limited economic resources.Climate change poses a wide range of risks to population health – risks that will increase in future decades, often to critical levels, if global climate change continues on its current trajectory. The three main categories of health risks include: (i) direct-acting effects (e.g. due to heat waves, amplified air pollution, and physical weather disasters), (ii) impacts mediated via climate-related changes in ecological systems and relationships (e.g. crop yields, mosquito ecology, marine productivity), and (iii) the more diffuse (indirect) consequences relating to impoverishment, displacement, resource conflicts (e.g. water), and post-disaster mental health problems.Climate change thus threatens to slow, halt or reverse international progress towards reducing child under-nutrition, deaths from diarrheal diseases and the spread of other infectious diseases. Climate change acts predominantly by exacerbating the existing, often enormous, health problems, especially in the poorer parts of the world. Current variations in weather conditions already have many adverse impacts on the health of poor people in developing nations, and these too are likely to be 'multiplied' by the added stresses of climate change.A changing climate thus affects the prerequisites of population health: clean air and water, sufficient food, natural constraints on infectious disease agents, and the adequacy and security of shelter. A warmer and more variable climate leads to higher levels of some air pollutants. It increases the rates and ranges of transmission of infectious diseases through unclean water and contaminated food, and by affecting vector organisms (such as mosquitoes) and intermediate or reservoir host species that harbour the infectious agent (such as cattle, bats and rodents). Changes in temperature, rainfall and seasonality compromise agricultural production in many regions, including some of the least developed countries, thus jeopardising child health and growth and the overall health and functional capacity of adults. As warming proceeds, the severity (and perhaps frequency) of weather-related disasters will increase – and appears to have done so in a number of regions of the world over the past several decades. Therefore, in summary, global warming, together with resultant changes in food and water supplies, can indirectly cause increases in a range of adverse health outcomes, including malnutrition, diarrhea, injuries, cardiovascular and respiratory diseases, and water-borne and insect-transmitted diseases.Health equity and climate change have a major impact on human health and quality of life, and are interlinked in a number of ways. The report of the WHO Commission on Social Determinants of Health points out that disadvantaged communities are likely to shoulder a disproportionate share of the burden of climate change because of their increased exposure and vulnerability to health threats. Over 90 percent of malaria and diarrhea deaths are borne by children aged 5 years or younger, mostly in developing countries. Other severely affected population groups include women, the elderly and people living in small island developing states and other coastal regions, mega-cities or mountainous areas.A 2011 article in the American Psychologist identified three classes of psychological impacts from global climate change:Direct - ""Acute or traumatic effects of extreme weather events and a changed environment""Indirect - ""Threats to emotional well-being based on observation of impacts and concern or uncertainty about future risks""Psychosocial – ""Chronic social and community effects of heat, drought, migrations, and climate-related conﬂicts, and postdisaster adjustment""   A psychological impact is shown through peoples behaviours and how they act towards the actual situation. The topic of climate change is very complex and difficult for people to understand, which effects how they act upon it. It is shown by Ranney and Clark(2016) that by informing people to make them understand the topic of climate science clearly, it promotes the change in behaviour towards mitigation of climate change.This trend towards more variability and fluctuation is perhaps more important, in terms of its impact on human health, than that of a gradual and long-term trend towards higher average temperature. Infectious disease often accompanies extreme weather events, such as floods, earthquakes and drought. These local epidemics occur due to loss of infrastructure, such as hospitals and sanitation services, but also because of changes in local ecology and environment.Climate change may lead to dramatic increases in prevalence of a variety of infectious diseases. Beginning in the mid-'70s, there has been an “emergence, resurgence and redistribution of infectious diseases”. Reasons for this are likely multicausal, dependent on a variety of social, environmental and climatic factors, however, many argue that the “volatility of infectious disease may be one of the earliest biological expressions of climate instability”. Though many infectious diseases are affected by changes in climate, vector-borne diseases, such as malaria, dengue fever and leishmaniasis, present the strongest causal relationship. Observation and research detect a shift of pests and pathogens in the distribution away from the equator and towards Earth's poles.Increased precipitation like rain could increase the number of mosquitos indirectly by expanding larval habitat and food supply. Malaria kills approximately 300,000 children (under age 5) annually, poses an imminent threat through temperature increase . Models suggest, conservatively, that risk of malaria will increase 5-15% by 2100 due to climate change. In Africa alone, according to the MARA Project (Mapping Malaria Risk in Africa), there is a projected increase of 16–28% in person-month exposures to malaria by 2100.Sociodemographic factors include, but are not limited to: patterns of human migration and travel, effectiveness of public health and medical infrastructure in controlling and treating disease, the extent of anti-malarial drug resistance  and the underlying health status  of the population at hand. Environmental factors include: changes in land-use (e.g. deforestation), expansion of agricultural and water development projects (which tend to increase mosquito breeding habitat), and the overall trend towards urbanization (i.e. increased concentration of human hosts). Patz and Olson argue that these changes in landscape can alter local weather more than long term climate change. For example, the deforestation and cultivation of natural swamps in the African highlands has created conditions favourable for the survival of mosquito larvae, and has, in part, led to the increasing incidence of malaria. The effects of these non-climatic factors complicate things and make a direct causal relationship between climate change and malaria difficult to confirm. It is highly unlikely that climate exerts an isolated effect.Climate change may dramatically impact habitat loss, for example, arid conditions may cause the deforestation of rainforests, as has occurred in the past.A sustained wet-bulb temperature exceeding 35° is a threshold at which the resilience of human systems is no longer able to adequately cool the skin. A study by NOAA from 2013 concluded that heat stress will reduce labor capacity considerably under current emissions scenarios. There is evidence to show that high temperatures can increase mortality rates among fetuses and children Although the main focus is often on the health impacts and risks of higher temperatures, it should be remembered that they also reduce learning and worker productivity, which can impact a country's economy and development.The freshwater resources that humans rely on are highly sensitive to variations in weather and climate.  In 2007, the IPCC reported with high confidence that climate change has a net negative impact on water resources and freshwater ecosystems in all regions.  The IPCC also found with very high confidence that arid and semi-arid areas are particularly exposed to freshwater impacts.As the climate warms, it changes the nature of global rainfall, evaporation, snow, stream flow and other factors that affect water supply and quality. Specific impacts include:Warmer water temperatures affect water quality and accelerate water pollution.Sea level rise is projected to increase salt-water intrusion into groundwater in some regions.  This reduces the amount of freshwater available for drinking and farming.In some areas, shrinking glaciers and snow deposits threaten the water supply. Areas that depend on melted water runoff will likely see that runoff depleted, with less flow in the late summer and spring peaks occurring earlier. This can affect the ability to irrigate crops. (This situation is particularly acute for irrigation in South America, for irrigation and drinking supplies in Central Asia, and for hydropower in Norway, the Alps, and the Pacific Northwest of North America.)Increased extreme weather means more water falls on hardened ground unable to absorb it, leading to flash floods instead of a replenishment of soil moisture or groundwater levels.Increased evaporation will reduce the effectiveness of reservoirs.At the same time, human demand for water will grow for the purposes of cooling and hydration.Climate change causes displacement of people in several ways, the most obvious—and dramatic—being through the increased number and severity of weather-related disasters which destroy homes and habitats causing people to seek shelter or livelihoods elsewhere. Effects of climate change such as desertification and rising sea levels gradually erode livelihood and force communities to abandon traditional homelands for more accommodating environments. This is currently happening in areas of Africa’s Sahel, the semi-arid belt that spans the continent just below its northern deserts. Deteriorating environments triggered by climate change can also lead to increased conflict over resources which in turn can displace people.The IPCC has estimated that 150 million environmental migrants will exist by the year 2050, due mainly to the effects of coastal flooding, shoreline erosion and agricultural disruption.  However, the IPCC also cautions that it is extremely difficult to measure the extent of environmental migration due to the complexity of the issue and a lack of data.According to the Internal Displacement Monitoring Centre, more than 42 million people were displaced in Asia and the Pacific during 2010 and 2011, more than twice the population of Sri Lanka. This figure includes those displaced by storms, floods, and heat and cold waves. Still others were displaced drought and sea-level rise. Most of those compelled to leave their homes eventually returned when conditions improved, but an undetermined number became migrants, usually within their country, but also across national borders.Asia and the Pacific is the global area most prone to natural disasters, both in terms of the absolute number of disasters and of populations affected. It is highly exposed to climate impacts, and is home to highly vulnerable population groups, who are disproportionately poor and marginalized. A recent Asian Development Bank report highlights “environmental hot spots” that are particular risk of flooding, cyclones, typhoons, and water stress.Some Pacific Ocean island nations, such as Tuvalu, Kiribati, and the Maldives, are considering the eventual possibility of evacuation, as flood defense may become economically unrealistic. Tuvalu already has an ad hoc agreement with New Zealand to allow phased relocation. However, for some islanders relocation is not an option. They are not willing to leave their homes, land and families. Some simply don’t know the threat that climate change has on their island and this is mainly down to the lack of awareness that climate change even exists. In Vutia on Viti Levu, Fiji’s main island, half the respondents to a survey had not heard of climate change (Lata and Nuun 2012). Even where there is awareness many believe that it is a problem caused by developed countries and should therefore be solved by developed countries.Governments have considered various approaches to reduce migration compelled by environmental conditions in at-risk communities, including programs of social protection, livelihoods development, basic urban infrastructure development, and disaster risk management.  Some experts even support migration as an appropriate way for people to cope with environmental changes.  However, this is controversial because migrants – particularly low-skilled ones – are among the most vulnerable people in society and are often denied basic protections and access to services.Climate change is only one factor that may contribute to a household's decision to migrate other factors may include poverty, population growth or employment options. For this reason, it is difficult to classify environmental migrants as actual ""refugees"" as legally defined by the UNHCR. Neither the UN Framework Convention on Climate Change nor its Kyoto Protocol, an international agreement on climate change, includes any provisions concerning specific assistance or protection for those who will be directly affected by climate change.In small islands and megadeltas, inundation as a result of sea level rise is expected to threaten vital infrastructure and human settlements. This could lead to issues of statelessness for populations in countries such as the Maldives and Tuvalu and homelessness in countries with low-lying areas such as Bangladesh.The World Bank predicts that a “severe hit” will spur conflict and migration across the Middle East, Central Asia, and Africa.Climate change has the potential to exacerbate existing tensions or create new ones — serving as a threat multiplier. It can be a catalyst for violent conflict and a threat to international security. A meta-analysis of over 50 quantitative studies that examine the link between climate and conflict found that ""for each 1 standard deviation (1σ) change in climate toward warmer temperatures or more extreme rainfall, median estimates indicate that the frequency of interpersonal violence rises 4% and the frequency of intergroup conflict rises 14%"". The IPCC has suggested that the disruption of environmental migration may serve to exacerbate conflicts, though they are less confident of the role of increased resource scarcity. Of course, climate change does not always lead to violence, and conflicts are often caused by multiple interconnected factors.A variety of experts have warned that climate change may lead to increased conflict.  The Military Advisory Board, a panel of retired U.S. generals and admirals, predicted that global warming will serve as a ""threat multiplier"" in already volatile regions.  The Center for Strategic and International Studies and the Center for a New American Security, two Washington think tanks, have reported that flooding ""has the potential to challenge regional and even national identities,"" leading to ""armed conflict over resources.""  They indicate that the greatest threat would come from ""large-scale migrations of people — both inside nations and across existing national borders.""  However, other researchers have been more skeptical: One study found no statistically meaningful relationship between climate and conflict using data from Europe between the years 1000 and 2000.The link between climate change and security is a concern for authorities across the world, including United Nations Security Council and the G77 group of developing nations.  Climate change's impact as a security threat is expected to hit developing nations particularly hard. In Britain, Foreign Secretary Margaret Beckett has argued that ""An unstable climate will exacerbate some of the core drivers of conflict, such as migratory pressures and competition for resources.""The links between the human impact of climate change and the threat of violence and armed conflict are particularly important because multiple destabilizing conditions are affected simultaneously.Experts have suggested links to climate change in several major conflicts:War in Darfur, where sustained drought encouraged conflict between herders and farmersSyrian Civil War, preceded by the displacement of 1.5 million people due to drought-induced crop and livestock failureIslamist insurgency in Nigeria, which exploited natural resource shortages to fuel anti-government sentimentSomali Civil War, in which droughts and extreme high temperatures have been linked to violenceAdditionally, researchers studying ancient climate patterns (paleoclimatology) have shown that long-term fluctuations of war frequency and population changes have followed cycles of temperature change since the preindustrial era. A 2016 study finds that ""drought can contribute to sustaining conflict, especially for agriculturally dependent groups and politically excluded groups in very poor countries. These results suggest a reciprocal nature–society interaction in which violent conflict and environmental shock constitute a vicious circle, each phenomenon increasing the group’s vulnerability to the other.""The consequences of climate change and poverty are not distributed uniformly within communities. Individual and social factors such as gender, age, education, ethnicity, geography and language lead to differential vulnerability and capacity to adapt to the effects of climate change. Climate change effects such as hunger, poverty and diseases like diarrhea and malaria, disproportionately impact children about 90 percent of malaria and diarrhea deaths are among young children. Children are also 14–44 percent more likely to die from environmental factors, again leaving them the most vulnerable.Those in urban areas will be affected by lower air quality and overcrowding, and will struggle the most to better their situation.As the World Meteorological Organization explains, ""recent increase in societal impact from tropical cyclones has largely been caused by rising concentrations of population and infrastructure in coastal regions."" Pielke et al. (2008) normalized mainland U.S. hurricane damage from 1900 to 2005 to 2005 values and found no remaining trend of increasing absolute damage. The 1970s and 1980s were notable because of the extremely low amounts of damage compared to other decades. The decade 1996–2005 has the second most damage among the past 11 decades, with only the decade 1926–1935 surpassing its costs. The most damaging single storm is the 1926 Miami hurricane, with $157 billion of normalized damage.The American Insurance Journal predicted that ""catastrophe losses should be expected to double roughly every 10 years because of increases in construction costs, increases in the number of structures and changes in their characteristics."" The Association of British Insurers has stated that limiting carbon emissions would avoid 80% of the projected additional annual cost of tropical cyclones by the 2080s. The cost is also increasing partly because of building in exposed areas such as coasts and floodplains. The ABI claims that reduction of the vulnerability to some inevitable effects of climate change, for example through more resilient buildings and improved flood defences, could also result in considerable cost-savings in the longterm.A major challenge for human settlements is sea-level rise, indicated by ongoing observation and research of rapid declines in ice-mass balance from both Greenland and Antarctica. Estimates for 2100 are at least twice as large as previously estimated by IPCC AR4, with an upper limit of about two meters. Depending on regional changes, increased precipitation patterns can cause more flooding or extended drought stresses water resources.For historical reasons to do with trade, many of the world's largest and most prosperous cities are on the coast. In developing countries, the poorest often live on floodplains, because it is the only available space, or fertile agricultural land. These settlements often lack infrastructure such as dykes and early warning systems. Poorer communities also tend to lack the insurance, savings, or access to credit needed to recover from disasters.In a journal paper, Nicholls and Tol (2006) considered the effects of sea level rise:The most vulnerable future worlds to sea-level rise appear to be the A2 and B2 [IPCC] scenarios, which primarily reflects differences in the socio-economic situation (coastal population, Gross Domestic Product (GDP) and GDP/capita), rather than the magnitude of sea-level rise. Small islands and deltaic settings stand out as being more vulnerable as shown in many earlier analyses. Collectively, these results suggest that human societies will have more choice in how they respond to sea-level rise than is often assumed. However, this conclusion needs to be tempered by recognition that we still do not understand these choices and significant impacts remain possible.The IPCC reported that socioeconomic impacts of climate change in coastal and low-lying areas would be overwhelmingly adverse. The following impacts were projected with very high confidence:Coastal and low-lying areas would be exposed to increasing risks including coastal erosion due to climate change and sea level rise.By the 2080s, millions of people would experience floods every year due to sea level rise. The numbers affected were projected to be largest in the densely populated and low-lying mega-deltas of Asia and Africa and smaller islands were judged to be especially vulnerable.A study in the April 2007 issue of Environment and Urbanization reports that 634 million people live in coastal areas within 30 feet (9.1 m) of sea level.  The study also reported that about two thirds of the world's cities with over five million people are located in these low-lying coastal areas.Oil and natural gas infrastructure is vulnerable to the effects of climate change and the increased risk of disasters such as storm, cyclones, flooding and long-term increases in sea level. Minimising these risks by building in less disaster prone areas, can be expensive and impossible in countries with coastal locations or island states. All thermal power stations depend on water to cool them.  Not only is there increased demand for fresh water, but climate change can increase the likelihood of drought and fresh water shortages. Another impact for thermal power plants, is that increasing the temperatures in which they operate reduces their efficiency and hence their output. The source of oil often comes from areas prone to high natural disaster risks such as tropical storms, hurricanes, cyclones, and floods. An example is Hurricane Katrina's impact on oil extraction in the Gulf of Mexico, as it destroyed 126 oil and gas platforms and damaged 183 more.However, previously pristine arctic areas will now be available for resource extractionClimate change, along with extreme weather and natural disasters can affect nuclear power plants in a similar way to those using oil, coal, and natural gas. However, the impact of water shortages on nuclear power plants cooled by rivers will be greater than on other thermal power plants.  This is because old reactor designs with water-cooled cores must run at lower internal temperatures and thus, paradoxically, must dump more heat to the environment to produce a given amount of electricity. This situation has forced some nuclear reactors to be shut down and will do so again unless the cooling systems of these plants are enhanced to provide more capacity.  Nuclear power supply was diminished by low river ﬂow rates and droughts, which meant rivers had reached the maximum temperatures for cooling. Such shutdowns happened in France during the 2003 and 2006 heat waves.  During the heat waves, 17 reactors had to limit output or shut down. 77% of French electricity is produced by nuclear power and in 2009 a similar situation created a 8GW shortage, and forced the French government to import electricity. Other Cases have been reported from Germany, where extreme temperatures have reduced nuclear power production 9 times due to high temperatures between 1979 and 2007. In particular: The Unterweser nuclear power plant reduced output by 90% between June and September 2003.The Isar nuclear power plant cut production by 60% for 14 days due to excess river temperatures and low stream ﬂow in the river Isar in 2006.Similar events have happened elsewhere in Europe during those same hot summers. Many scientists agree that if global warming continues, this disruption is likely to increase.Changes in the amount of river flow will correlate with the amount of energy produced by a dam. Lower river flows because of drought, climate change, or upstream dams and diversions, will reduce the amount of live storage in a reservoir therefore reducing the amount of water that can be used for hydroelectricity. The result of diminished river flow can be a power shortage in areas that depend heavily on hydroelectric power. The risk of flow shortage may increase as a result of climate change. Studies from the Colorado River in the United States suggests that modest climate changes (such as a 2 degree change in Celsius that could result in a 10% decline in precipitation), might reduce river run-off by up to 40%. Brazil in particular, is vulnerable due to its having reliance on hydroelectricity as increasing temperatures, lower water ﬂow, and alterations in the rainfall regime, could reduce total energy production by 7% annually by the end of the century.The scientific evidence for links between global warming and the increasing cost of natural disasters due to weather events is weak, but, nevertheless, prominent mainstream environmental spokesmen such as Barack Obama and Al Gore have emphasized the possible connection. For the most part increased costs due to events such as Hurricane Sandy are due to increased exposure to loss resulting from building insured facilities in vulnerable locations. This information has been denounced by Paul Krugman and ThinkProgress as climate change denial.An industry directly affected by the risks of climate change is the insurance industry. According to a 2005 report from the Association of British Insurers, limiting carbon emissions could avoid 80% of the projected additional annual cost of tropical cyclones by the 2080s. A June 2004 report by the Association of British Insurers declared ""Climate change is not a remote issue for future generations to deal with it is, in various forms here already, impacting on insurers' businesses now."" The report noted that weather-related risks for households and property were already increasing by 2–4% per year due to the changing weather conditions, and claims for storm and flood damages in the UK had doubled to over £6 billion over the period from 1998–2003 compared to the previous five years. The results are rising insurance premiums, and the risk that in some areas flood insurance will become unaffordable for those in the lower income brackets.Financial institutions, including the world's two largest insurance companies: Munich Re and Swiss Re, warned in a 2002 study that ""the increasing frequency of severe climatic events, coupled with social trends could cost almost 150 billion US$ each year in the next decade"". These costs would burden customers, taxpayers, and the insurance industry, with increased costs related to insurance and disaster relief.In the United States, insurance losses have also greatly increased. It has been shown that a 1% climb in annual precipitation can increase catastrophe loss by as much as 2.8%. Gross increases are mostly attributed to increased population and property values in vulnerable coastal areas though there was also an increase in frequency of weather-related events like heavy rainfalls since the 1950s.Roads, airport runways, railway lines and pipelines, (including oil pipelines, sewers, water mains etc.) may require increased maintenance and renewal as they become subject to greater temperature variation. Regions already adversely affected include areas of permafrost, which are subject to high levels of subsidence, resulting in buckling roads, sunken foundations, and severely cracked runways.Food securityLong-term effects of global warmingUniversity for PeaceWeather and climate effects on Lyme disease exposureAddressing Climate Change in Asia & the Pacific 2012Climate Change 2007: Synthesis Report, 4th Assessment Report, Intergovernmental Panel on Climate ChangeReport on the Economics of Climate Change (2006), Stern ReviewHuman Impact Report: The Anatomy of a Silent Crisis (2009), Global Humanitarian ForumHuman Development Report 2007/2008, United Nations Development ProgrammeWoodward, A. (1995). ""Doctoring the planet: health effects of global change*"". Australian and New Zealand Journal of Medicine. 25 (1): 46–53. doi:10.1111/j.1445-5994.1995.tb00579.x. ISSN 0004-8291. PMID 7786246. Climate change, water stress, conflict and migration Proceedings of a conference in The Hague, September 2011Warming world: impacts by degreeGlobal Humanitarian ForumTck Tck Tck Time for Climate Justice campaignUnited Nations Environment Programme and climate changeOffice of the High Commissioner for Human Rights human rights and climate changeOffice of the High Commissioner for Refugees climate changeCare International Climate Change Information CentreRed Cross/Red Crescent Climate CentreWorld Bank and Climate ChangeClimate change and human health on World Health OrganizationInternational Strategy for Disaster Reduction disaster risk reduction and climate change";environmental disaster;Effects of climate change on humans;0
"Emergency management or disaster management is the organization and management of the resources and responsibilities for dealing with all humanitarian aspects of emergencies (preparedness, response, and recovery). The aim is to reduce the harmful effects of all hazards, including disasters. It should not be equated to ""disaster management"".The World Health Organization defines an emergency as the state in which normal procedures are interrupted, and immediate measures need to be taken to prevent that state turning into a disaster. Thus, emergency management is crucial to avoid the disruption transforming into a disaster, which is even harder to recover from. Disaster management means helping the people to recover their conditions from the disaster occurred in that particular region. Emergency management is also known as disaster managementIf possible, emergency planning should aim to prevent emergencies from occurring, and failing that, should develop a good action plan to mitigate the results and effects of any emergencies. As time goes on, and more data become available, usually through the study of emergencies as they occur, a plan should evolve. The development of emergency plans is a cyclical process, common to many risk management disciplines, such as Business Continuity and Security Risk Management, as set out below:Recognition or identification of risksRanking or evaluation of risksResponding to significant risksTolerateTreatTransferTerminateResourcing controls and planningReaction PlanningReporting & monitoring risk performanceReviewing the Risk Management frameworkThere are a number of guidelines and publications regarding Emergency Planning, published by various professional organizations such as ASIS, National Fire Protection Association (NFPA), and the International Association of Emergency Managers (IAEM). There are very few Emergency Management specific standards, and emergency management as a discipline tends to fall under business resilience standards.In order to avoid, or reduce significant losses to a business, emergency managers should work to identify and anticipate potential risks, hopefully to reduce their probability of occurring. In the event that an emergency does occur, managers should have a plan prepared to mitigate the effects of that emergency, as well as to ensure Business Continuity of critical operations post-incident. It is essential for an organization to include procedures for determining whether an emergency situation has occurred and at what point an emergency management plan should be activated. An emergency plan must be regularly maintained, in a structured and methodical manner, to ensure it is up-to-date in the event of an emergency. Emergency managers generally follow a common process to anticipate, assess, prevent, prepare, respond and recover from an incident.Cleanup during disaster recovery involves many occupational hazards. Often these hazards are exacerbated by the conditions of the local environment as a result of the natural disaster. While individual workers should be aware of these potential hazards, employers are responsible to minimize exposure to these hazards and protect workers, when possible. This includes identification and thorough assessment of potential hazards, application of appropriate personal protective equipment (PPE), and the distribution of other relevant information in order to enable safe performance of the work. Maintaining a safe and healthy environment for these workers ensures that the effectiveness of the disaster recovery is unaffected.Flood-associated injuries: Flooding disasters often expose workers to trauma from sharp and blunt objects hidden under murky waters causing lacerations, as well as open and closed fractures. These injuries are further exacerbated with exposure to the often contaminated waters, leading to increased risk for infection. When working around water, there is always the risk of drowning. In addition, the risk of hypothermia significantly increases with prolonged exposure to water temperatures less than 75 degrees Fahrenheit. Non-infectious skin conditions may also occur including miliaria, immersion foot syndrome (including trench foot), and contact dermatitis.Earthquake-associated injuries: The predominant exposure are related to building structural components, including falling debris with possible crush injury, trapped under rubble, burns, and electric shock.Chemicals can pose a risk to human health when exposed to humans at certain quantities. After a natural disaster, certain chemicals can be more prominent in the environment. These hazardous materials can be released directly or indirectly. Chemical hazards directly released after a natural disaster often occur concurrent with the event so little to no mitigation actions can take place for mitigation. For example, airborne magnesium, chloride, phosphorus, and ammonia can be generated by droughts. Dioxins can be produced by forest fires, and silica can be emitted by forest fires. Indirect release of hazardous chemicals can be intentionally released or unintentionally released. An example of intentional release is insecticides used after a flood or chlorine treatment of water after a flood. Unintentional release is when a hazardous chemical is not intentionally released. The chemical released is often toxic and serves beneficial purpose when released to the environment. These chemicals can be controlled through engineering to minimize their release when a natural disaster strikes. An example of this is agrochemicals from inundated storehouses or manufacturing facilities poisoning the floodwaters or asbestos fibers released from a building collapse during a hurricane. The flowchart to the right has been adopted from research performed by Stacy Young, et al., and can be found here.Exposure limitsBelow are TLV-TWA, PEL, and IDLH values for common chemicals workers are exposed to after a natural disaster.Direct releaseMagnesiumPhosphorusAmmoniaSilicaIntentional releaseInsecticidesChlorine dioxideUnintentional releaseCrude oil components…Benzene, N-hexane, Hydrogen Sulfide, Cumene, Ethylbenzene, Naphthalene, Toluene, Xylenes, PCBsAgrochemicalsAsbestosExposure routesWhen a toxicant is prominent in an environment after a natural disaster, it is important to determine the route of exposure to worker safety for the disaster management workers. The 3 components are source of exposure, pathway of the chemical, and receptor. Questions to ask when dealing with chemical source is the material itself, how it’s used, how much is used, how often the chemical is used, temperature, vapor pressure, physical processes. The physical state of the chemical is important to identify. If working indoors, room ventilation, and volume of room needs to be noted to help mitigate health defects from the chemical. Lastly, to ensure worker safety, routes of entry for the chemical should be determined as well as relevant personal protective equipment needs to be worn.RespiratorsAccording to the CDC “If you need to collect belongings or do basic clean up in your previously flooded home, you do not usually need to use a respirator (a mask worn to prevent breathing in harmful substances).” A respirator should be worn when performing an operation in an enclosed environment such as a house that creates ample amounts of dust. These activities could include sweeping dust, using power saws and equipment, or cleaning up mold. If you encounter dust, the CDC says to “limit your contact with the dust as much as possible. Use wet mops or vacuums with HEPA filters instead of dry sweeping and lastly wear a respirator that protects against dust in the air. A respirator that is approved by the CDC/NIOSH is the N95 respirator and can be a good personal protective equipment to protect from dust and mold in the air from the associated natural disaster.Mold exposures: Exposure to mold is commonly seen after a natural disaster such as flooding, hurricane, tornado or tsunami. Mold growth can occur on both the exterior and interior of residential or commercial buildings. Warm and humid condition encourages mold growth therefore, standing water and excess moisture after a natural disaster would provide an ideal environment for mold growth especially in tropical regions. While the exact number of mold species is unknown, some examples of commonly found indoor molds are Aspergillus, Cladosporium, Alternaria and Penicillium. Reaction to molds differ between individuals and can range from mild symptoms such as eye irritation, cough to severe life-threatening asthmatic or allergic reactions. People with history of chronic lung disease, asthma, allergy, other breathing problems or those that are immunocompromised could be more sensitive to molds and may develop fungal pneumonia.The most effective approach to control mold growth after a natural disaster is to control moisture level. Some ways to prevent mold growth after a natural disaster include opening all doors and windows, using fans to dry out the building, positioning fans to blow air out of the windows and cleaning up the building within the first 24–48 hours. All wet items that cannot be properly cleaned and dried within the first 48 hours should be promptly removed and discarded from the building. If mold growth is found in the building, it is important to concurrently remove the molds and fix the underlying moisture problem. When removing molds, N-95 masks or respirators with a higher protection level should be used to prevent inhalation of molds into the respiratory system. Molds can be removed from hard surfaces by soap and water, a diluted bleach solution or commercial products.Human remains: According to the Center for Disease Control and Prevention (CDC), ""There is no direct risk of contagion or infectious disease from being near human remains for people who are not directly involved in recovery or other efforts that require handling dead bodies.” Most viruses and bacteria perish along with the human body after death. Therefore, no excessive measures are necessary when handling human remains indirectly. However, for workers in direct contact with human remains, universal precautions should be exercised in order to prevent unnecessary exposure to blood-borne viruses and bacteria. Relevant PPE includes eye protection, face mask or shield, and gloves. The predominant health risk are gastrointestinal infections through fecal-oral contamination, so hand hygiene is paramount to prevention. Mental health support should also be available to workers who endure psychological stress during and after recovery.Flood-associated skin infections: Flood waters are often contaminated with bacteria and waste as well as chemicals on occasion. Prolonged, direct contact with these waters leads to an increased risk for skin infection, especially with open wounds in the skin or history of a previous skin condition, such as atopic dermatitis or psoriasis. These infections are exacerbated with a compromised immune system or an aging population. The most common bacterial skin infections are usually with Staphylococcus and Streptococcus. One of the most uncommon, but well-known bacterial infections is from Vibrio vulnificus, which causes a rare, but often fatal infection called necrotizing fasciitis. Other salt-water Mycobacterium infections include the slow growing M. marinum and fast growing M. fortuitum, M. chelonae, and M. abscessus. Fresh-water bacterial infections include aeromonas hydrophila, Burkholderia pseudomallei causing melioidosis, leptospira interrogans causing leptospirosis, and chromobacterium violaceum. Fungal infections may lead to chromoblastomycosis, blastomycosis, mucormycosis, and dermatophytosis. Numerous other arthropod, protozoal, and parasitic infections have been described. A worker can reduce the risk of flood-associated skin infections by avoiding the water if an open wound is present, or at minimum, cover the open wound with a waterproof bandage. Should contact with flood water occur, the open wound should be washed thoroughly with soap and clean water.Providing disaster recovery assistance is both rewarding and stressful. According to the CDC, ""Sources of stress for emergency responders may include witnessing human suffering, risk of personal harm, intense workloads, life-and-death decisions, and separation from family."" These stresses need to be prevented or effectively managed in order to optimize assistance without causing danger to oneself. Preparation as an emergency responder is key, in addition to establishing care for responsibilities at home. During the recovery efforts, it is critical to understand and recognize burnout and sources of stress. After the recovery, it is vital to take time away from the disaster scene and slowly re-integrate back to the normal work environment. Substance Abuse and Mental Health Services Administration (SAMHSA) provides stress prevention and management resources for disaster recovery responders.The Federal Emergency Management Agency (FEMA) advises those who desire to assist go through organized volunteer organizations and not to self-deploy to affected locations. The National Volunteer Organizations Active in Disaster (VOAD) serves as the primary point of contact for volunteer organization coordination. All states have their own state VOAD organization. As a volunteer, since an employer does not have oversight, one must be vigilant and protect against possible physical, chemical, biological, and psychosocial exposures. Furthermore, there must be defined roles with relevant training available. Proper tools and PPE may or may not be available, so safety and liability should always be considered.Every employer is required to maintain a safe and healthy workplace for its employees. When an emergency situation occurs, employers are expected to protect workers from all harm resulting from any potential hazard, including physical, chemical, and biological exposure. In addition, an employer should provide pre-emergency training and build an emergency action plan.A written document about what actions employers and employees should take when responding to an emergency situation. According to OSHA regulations 1910.38, an employer must have an emergency action plan whenever an OSHA standard in this part requires one. To develop an emergency action plan, an employer should start from workplace evaluation. Typically, most of the occupational emergency management can be divided into worksite evaluation, exposure monitoring, hazard control, work practices, and training.Worksite evaluation is about identifying the source and location of the potential hazards such as fall, noise, cold, heat, hypoxia, infectious materials, and toxic chemicals that each of the workers may encounter during emergency situations.After identifying the source and location of the hazard(s), it is essential to monitor how employees may be exposed to these dangers. Employers should conduct task-specific exposure monitoring when they meet following requirements:When the exposed substance has specific standard required by OSHAWhen employers anticipate workers will be exposed to more hazards than the action level set by OSHAWhen there is a worker complaint or concern about the exposureWhen an employee questions the effectiveness of the existing hazard control methodsTo effectively acquire the above information, an employer can ask workers how they perform the task or use direct reading instruments to identify the exposure level and exposure route.Employers can conduct hazard control by:Elimination or substitution: Eliminating the hazard from the workplace.Engineering controlsWork practice or administrative controls: Change the how the task was performed to reduce the probability of exposure.Personal protective equipmentEmployers should train their employees annually before an emergency action plan is implemented. [29 CFR 1910.38(e)] The purpose of training is to inform employees of their responsibilities and/or plan of action during emergency situations. The training program should include the types of emergencies that may occur, the appropriate response, evacuation procedure, warning/reporting procedure, and shutdown procedures. Training requirements are different depending on the size of workplace and workforce, processes used, materials handled, available resources and who will be in charge during an emergency.The training program should address the following information:Workers' roles and responsibilities.Potential hazards and hazard-preventing actions.Notification alarm system, and communications processCommunication means between family members in an emergency.First Aid KitEmergency response procedures.Evacuation procedures.A list of emergency equipment including its location and function.Emergency shutdown procedures.After the emergency action plan is completed, employer and employees should review the plan carefully and post it in a public area that is accessible to everyone. In addition, another responsibility of the employer is to keep a record of any injury or illness of workers according to OSHA/State Plan Record-keeping regulations.Emergency management plans and procedures should include the identification of appropriately trained staff members responsible for decision-making when an emergency occurs. Training plans should include internal people, contractors and civil protection partners, and should state the nature and frequency of training and testing.Testing of a plan's effectiveness should occur regularly. In instances where several business or organisations occupy the same space, joint emergency plans, formally agreed to by all parties, should be put into place.Drills and exercises in preparation for foreseeable hazards are often held, with the participation of the services that will be involved in handling the emergency, and people who will be affected. Drills are held to prepare for the hazards of fires, tornadoes, lockdown for protection, earthquakes, etc.Communication is one of the key issues during any emergency, pre-planning of communications is critical. Miscommunication can easily result in emergency events escalating unnecessarily.Once an emergency has been identified a comprehensive assessment evaluating the level of impact and its financial implications should be undertaken. Following assessment, the appropriate plan or response to be activated will depend on a specific pre-set criteria within the emergency plan. The steps necessary should be prioritized to ensure critical functions are operational as soon as possible. The critical functions are those that makes the plan untenable if not operationalized.The Communication policy must be well known and rehearsed and all targeted audiences or publics and individuals must be alert. All Communication infrastructure must be as prepared as possible with all information on groupings clearly identified.Emergency management consists of five phases: prevention, mitigation, preparedness, response and recovery. http://www.fema.gov/mission-areasIt focuses on preventing the human hazard, primarily from potential natural disasters or terrorist attacks. Preventive measures are taken on both the domestic and international levels, designed to provide permanent protection from disasters. also by doing this the risk of loss of life and injury can be mitigated with good evacuation plans, environmental planning and design standards. In January 2005, 167 Governments adopted a 10-year global plan for natural disaster risk reduction called the Hyogo Framework.Preventing or reducing the impacts of disasters on our communities is a key focus for emergency management efforts today. Prevention and mitigation also help reduce the financial costs of disaster response and recovery. Public Safety Canada is working with provincial and territorial governments and stakeholders to promote disaster prevention and mitigation using a risk-based and all-hazards approach. In 2009, Federal/Provincial/Territorial Ministers endorsed a National Disaster Mitigation Strategy.Disaster mitigation measures are those that eliminate or reduce the impacts and risks of hazards through proactive measures taken before an emergency or disaster occurs.Preventive or mitigation measures take different forms for different types of disasters. In earthquake prone areas, these preventive measures might include structural changes such as the installation of an earthquake valve to instantly shut off the natural gas supply, seismic retrofits of property, and the securing of items inside a building. The latter may include the mounting of furniture, refrigerators, water heaters and breakables to the walls, and the addition of cabinet latches. In flood prone areas, houses can be built on poles/stilts. In areas prone to prolonged electricity black-outs installation of a generator ensures continuation of electrical service. The construction of storm cellars and fallout shelters are further examples of personal mitigative actions.On a national level, governments might implement large scale mitigation measures. After the monsoon floods of 2010, the Punjab government subsequently constructed 22 'disaster-resilient' model villages, comprising 1885 single-storey homes, together with schools and health centres.One of the best known examples of investment in disaster mitigation is the Red River Floodway. The building of the Floodway was a joint provincial/federal undertaking to protect the City of Winnipeg and reduce the impact of flooding in the Red River Basin. It cost $62.7 million to build in the 1960s. Since then, the floodway has been used over 20 times. Its use during the 1997 Red River Flood alone saved an estimated $4.5 billion in costs from potential damage to the city. The Floodway was expanded in 2006 as a joint provincial/federal initiative.Preparedness focuses on preparing equipment and procedures for use when a disaster occurs. This equipment and these procedures can be used to reduce vulnerability to disaster, to mitigate the impacts of a disaster or to respond more efficiently in an emergency. The Federal Emergency Management Agency (FEMA) has set out a basic four-stage vision of preparedness flowing from mitigation to preparedness to response to recovery and back to mitigation in a circular planning process. This circular, overlapping model has been modified by other agencies, taught in emergency class and discussed in academic papers.FEMA also operates a Building Science Branch that develops and produces multi-hazard mitigation guidance that focuses on creating disaster-resilient communities to reduce loss of life and property.FEMA advises citizens to prepare their homes with some emergency essentials in the case that the food distribution lines are interrupted. FEMA has subsequently prepared for this contingency by purchasing hundreds of thousands of freeze dried food emergency meals ready to eat (MRE's) to dispense to the communities where emergency shelter and evacuations are implemented.Some guidelines for household preparedness have been put online by the State of Colorado, on the topics of water, food, tools, and so on.Emergency preparedness can be difficult to measure. CDC focuses on evaluating the effectiveness of its public health efforts through a variety of measurement and assessment programs.Local Emergency Planning Committees (LEPCs) are required by the United States Environmental Protection Agency under the Emergency Planning and Community Right-to-Know Act to develop an emergency response plan, review the plan at least annually, and provide information about chemicals in the community to local citizens. This emergency preparedness effort focuses on hazards presented by use and storage of extremely hazardous and toxic chemicals. Particular requirements of LEPCs includeIdentification of facilities and transportation routes of extremely hazardous substancesDescription of emergency response procedures, on and off siteDesignation of a community coordinator and facility emergency coordinator(s) to implement the planOutline of emergency notification proceduresDescription of how to determine the probable affected area and population by releasesDescription of local emergency equipment and facilities and the persons responsible for themOutline of evacuation plansA training program for emergency responders (including schedules)Methods and schedules for exercising emergency response plansAccording to the EPA, ""Many LEPCs have expanded their activities beyond the requirements of EPCRA, encouraging accident prevention and risk reduction, and addressing homeland security in their communities"" and the Agency offers advice on how to evaluate the effectiveness of these committees.Preparedness measures can take many forms ranging from focusing on individual people, locations or incidents to broader, government-based ""all hazard"" planning. There are a number of preparedness stages between ""all hazard' and individual planning, generally involving some combination of both mitigation and response planning. Business continuity planning encourages businesses to have a Disaster Recovery Plan. Community- and faith-based organizations mitigation efforts promote field response teams and inter-agency planning. School-based response teams cover everything from live shooters to gas leaks and nearby bank robberies. Educational institutions plan for cyberattacks and windstorms. Industry specific guidance exists for horse farms, boat owners and more.Family preparedness for disaster is fairly unusual. A 2013 survey found that only 19% of American families felt that they were ""very prepared"" for a disaster. Still, there are many resources available for family disaster planning. The Department of Homeland Security's Ready.gov page includes a Family Emergency Plan Checklist, has a whole webpage devoted to readiness for kids, complete with cartoon-style superheroes, and ran a Thunderclap Campaign in 2014. The Center for Disease Control has a Zombie Apocalypse website.Disasters take a variety of forms to include earthquakes, tsunamis or regular structure fires. That a disaster or emergency is not large scale in terms of population or acreage impacted or duration does not make it any less of a disaster for the people or area impacted and much can be learned about preparedness from so-called small disasters. The Red Cross states that it responds to nearly 70,000 disasters a year, the most common of which is a single family fire. Preparedness starts with an individual's everyday life and involves items and training that would be useful in an emergency. What is useful in an emergency is often also useful in everyday life. From personal preparedness, preparedness continues on a continuum through family preparedness, community preparedness and then business, non-profit and governmental preparedness. Some organizations blend these various levels. For example, the International Red Cross and Red Crescent Movement has a webpage on disaster training as well as offering training on basic preparedness such as Cardiopulmonary resuscitation and First Aid. Other non-profits such as Team Rubicon bring specific groups of people into disaster preparedness and response operations. FEMA breaks down preparedness into a pyramid, with citizens on the foundational bottom, on top of which rests local government, state government and federal government in that order.The basic theme behind preparedness is to be ready for an emergency and there are a number of different variations of being ready based on an assessment of what sort of threats exist. Nonetheless, there is basic guidance for preparedness that is common despite an area's specific dangers. FEMA recommends that everyone have a three-day survival kit for their household. Because individual household sizes and specific needs might vary, FEMA's recommendations are not item specific, but the list includes:Three-day supply of non-perishable food.Three-day supply of water – one gallon of water per person, per day.Portable, battery-powered radio or television and extra batteries.Flashlight and extra batteries.First aid kit and manual.Sanitation and hygiene items (e.g. toilet paper, menstrual hygiene products).Matches and waterproof container.Whistle.Extra clothing.Kitchen accessories and cooking utensils, including a can opener.Photocopies of credit and identification cards.Cash and coins.Special needs items, such as prescription medications, eyeglasses, contact lens solutions, and hearing aid batteries.Items for infants, such as formula, diapers, bottles, and pacifiers.Other items to meet unique family needs.Along similar lines, but not exactly the same, CDC has its own list for a proper disaster supply kit.Water—one gallon per person, per dayFood—nonperishable, easy-to-prepare itemsFlashlightBattery powered or hand crank radio (NOAA Weather Radio, if possible)Extra batteriesFirst aid kitMedications (7-day supply), other medical supplies, and medical paperwork (e.g., medication list and pertinent medical information)Multipurpose tool (e.g., Swiss army knife)Sanitation and personal hygiene itemsCopies of personal documents (e.g., proof of address, deed/lease to home, passports, birth certificates, and insurance policies)Cell phone with chargersFamily and emergency contact informationExtra cashEmergency blanketMap(s) of the areaExtra set of car keys and house keysManual can openerChildren are a special population when considering Emergency preparedness and many resources are directly focused on supporting them. SAMHSA has list of tips for talking to children during infectious disease outbreaks, to include being a good listener, encouraging children to ask questions and modeling self-care by setting routines, eating healthy meals, getting enough sleep and taking deep breaths to handle stress. FEMA has similar advice, noting that ""Disasters can leave children feeling frightened, confused, and insecure"" whether a child has experienced it first hand, had it happen to a friend or simply saw it on television. In the same publication, FEMA further notes, ""Preparing for disaster helps everyone in the family accept the fact that disasters do happen, and provides an opportunity to identify and collect the resources needed to meet basic needs after disaster. Preparation helps when people feel prepared, they cope better and so do children.""To help people assess what threats might be in order to augment their emergency supplies or improve their disaster response skills, FEMA has published a booklet called the ""Threat and Hazard Identification and Risk Assessment Guide."" (THIRA) This guide, which outlines the THIRA process, emphasizes ""whole community involvement,"" not just governmental agencies, in preparedness efforts. In this guide, FEMA breaks down hazards into three categories: Natural, technological and human caused and notes that each hazard should be assessed for both its likelihood and its significance. According to FEMA, ""Communities should consider only those threats and hazards that could plausibly occur"" and ""Communities should consider only those threats and hazards that would have a significant effect on them."" To develop threat and hazard context descriptions, communities should take into account the time, place, and conditions in which threats or hazards might occur.Not all preparedness efforts and discussions involve the government or established NGOs like the Red Cross. Emergency preparation discussions are active on the internet, with many blogs and websites dedicated to discussing various aspects of preparedness. On-line sales of items such as survival food, medical supplies and heirloom seeds allow people to stock basements with cases of food and drinks with 25 year shelf lives, sophisticated medical kits and seeds that are guaranteed to sprout even after years of storage.One group of people who put a lot of effort in disaster preparations is called Doomsday Preppers. This subset of preparedness-minded people often share a belief that the FEMA or Red Cross emergency preparation suggestions and training are not extensive enough. Sometimes called survivalists, Doomsday Preppers are often preparing for The End Of The World As We Know It, abbreviated as TEOTWAWKI. With a motto some have that ""The Future Belongs to those who Prepare,"" this Preparedness subset has its own set of Murphy's Rules, including ""Rule Number 1: Food, you still don't have enough"" and ""Rule Number 26: People who thought the Government would save them, found out that it didn't.""Not all emergency preparation efforts revolve around food, guns and shelters, though these items help address the needs in the bottom two sections of Maslow's hierarchy of needs. The American Preppers Network has an extensive list of items that might be useful in less apparent ways than a first aid kid or help add 'fun' to challenging times. These items include:Books and magazinesArts and crafts  paintingChildren's entertainmentCrayons and coloring booksNotebooks and writing suppliesNuts, bolts, screws, nails, etc.Religious materialSporting equipment, card games and board gamesPosters and banners creating awarenessEmergency preparedness goes beyond immediate family members. For many people, pets are an integral part of their families and emergency preparation advice includes them as well. It is not unknown for pet owners to die while trying to rescue their pets from a fire or from drowning. CDC's Disaster Supply Checklist for Pets includes:Food and water for at least 3 days for each pet bowls, and a manual can opener.Depending on the pet you may need a litter box, paper towels, plastic trash bags, grooming items, and/or household bleach.Medications and medical records stored in a waterproof container.First aid kit with a pet first aid book.Sturdy leash, harness, and carrier to transport pet safely. A carrier should be large enough for the animal to stand comfortably, turn around, and lie down. Your pet may have to stay in the carrier for several hours.Pet toys and the pet's bed, if you can easily take it, to reduce stress.Current photos and descriptions of your pets to help others identify them in case you and your pets become separated, and to prove that they are yours.Information on feeding schedules, medical conditions, behavior problems, and the name and telephone number of your veterinarian in case you have to board your pets or place them in foster care.Emergency preparedness also includes more than physical items and skill-specific training. Psychological preparedness is also a type of emergency preparedness and specific mental health preparedness resources are offered for mental health professionals by organizations such as the Red Cross. These mental health preparedness resources are designed to support both community members affected by a disaster and the disaster workers serving them. CDC has a website devoted to coping with a disaster or traumatic event. After such an event, the CDC, through the Substance Abuse and Mental Health Services Administration (SAMHSA), suggests that people seek psychological help when they exhibit symptoms such as excessive worry, crying frequently, an increase in irritability, anger, and frequent arguing, wanting to be alone most of the time, feeling anxious or fearful, overwhelmed by sadness, confused, having trouble thinking clearly and concentrating, and difficulty making decisions, increased alcohol and/or substance use, increased physical (aches, pains) complaints such as headaches and trouble with ""nerves.""Sometimes emergency supplies are kept in what is called a Bug-out bag. While FEMA does not actually use the term ""Bug out bag,"" calling it instead some variation of a ""Go Kit,"" the idea of having emergency items in a quickly accessible place is common to both FEMA and CDC, though on-line discussions of what items a ""bug out bag"" should include sometimes cover items such as firearms and great knives that are not specifically suggested by FEMA or CDC. The theory behind a ""bug out bag"" is that emergency preparations should include the possibility of Emergency evacuation. Whether fleeing a burning building or hastily packing a car to escape an impending hurricane, flood or dangerous chemical release, rapid departure from a home or workplace environment is always a possibility and FEMA suggests having a Family Emergency Plan for such occasions. Because family members may not be together when disaster strikes, this plan should include reliable contact information for friends or relatives who live outside of what would be the disaster area for household members to notify they are safe or otherwise communicate with each other. Along with the contact information, FEMA suggests having well-understood local gathering points if a house must be evacuated quickly to avoid the dangers of re-reentering a burning home. Family and emergency contact information should be printed on cards and put in each family member's backpack or wallet. If family members spend a significant amount of time in a specific location, such as at work or school, FEMA suggests learning the emergency preparation plans for those places. FEMA has a specific form, in English and in Spanish, to help people put together these emergency plans, though it lacks lines for email contact information.Like children, people with disabilities and other special needs have special emergency preparation needs. While ""disability"" has a specific meaning for specific organizations such as collecting Social Security benefits, for the purposes of emergency preparedness, the Red Cross uses the term in a broader sense to include people with physical, medical, sensor or cognitive disabilities or the elderly and other special needs populations. Depending on the particular disability, specific emergency preparations might be required. FEMA's suggestions for people with disabilities includes having copies of prescriptions, charging devices for medical devices such as motorized wheel chairs and a week's supply of medication readily available LINK or in a ""go stay kit."" In some instances, lack of competency in English may lead to special preparation requirements and communication efforts for both individuals and responders.FEMA notes that long term power outages can cause damage beyond the original disaster that can be mitigated with emergency generators or other power sources to provide an Emergency power system. The United States Department of Energy states that 'homeowners, business owners, and local leaders may have to take an active role in dealing with energy disruptions on their own."" This active role may include installing or other procuring generators that are either portable or permanently mounted and run on fuels such as propane or natural gas or gasoline. Concerns about carbon monoxide poisoning, electrocution, flooding, fuel storage and fire lead even small property owners to consider professional installation and maintenance. Major institutions like hospitals, military bases and educational institutions often have or are considering extensive backup power systems. Instead of, or in addition to, fuel-based power systems, solar, wind and other alternative power sources may be used. Standalone batteries, large or small, are also used to provide backup charging for electrical systems and devices ranging from emergency lights to computers to cell phones.Emergency preparedness does not stop at home or at school. The United States Department of Health and Human Services addresses specific emergency preparedness issues hospitals may have to respond to, including maintaining a safe temperature, providing adequate electricity for life support systems and even carrying out evacuations under extreme circumstances. FEMA encourages all businesses to have businesses to have an emergency response plan and the Small Business Administration specifically advises small business owners to also focus emergency preparedness and provides a variety of different worksheets and resources.FEMA cautions that emergencies happen while people are travelling as well and provides guidance around emergency preparedness for a range travelers to include commuters, Commuter Emergency Plan and holiday travelers. In particular, Ready.gov has a number of emergency preparations specifically designed for people with cars. These preparations include having a full gas tank, maintaining adequate windshield wiper fluid and other basic car maintenance tips. Items specific to an emergency include:Jumper cables: might want to include flares or reflective triangleFlashlights, to include extra batteries (batteries have less power in colder weather)First Aid Kit, to include any necessary medications, baby formula and diapers if caring for small childrenNon-perishable food such as canned food (be alert to liquids freezing in colder weather), and protein rich foods like nuts and energy barsManual can openerAt least 1 gallon of water per person a day for at least 3 days (be alert to hazards of frozen water and resultant container rupture)Basic toolkit: pliers, wrench, screwdriverPet supplies: food and waterRadio: battery or hand crankedFor snowy areas: cat litter or sand for better tire traction shovel ice scraper warm clothes, gloves, hat, sturdy boots, jacket and an extra change of clothesBlankets or sleeping bagsCharged Cell Phone: and car chargerIn addition to emergency supplies and training for various situations, FEMA offers advice on how to mitigate disasters. The Agency gives instructions on how to retrofit a home to minimize hazards from a Flood, to include installing a Backflow prevention device, anchoring fuel tanks and relocating electrical panels. Given the explosive danger posed by natural gas leaks, Ready.gov states unequivocally that ""It is vital that all household members know how to shut off natural gas"" and that property owners must ensure they have any special tools needed for their particular gas hookups. Ready.gov also notes that ""It is wise to teach all responsible household members where and how to shut off the electricity,"" cautioning that individual circuits should be shut off before the main circuit. Ready.gov further states that ""It is vital that all household members learn how to shut off the water at the main house valve"" and cautions that the possibility that rusty valves might require replacement.The response phase of an emergency may commence with Search and Rescue but in all cases the focus will quickly turn to fulfilling the basic humanitarian needs of the affected population. This assistance may be provided by national or international agencies and organizations. Effective coordination of disaster assistance is often crucial, particularly when many organizations respond and local emergency management agency (LEMA) capacity has been exceeded by the demand or diminished by the disaster itself. The National Response Framework is a United States government publication that explains responsibilities and expectations of government officials at the local, state, federal, and tribal levels. It provides guidance on Emergency Support Functions that may be integrated in whole or parts to aid in the response and recovery process.On a personal level the response can take the shape either of a shelter in place or an evacuation. In a shelter-in-place scenario, a family would be prepared to fend for themselves in their home for many days without any form of outside support. In an evacuation, a family leaves the area by automobile or other mode of transportation, taking with them the maximum amount of supplies they can carry, possibly including a tent for shelter. If mechanical transportation is not available, evacuation on foot would ideally include carrying at least three days of supplies and rain-tight bedding, a tarpaulin and a bedroll of blankets.Donations are often sought during this period, especially for large disasters that overwhelm local capacity. Due to efficiencies of scale, money is often the most cost-effective donation if fraud is avoided. Money is also the most flexible, and if goods are sourced locally then transportation is minimized and the local economy is boosted. Some donors prefer to send gifts in kind, however these items can end up creating issues, rather than helping. One innovation by Occupy Sandy volunteers is to use a donation registry, where families and businesses impacted by the disaster can make specific requests, which remote donors can purchase directly via a web site.Medical considerations will vary greatly based on the type of disaster and secondary effects. Survivors may sustain a multitude of injuries to include lacerations, burns, near drowning, or crush syndrome.The recovery phase starts after the immediate threat to human life has subsided. The immediate goal of the recovery phase is to bring the affected area back to normalcy as quickly as possible. During reconstruction it is recommended to consider the location or construction material of the property.The most extreme home confinement scenarios include war, famine and severe epidemics and may last a year or more. Then recovery will take place inside the home. Planners for these events usually buy bulk foods and appropriate storage and preparation equipment, and eat the food as part of normal life. A simple balanced diet can be constructed from vitamin pills, whole-meal wheat, beans, dried milk, corn, and cooking oil. Vegetables, fruits, spices and meats, both prepared and fresh-gardened, are included when possible.Professional emergency managers can focus on government and community preparedness, or private business preparedness. Training is provided by local, state, federal and private organizations and ranges from public information and media relations to high-level incident command and tactical skills.In the past, the field of emergency management has been populated mostly by people with a military or first responder background. Currently, the field has become more diverse, with many managers coming from a variety of backgrounds other than the military or first responder fields. Educational opportunities are increasing for those seeking undergraduate and graduate degrees in emergency management or a related field. There are over 180 schools in the US with emergency management-related programs, but only one doctoral program specifically in emergency management.Professional certifications such as Certified Emergency Manager (CEM) and Certified Business Continuity Professional (CBCP) are becoming more common as professional standards are raised throughout the field, particularly in the United States. There are also professional organizations for emergency managers, such as the National Emergency Management Association and the International Association of Emergency Managers.In 2007, Dr. Wayne Blanchard of FEMA's Emergency Management Higher Education Project, at the direction of Dr. Cortez Lawrence, Superintendent of FEMA's Emergency Management Institute, convened a working group of emergency management practitioners and academics to consider principles of emergency management. This was the first time the principles of the discipline were to be codified. The group agreed on eight principles that will be used to guide the development of a doctrine of emergency management. Below is a summary:Comprehensive – consider and take into account all hazards, all phases, all stakeholders and all impacts relevant to disasters.Progressive – anticipate future disasters and take preventive and preparatory measures to build disaster-resistant and disaster-resilient communities.Risk-driven – use sound risk management principles (hazard identification, risk analysis, and impact analysis) in assigning priorities and resources.Integrated – ensure unity of effort among all levels of government and all elements of a community.Collaborative – create and sustain broad and sincere relationships among individuals and organizations to encourage trust, advocate a team atmosphere, build consensus, and facilitate communication.Coordinated – synchronize the activities of all relevant stakeholders to achieve a common purpose.Flexible – use creative and innovative approaches in solving disaster challenges.Professional – value a science and knowledge-based approach based on education, training, experience, ethical practice, public stewardship and continuous improvement.A fuller description of these principles can be found atIn recent years the continuity feature of emergency management has resulted in a new concept, Emergency Management Information Systems (EMIS). For continuity and inter-operability between emergency management stakeholders, EMIS supports an infrastructure that integrates emergency plans at all levels of government and non-government involvement for all four phases of emergencies. In the healthcare field, hospitals utilize the Hospital Incident Command System (HICS), which provides structure and organization in a clearly defined chain of command.Practitioners in emergency management come from an increasing variety of backgrounds. Professionals from memory institutions (e.g., museums, historical societies, etc.) are dedicated to preserving cultural heritage—objects and records. This has been an increasingly major component within this field as a result of the heightened awareness following the September 11 attacks in 2001, the hurricanes in 2005, and the collapse of the Cologne Archives.To increase the potential successful recovery of valuable records, a well-established and thoroughly tested plan must be developed. This plan should emphasize simplicity in order to aid in response and recovery: employees should perform similar tasks in the response and recovery phase that they perform under normal conditions. It should also include mitigation strategies such as the installation of sprinklers within the institution. Professional associations hold regular workshops to keep individuals up to date with tools and resources in order to minimize risk and maximize recovery.In 2008, the U.S. Agency for International Development created a web-based tool for estimating populations impacted by disasters. Called Population Explorer the tool uses land scan population data, developed by Oak Ridge National Laboratory, to distribute population at a resolution 1 km2 for all countries in the world. Used by USAID's FEWS NET Project to estimate populations vulnerable and or imd by food insecurity, Population Explorer is gaining wide use in a range of emergency analysis and response actions, including estimating populations impacted by floods in Central America and the Pacific Ocean tsunami event in 2009.In 2007, a checklist for veterinarians was published in the Journal of the American Veterinary Medical Association, it had two sets of questions for a professional to ask themselves before assisting with an emergency:Absolute requirements for participation:Have I chosen to participate?Have I taken ICS training?Have I taken other required background courses?Have I made arrangements with my practice to deploy?Have I made arrangements with my family?Incident participation:Have I been invited to participate?Are my skill sets a match for the mission?Can I access just-in-time training to refresh skills or acquire needed new skills?Is this a self-support mission?Do I have supplies needed for three to five days of self-support?While written for veterinarians, this checklist is applicable for any professional to consider before assisting with an emergency.The International Emergency Management Society (TIEMS), is an international non-profit NGO, registered in Belgium. TIEMS is a Global Forum for Education, Training, Certification and Policy in Emergency and Disaster Management. TIEMS' goal is to develop and bring modern emergency management tools, and techniques into practice, through the exchange of information, methodology innovations and new technologies.TIEMS provides a platform for stakeholders to meet, network and learn about new technical and operational methodologies. TIEMS focuses on cultural differences to be understood and included in the society's events, education and research programs. This is achieved by establishing local chapters worldwide. Today, TIEMS has chapters in Benelux, Romania, Finland, Italy, Middle East and North Africa (MENA), Iraq, India, Korea, Japan and China.The International Association of Emergency Managers (IAEM) is a non-profit educational organization aimed at promoting the goals of saving lives and property protection during emergencies. The mission of IAEM is to serve its members by providing information, networking and professional opportunities, and to advance the emergency management profession.It has seven councils around the world: Asia, Canada, Europa, International, Oceania, Student and USA.The Air Force Emergency Management Association, affiliated by membership with the IAEM, provides emergency management information and networking for U.S. Air Force Emergency Management personnel.The International Recovery Platform (IRP) was conceived at the World Conference on Disaster Reduction (WCDR) in Kobe, Hyogo, Japan in January 2005, as part of the Hyogo Framework for Action (HFA) 2005–2015. The HFA is a global plan for disaster risk reduction adopted by 168 governments.The key role of IRP is to identify gaps in post disaster recovery and to serve as a catalyst for the development of tools and resources for recovery efforts.The International Federation of Red Cross and Red Crescent Societies (IFRC) works closely with National Red Cross and Red Crescent societies in responding to emergencies, many times playing a pivotal role. In addition, the IFRC may deploy assessment teams, e.g. Field Assessment and Coordination Teams (FACT), to the affected country if requested by the national society. After assessing the needs, Emergency Response Units (ERUs) may be deployed to the affected country or region. They are specialized in the response component of the emergency management framework.Baptist Global Response (BGR) is a disaster relief and community development organization. BGR and its partners respond globally to people with critical needs worldwide, whether those needs arise from chronic conditions or acute crises such as natural disasters. While BGR is not an official entity of the Southern Baptist Convention, it is rooted in Southern Baptist life and is the international partnership of Southern Baptist Disaster Relief teams, which operate primarily in the US and Canada.The United Nations system rests with the Resident Coordinator within the affected country. However, in practice, the UN response will be coordinated by the UN Office for the Coordination of Humanitarian Affairs (UN-OCHA), by deploying a UN Disaster Assessment and Coordination (UNDAC) team, in response to a request by the affected country's government. Finally UN-SPIDER designed as a networking hub to support disaster management by application of satellite technologySince 1980, the World Bank has approved more than 500 projects related to disaster management, dealing with both disaster mitigation as well as reconstruction projects, amounting to more than US$40 billion. These projects have taken place all over the world, in countries such as Argentina, Bangladesh, Colombia, Haiti, India, Mexico, Turkey and Vietnam.Prevention and mitigation projects include forest fire prevention measures, such as early warning measures and education campaigns early-warning systems for hurricanes flood prevention mechanisms (e.g. shore protection, terracing, etc.) and earthquake-prone construction. In a joint venture with Columbia University under the umbrella of the ProVention Consortium the World Bank has established a Global Risk Analysis of Natural Disaster Hotspots.In June 2006, the World Bank, in response to the HFA, established the Global Facility for Disaster Reduction and Recovery (GFDRR), a partnership with other aid donors to reduce disaster losses. GFDRR helps developing countries fund development projects and programs that enhance local capacities for disaster prevention and emergency preparedness.In 2001 the EU adopted Community Mechanism for Civil Protection, to facilitate co-operation in the event of major emergencies requiring urgent response actions. This also applies to situations where there may be an imminent threat as well.The heart of the Mechanism is the Monitoring and Information Center (MIC), part of the European Commission's Directorate-General for Humanitarian Aid & Civil Protection. Accessible 24 hours a day, it gives countries access to a one-stop-shop of civil protections available amongst all the participating states. Any country inside or outside the Union affected by a major disaster can make an appeal for assistance through the MIC. It acts as a communication hub, and provides useful and updated information on the actual status of an ongoing emergency.UN-SPIDERNaers are part of life in Australia. Heatwaves have killed more Australians than any other type of natural disaster in the 20th century. Australia's emergency management processes embrace the concept of the prepared community. The principal government agency in achieving this is Emergency Management Australia.Public Safety Canada is Canada's national emergency management agency. Each province is required to have both legislation for dealing with emergencies, and provincial emergency management agencies, typically called ""Emergency Measures Organizations"" (EMO). Public Safety Canada co-ordinates and supports the efforts of federal organizations as well as other levels of government, first responders, community groups, the private sector, and other nations. The Public Safety and Emergency Preparedness Act defines the powers, duties and functions of PS are outlined. Other acts are specific to individual fields such as corrections, law enforcement, and national security.In Germany the Federal Government controls the German Katastrophenschutz (disaster relief), the Technisches Hilfswerk (Federal Agency for Technical Relief, THW), and the Zivilschutz (civil protection) programs coordinated by the Federal Office of Civil Protection and Disaster Assistance. Local fire department units, the German Armed Forces (Bundeswehr), the German Federal Police and the 16 state police forces (Länderpolizei) are also deployed during disaster relief operations.There are several private organizations in Germany that also deal with emergency relief. Among these are the German Red Cross, Johanniter-Unfall-Hilfe (the German equivalent of the St. John Ambulance), the Malteser-Hilfsdienst, and the Arbeiter-Samariter-Bund. As of 2006, there is a program of study at the University of Bonn leading to the degree ""Master in Disaster Prevention and Risk Governance"" As a support function radio amateurs provide additional emergency communication networks with frequent trainings.The National Disaster Management Authority is the primary government agency responsible for planning and capacity-building for disaster relief. Its emphasis is primarily on strategic risk management and mitigation, as well as developing policies and planning. The National Institute of Disaster Management is a policy think-tank and training institution for developing guidelines and training programs for mitigating disasters and managing crisis response.The National Disaster Response Force is the government agency primarily responsible for emergency management during natural and man-made disasters, with specialized skills in search, rescue and rehabilitation. The Ministry of Science and Technology also contains an agency that brings the expertise of earth scientists and meteorologists to emergency management. The Indian Armed Forces also plays an important role in the rescue/recovery operations after disasters.Aniruddha's Academy of Disaster Management (AADM) is a non-profit organization in Mumbai, India with 'disaster management' as its principal objective.In Malaysia, The National Disaster Management Agency (NADMA Malaysia) under the Prime Minister's Department was established on 2 October 2015 following the Flood Disaster in 2014 and taken over the roles previously National Security Council. NADMA Malaysia is the focal point in managing disaster in Malaysia. Ministry of Home Affairs Malaysia, Ministry of Health Malaysia and Ministry of Housing, Urban Wellbeing and Local Government Malaysia are also having responsibility in managing emergency. Several agencies are involved in emergency managements are Royal Malaysian Police, Malaysian Fire and Rescue Department, Malaysian Civil Defence Force, Ministry of Health Malaysia and Malaysian Maritime Enforcement Agency. There were also some voluntary organisation who involved themselves in emergency/ disaster management such as St. John Ambulance of Malaysia, Malaysian Red Crescent Society and so on.The Nepal Risk Reduction Consortium (NRRC) is based on hyogo Framework and Nepal's National Strategy for Disaster Risk Management. This arrangement unites humanitarian and development partners with Government of Nepal and had identified 5 flagship priorities for sustainable disaster risk management.In New Zealand, depending on the scope of the emergency/disaster, responsibility may be handled at either the local or national level. Within each region, local governments are organized into 16 Civil Defence Emergency Management Groups (CMGs). If local arrangements are overwhelmed, pre-existing mutual-support arrangements are activated. Central government has the authority to coordinate the response through the National Crisis Management Centre (NCMC), operated by the Ministry of Civil Defence & Emergency Management (MCDEM). These structures are defined by regulation, and explained in The Guide to the National Civil Defence Emergency Management Plan 2006, roughly equivalent to the U.S. Federal Emergency Management Agency's National Response Framework.New Zealand uses unique terminology for emergency management. Emergency management is rarely used, many government publications retaining the use of the term civil defence. For example, the Minister of Civil Defence is responsible for the MCDEM. Civil Defence Emergency Management is a term in its own right, defined by statute. And disaster rarely appears in official publications, emergency and incident being the preferred terms, with the term event also being used. For example, publications refer to the Canterbury Snow Event 2002.""4Rs"" is the emergency management cycle used in New Zealand, its four phases are known as:Reduction = MitigationReadiness = PreparednessResponseRecoveryDisaster management in Pakistan revolves around flood disasters focusing on rescue and relief.Federal Flood Commission was established in 1977 under Ministry of Water and Power to manage the issues of flood management on country-wide basis.The National Disaster Management Ordinance, 2006 and National Disaster Management Act, 2010 were enacted after 2005 Kashmir earthquake and 2010 Pakistan floods respectively to deal with disaster management.  The primary central authority mandated to deal with whole spectrum of disasters and their management in the country is National Disaster Management Authority.In addition each province along with FATA, Gilgit Baltistan and Pakistani administered Kashmir has its own provincial disaster management authority responsible for implementing policies and plans for Disaster Management in the Province.Each District has its own District Disaster Management Authority for planning, coordinating and implementing body for disaster management and take all measures for the purposes of disaster management in the districts in accordance with the guidelines laid down by the National Authority and the Provincial Authority.In the Philippines, the National Disaster Risk Reduction and Management Council is responsible for the protection and welfare of people during disasters or emergencies. It is a working group composed of various government, non-government, civil sector and private sector organizations of the Government of the Republic of the Philippines. Headed by the Secretary of National Defense (under the Office of Civil Defense, the NDRRMCs implementing organization), it coordinates all the executive branches of government, presidents of the leagues of local government units throughout the country, the Armed Forces of the Philippines, Philippine National Police, Bureau of Fire Protection (which is an agency under the Department of Interior and Local Government, and the public and private medical services in responding to natural and manmade disasters, as well as planning, coordination, and training of these responsible units. Non-governmental organizations such as the Philippine Red Cross also provide manpower and material support for NDRRMC.Regional, provincial, city, municipal, and barangay emergency management are handled by Local Disaster Risk Reduction Management Councils (LDRRMCs), which are the functional arm of the local government unit (LGU). Each LDRRMC is headed by a Chief DRRM Officer, and each Office is tasked to organize their own emergency teams and command-and-control centers when activated at times of local emergencies (identified on the regional, provincial, city, municipal, and/or barangay level).In Russia, the Ministry of Emergency Situations (EMERCOM) is engaged in fire fighting, civil defense, and search and rescue after both natural and human-made disasters.In Somalia, the Federal Government announced in May 2013 that the Cabinet had approved draft legislation on a new Somali Disaster Management Agency (SDMA), which had originally been proposed by the Ministry of Interior. According to the Prime Minister's Media Office, the SDMA will lead and coordinate the government's response to various natural disasters. It is part of a broader effort by the federal authorities to re-establish national institutions. The Federal Parliament is now expected to deliberate on the proposed bill for endorsement after any amendments.In the Netherlands the Ministry of Security and Justice is responsible for emergency preparedness and emergency management on a national level and operates a national crisis centre (NCC). The country is divided into 25 safety regions (veiligheidsregio). In a safety region, there are four components: the regional fire department, the regional department for medical care(ambulances and psycho-sociological care etc.), the regional dispatch and a section for risk- and crisis management. The regional dispatch operates for police, fire department and the regional medical care. The dispatch has all these three services combined into one dispatch for the best multi-coordinated response to an incident or an emergency. And also facilitates in information management, emergency communication and care of citizens. These services are the main structure for a response to an emergency. It can happen that, for a specific emergency, the co-operation with an other service is needed, for instance the Ministry of Defence, water board(s) or Rijkswaterstaat. The veiligheidsregio can integrate these other services into their structure by adding them to specific conferences on operational or administrative level.All regions operate according to the Coordinated Regional Incident Management system.Following the 2000 fuel protests and severe flooding that same year, as well as the foot-and-mouth crisis in 2001, the United Kingdom passed the Civil Contingencies Act 2004 (CCA). The CCA defined some organisations as Category 1 and 2 Responders, setting responsibilities regarding emergency preparedness and response. It is managed by the Civil Contingencies Secretariat through Regional Resilience Forums and local authorities.Disaster Management training is generally conducted at the local level, and consolidated through professional courses that can be taken at the Emergency Planning College. Diplomas, undergraduate and postgraduate qualifications can be gained at universities throughout the country. The Institute of Emergency Management is a charity, established in 1996, providing consulting services for the government, media and commercial sectors. There are a number of professional societies for Emergency Planners including the Emergency Planning Society and the Institute of Civil Protection and Emergency Management.One of the largest emergency exercises in the UK was carried out on 20 May 2007 near Belfast, Northern Ireland: a simulated plane crash-landing at Belfast International Airport. Staff from five hospitals and three airports participated in the drill, and almost 150 international observers assessed its effectiveness.Disaster management in the United States has utilized the functional All-Hazards approach for over 20 years, in which managers develop processes (such as communication & warning or sheltering) rather than developing single-hazard or threat focused plans (e.g., a tornado plan). Processes are then mapped to specific hazards or threats, with the manager looking for gaps, overlaps, and conflicts between processes.Given these notions, emergency managers must identify, contemplate, and assess possible man-made threats and natural threats that may affect their respective locales. Because of geographical differences throughout the nation, a variety of different threats affect communities among the states. Thus, although similarities may exist, no two emergency plans will be completely identical. Additionally, each locale has different resources and capacities (e.g., budgets, personnel, equipment, etc.) for dealing with emergencies. Each individual community must craft its own unique emergency plan that addresses potential threats that are specific to the locality.This creates a plan more resilient to unique events because all common processes are defined, and it encourages planning done by the stakeholders who are closer to the individual processes, such as a traffic management plan written by a public works director. This type of planning can lead to conflict with non-emergency management regulatory bodies, which require the development of hazard/threat specific plans, such as the development of specific H1N1 flu plans and terrorism-specific plans.In the United States, all disasters are initially local, with local authorities, with usually a police, fire, or EMS agency, taking charge. Many local municipalities may also have a separate dedicated office of emergency management (OEM), along with personnel and equipment. If the event becomes overwhelming to the local government, state emergency management (the primary government structure of the United States) becomes the controlling emergency management agency. Federal Emergency Management Agency (FEMA), part of the Department of Homeland Security (DHS), is the lead federal agency for emergency management. The United States and its territories are broken down into ten regions for FEMA's emergency management purposes. FEMA supports, but does not override, state authority.The Citizen Corps is an organization of volunteer service programs, administered locally and coordinated nationally by DHS, which seek to mitigate disasters and prepare the population for emergency response through public education, training, and outreach. Most disaster response is carried out by volunteer organizations. In the US, the Red Cross is chartered by Congress to coordinate disaster response services. It is typically the lead agency handling shelter and feeding of evacuees. Religious organizations, with their ability to provide volunteers quickly, are usually integral during the response process. The largest being the Salvation Army, with a primary focus on chaplaincy and rebuilding, and Southern Baptists who focus on food preparation and distribution, as well as cleaning up after floods and fires, chaplaincy, mobile shower units, chainsaw crews and more. With over 65,000 trained volunteers, Southern Baptist Disaster Relief is one of the largest disaster relief organizations in the US. Similar services are also provided by Methodist Relief Services, the Lutherans, and Samaritan's Purse. Unaffiliated volunteers show up at most large disasters. To prevent abuse by criminals, and for the safety of the volunteers, procedures have been implemented within most response agencies to manage and effectively use these 'SUVs' (Spontaneous Unaffiliated Volunteers).The US Congress established the Center for Excellence in Disaster Management and Humanitarian Assistance (COE) as the principal agency to promote disaster preparedness in the Asia-Pacific region.The National Tribal Emergency Management Council (NEMC) is a non-profit educational organization developed for Tribal organizations to share information and best practices, as well as to discuss issues regarding public health and safety, emergency management and homeland security, affecting those under Indian sovereignty. NTMC is organized into Regions, based on the FEMA 10 region system. NTMC was founded by the Northwest Tribal Emergency Management Council (NWTEMC), a consortium of 29 Tribal Nations and Villages in Washington, Idaho, Oregon, and Alaska.If a disaster or emergency is declared to be terror related or an ""Incident of National Significance,"" the Secretary of Homeland Security will initiate the National Response Framework (NRF). The NRF allows the integration of federal resources with local, country, state, or tribal entities, with management of those resources to be handled at the lowest possible level, utilizing the National Incident Management System (NIMS).The Centers for Disease Control and Prevention offer information for specific types of emergencies, such as disease outbreaks, natural disasters and severe weather, chemical and radiation accidents, etc. The Emergency Preparedness and Response Program of the National Institute for Occupational Safety and Health develops resources to address responder safety and health during responder and recovery operations.The Emergency Management Institute (EMI) serves as the national focal point for the development and delivery of emergency management training to enhance the capabilities of state, territorial, local, and tribal government officials volunteer organizations FEMA's disaster workforce other Federal agencies and the public and private sectors to minimize the impact of disasters and emergencies on the American public. EMI curricula are structured to meet the needs of this diverse audience with an emphasis on separate organizations working together in all-hazards emergencies to save lives and protect property. Particular emphasis is placed on governing doctrine such as the National Response Framework (NRF), National Incident Management System (NIMS), and the National Preparedness Guidelines. EMI is fully accredited by the International Association for Continuing Education and Training (IACET) and the American Council on Education (ACE).Approximately 5,500 participants attend resident courses each year while 100,000 individuals participate in non-resident programs sponsored by EMI and conducted locally by state emergency management agencies under cooperative agreements with FEMA. Another 150,000 individuals participate in EMI-supported exercises, and approximately 1,000 individuals participate in the Chemical Stockpile Emergency Preparedness Program (CSEPP).The independent study program at EMI consists of free courses offered to United States citizens in Comprehensive Emergency Management techniques. Course IS-1 is entitled ""Emergency Manager: An Orientation to the Position"" and provides background information on FEMA and the role of emergency managers in agency and volunteer organization coordination. The EMI Independent Study (IS) Program, a Web-based distance learning program open to the public, delivers extensive online training with approximately 200 courses. It has trained more than 2.8 million individuals. The EMI IS Web site receives 2.5 to 3 million visitors a day.In emergency or disaster management the SMAUG model of identifying and prioritizing risk of hazards associated with natural and technological threats is an effective tool. SMAUG stands for Seriousness, Manageability, Acceptability, Urgency and Growth and are the criteria used for prioritization of hazard risks. The SMAUG model provides an effective means of prioritizing hazard risks based upon the aforementioned criteria in order to address the risks posed by the hazards to the avail of effecting effective mitigation, reduction, response and recovery methods.Seriousness can be defined as ""The relative impact in terms of people and dollars."" This includes the potential for lives to be lost and potential for injury as well as the physical, social and as mentioned, economic losses that may be incurredManageability can be defined as ""the relative ability to mitigate or reduce the hazard (through managing the hazard, or the community or both)"". Hazards presenting a high risk and as such requiring significant amounts of risk reduction initiatives will be rated high.Acceptability – The degree to which the risk of hazard is acceptable in terms of political, environmental, social and economic impactUrgency – This is related to the probability of risk of hazard and is defined in terms of how imperative it is to address the hazard Growth – This is the potential for the hazard or event to expand or increase in either probability or risk to community or both. Should vulnerability increase, potential for growth may also increase.An example of the numerical ratings for each of the four criteria is shown below:Computer emergency response teamBusiness continuityDisaster medicineDisaster responseDisaster risk reductionEmergency communication systemEmergency sanitationHuman capital flightMass fatality incidentPublic health emergency (United States)Rohn emergency scaleNGOs:International Journal of Emergency Management, ISSN 1741-5071 (electronic) ISSN 1471-4825 (paper), Inderscience PublishersJournal of Homeland Security and Emergency Management ISSN 1547-7355, BepressAustralian Journal of Emergency Management (electronic) ISSN 1324-1540 (paper), Emergency Management AustraliaKaranasios, S. (2011). New and Emergent ICTs and Climate Change in Developing Countries. In R. Heeks & A. Ospina (Eds.). Manchester: Centre for Development Informatics, University of ManchesterThe ALADDIN Project, a consortium of universities developing automated disaster management toolsEmergency Management Australia (2003) Community Developments in Recovering from Disaster, Commonwealth of Australia, CanberraPlan and Preparation: Surviving the Zombie Apocalypse, (paperback), CreateSpace, Introductory concepts to planning and preparing for emergencies and disasters of any kind.Emergency Preparedness and Response Office The National Institute for Occupational Safety and HealthFAO in emergenciesResilient Livelihoods: Disaster Risk Reduction for Food and Nutrition Security – 2013 edition published by FAOCenters for Disease Control and Prevention's Information on Specific Types of EmergenciesEmergency Preparedness and Response Resources by the National Institute for Occupational Safety and Health";environmental disaster;Emergency management;0
Environmental emergencies are defined as “sudden-onset disasters or accidents resulting from natural, technological or human-induced factors, or a combination of these, that causes or threatens to cause severe environmental damage as well as loss of human lives and property.” (UNEP/GC.22/INF/5, 13 November 2002.)Following a disaster or conflict, an environmental emergency can occur when people’s health and livelihoods are at risk due to the release of hazardous and noxious substances, or because of significant damage to the ecosystem. Examples include fires, oil spills, chemical accidents, toxic waste dumping and groundwater pollution.The environmental risks can be acute and life-threatening. According to the International Disaster Database (EM-DAT), between 2003 and 2013, there were 380 industrial accidents reported, affecting 207 668 people and resulting in over US$22 million in losses. Climate change is having an unprecedented effect on the occurrence of natural disasters and the associated risk of environmental emergencies. With climate change already stretching the disaster relief system, future climate-related emergency events will generate increased and more costly demands for assistance.All disasters have some environmental impacts.Some of these may be immediate and life-threatening – for example, when an earthquake damages an industrial facility, which in turn releases hazardous materials. In such cases these so-called ‘secondary impacts’ may cause as much damage as the initial causal factor.For example, Typhoon Haiyan/Yolanda that struck the Philippines in November 2013, caused massive destruction and had a huge human toll but also generated a spill of around 800,000 litres of heavy oil, when a power barge ran aground in Estancia, Iloilo province, at the height of the typhoon.Disasters may also have longeOÑKL-term impacts. For example, natural disasters may cause long-term waste management or ecosystem damage.The Environmental Emergencies Forum is a unique biennial international forum that brings together disaster managers and environmental experts from governments, UN agencies, industries, academies, NGOs and civil society to improve prevention, preparedness, response and overall resilience to environmental emergencies. It also provides guidance for the Joint UNEP/OCHA Environment Unit, which provides a Secretariat to the meeting. The most recent meeting was held in Norway in June 2015. The next meeting will be held in Nairobi in June 2017.The Joint United Nations Environment Programme (UNEP)/Office for the Coordination of Humanitarian Affairs (OCHA) Environment Unit (JEU): By pairing the UNEP’s technical expertise with OCHA’s humanitarian response network, the Joint UNEP/OCHA Environment Unit (JEU) mobilizes and coordinates a comprehensive response to environmental emergencies to protect lives, livelihoods, ecosystems and future generations. The JEU can be reached 24 hours/day, seven days/week, all year round and operates at the request of affected countries. The JEU can be called by member states when acute environmental risks to life and health as a result of conflicts, natural disasters and industrial accidents are suspected.The JEU hosts the Environmental Emergencies Centre (www.eecentre.org), an online tool designed primarily to provide national responders with a one-stop-shop of all information relevant to the preparedness, prevention and response stages of an environmental emergency. Website: www.unocha/org/unep www.eecentre.orgThe biennial Green Star Awards recognize individuals, organizations, governments and companies who demonstrate achievements in prevention, preparedness and response to environmental emergencies. A joint initiative between Green Cross International, the UN Office for the Coordination of Humanitarian Affairs (OCHA) and the UN Environment Programme (UNEP), the Green Star Awards seeks to increase awareness of environmental emergencies by drawing attention to efforts made to prevent, prepare for and respond to such emergencies.UNISDRWorld Conference on Disaster ReductionNatural disastersDisaster managementRisk managementVulnerabilityEnvironmental disasterUN OCHA Environmental Emergencies SectionEnvironmental Emergencies CentreGreen Star AwardsGlobal Alliance for Disaster ReductionGlobal Disaster Information NetworkAPELLUNEP/GC.22/INF/5;environmental disaster;Environmental emergency;0
An environmental hazard is a substance, a state or an event which has the potential to threaten the surrounding natural environment / or adversely affect people's health, including pollution and natural disasters such as storms and earthquakesAny single or combination of toxic chemical, biological, or physical agents in the environment, resulting from human activities or natural processes, that may impact the health of exposed subjects, including pollutants such as heavy metals, pesticides, biological contaminants, toxic waste, industrial and home chemicals.Human-made hazards while not immediately health-threatening may turn out detrimental to man's well-being eventually, because deterioration in the environment can produce secondary, unwanted negative effects on the human ecosphere. The effects of water pollution may not be immediately visible because of a sewage system that helps drain off toxic substances. If those substances turn out to be persistent (e.g. persistent organic pollutant), however, they will literally be fed back to their producers via the food chain: plankton -> edible fish -> humans. In that respect, a considerable number of environmental hazards listed below are man-made (anthropogenic) hazards.Hazards can be categorized in four types:ChemicalPhysical (mechanical, etc.)BiologicalPsychosocial.Chemical hazards are defined in the Globally Harmonized System and in the European Union chemical regulations. They are caused by chemical substances causing significant damage to the environment. The label is particularly applicable towards substances with aquatic toxicity. An example is zinc oxide, a common paint pigment, which is extremely toxic to aquatic life.Toxicity or other hazards do not imply an environmental hazard, because elimination by sunlight (photolysis), water (hydrolysis) or organisms (biological elimination) neutralizes many reactive or poisonous substances. Persistence towards these elimination mechanisms combined with toxicity gives the substance the ability to do damage in the long term. Also, the lack of immediate human toxicity does not mean the substance is environmentally nonhazardous. For example, tanker truck-sized spills of substances such as milk can cause a lot of damage in the local aquatic ecosystems: the added biological oxygen demand causes rapid eutrophication, leading to anoxic conditions in the water body.All hazards in this category are mainly anthropogenic although there exist a number of natural carcinogens and chemical elements like radon and lead may turn up in health-critical concentrations in the natural environment:A physical hazard is a type of occupational hazard that involves environmental hazards that can cause harm with or without contact.Biological hazards, also known as biohazards, refer to biological substances that pose a threat to the health of living organisms, primarily that of humans. This can include medical waste or samples of a microorganism, virus or toxin (from a biological source) that can affect human health.Psychosocial hazards include but aren't limited to stress, violence and other workplace stressors. Work is generally beneficial to mental health and personal wellbeing. It provides people with structure and purpose and a sense of identity.Banana;environmental disaster;Environmental hazard;0
"The Fukushima disaster cleanup is an ongoing attempt to limit radioactive contamination from the three nuclear reactors involved in the Fukushima Daiichi nuclear disaster which followed the earthquake and tsunami on 11 March 2011.The affected reactors were adjacent to one another and accident management was made much more difficult because of the number of simultaneous hazards concentrated in a small area. Failure of emergency power following the tsunami resulted in loss of coolant from each reactor, hydrogen explosions damaging the reactor buildings, and water draining from open-air spent fuel pools. Plant workers were put in the position of trying to cope simultaneously with core meltdowns at three reactors and exposed fuel pools at three units.Automated cooling systems were installed within 3 months from the accident. A fabric cover was built to protect the buildings from storms and heavy rainfall. New detectors were installed at the plant to track emissions of xenon gas. Filters were installed to reduce contaminants from escaping the area of the plant into the area or atmosphere. Cement has been laid near to the seabed to control contaminants from accidentally entering the ocean.No strontium was released into the area from the accident however, in September 2013 it was reported that the level of strontium-90 detected in a drainage ditch located near a water storage tank from which around 300 tons of water was found to have leaked was believed to have exceeded the threshold set by the government.Decommissioning the plant is estimated to cost tens of billions of dollars and last 30–40 years. Initial fears that contamination of the soil was deep have been reduced with the knowledge that current crops are safe for human consumption and the contamination of the soil was not serious however, in July and August 2013, it was discovered that radioactive groundwater has been leaking into the sea.At the time of the initial event, 50 TEPCO employees remained onsite in the immediate aftermath to work to stabilize the plant and begin cleanup.Initially, TEPCO did not put forward a strategy to regain control of the situation in the reactors. Helmut Hirsch, a German physicist and nuclear expert, said ""they are improvising with tools that were not intended for this type of situation"". However, on 17 April 2011, TEPCO appeared to put forward the broad basis of a plan which included: (1) reaching ""cold shutdown in about six to nine months"" (2) ""restoring stable cooling to the reactors and spent fuel pools in about three months"" (3) putting ""special covers"" on Units 1, 3, and 4 starting in June(4) installing ""additional storage containers for the radioactive water that has been pooling in the turbine basements and outside trenches"" (5) using radio-controlled equipment to clean up the site and (6) using silt fences to limit ocean contamination. Previously, TEPCO publicly committed to installing new emergency generators 20 m above sea level, twice the height of the generators destroyed by the 11 March tsunami. Toshiba and Hitachi had both proposed plans for shuttering the facility.Cold shutdown was accomplished on December 11, 2011. From that point cooling was no longer required, but maintenance was still required to control large water leaks. Long term plans for Units 5 and 6 have not been announced, ""but they too may need to be decommissioned"".On 5 May 2011, workers were able to enter reactor buildings for the first time since the accident. The workers began to install air filtration systems to clean air of radioactive materials to allow additional workers to install water cooling systems.Japanese reactor maker Toshiba said it could decommission the earthquake-damaged Fukushima nuclear power plant in about 10 years, a third quicker than the American Three Mile Island plant. As a comparison, at Three Mile Island the vessel of the partially melted core was first opened 11 years after the accident, with cleanup activities taking several more years.TEPCO announced it restored the automated cooling systems in the damaged reactors in about three months, and had the reactors put into cold shutdown status in six months.First estimates included costs as high as ¥1 trillion (US$13 billion), as cited by the Japanese Prime Minister at the time, Yoshihiko Noda (野田 佳彦). However, this estimate was made before the scope of the problem was known. It seems that the contamination was less than feared. No strontium is detectable in the soil, and though the crops of the year of the disaster were contaminated, the crops produced by the area now are safe for human consumption.Japan's economy, trade, and industry ministry recently (as of  2016) estimated the total cost of dealing with the Fukushima disaster at ¥21.5 trillion (US$187 billion), more than twice the previous estimate of ¥11 trillion (US$96 billion). A rise in compensation for victims of the disaster from ¥5.4 trillion (US$47 billion) to ¥7.9 trillion (US$69 billion) was expected, with decontamination costs estimated to rise from ¥2.5 trillion (US$22 billion) to ¥4 trillion (US$35 billion), costs for interim storage of radioactive material to increase from ¥1.1 trillion (US$10 billion) to ¥1.6 trillion (US$14 billion), and costs of decomissioning reactors to increase from ¥2 trillion (US$17 billion) to ¥8 trillion (US$69 billion).There has been concern that the plant would be dangerous for workers. Two workers suffered skin burns from radiation, but no serious injuries or fatalities have been documented to have been caused by radiation at Fukushima Dai-ichi.The disaster in Fukushima has revealed the practice of Japanese nuclear power plants systematically using unskilled laborers with short contracts. These people are paid per day, and are hired per day from questionable agencies and firms. From data provided by NISA, it was concluded that 80 percent of all of the workforce hired in commercial nuclear power plants is done using temporary contracts, In Fukushima this number was even higher, at 89 percent. This had been practiced for decades. Unemployed people gathered in parks in the morning, and were picked up to be taken to the nuclear power plants. They would get a contract for a few months to do unskilled and the most dangerous labor. After the work was finished, these people were supposed to disappear.Two shelters for people working at the Fukushima-site were not listed as part of the radiation management zones although radiation levels in the shelters exceeded the legal limits. The consequence was, that the workers did not get paid the extra ""danger allowance"" that was paid to workers in these ""radiation management zones"". The shelters were constructed by Toshiba Corporation and the Kajima Corporation at a place some 2 kilometers west of the damaged reactors, just outside the plant compound, but quite near to the reactors 1 to 4. The shelters were built after the shelters at the plant-compound became overcrowded. At 7 October 2011 radiation levels in the Toshiba building were between 2 and 16 microsieverts per hour, in the Kajima dorm it was 2 to 8.5 microsieverts per hour. The Industrial Safety and Health Law on the prevention of health damage through ionizing radiation had set the limit for accumulated radiation dosage in radiation management zones at 1.3 millisieverts over three months, so the maximum level is 2.6 microsieverts/hour. In both dorms the radiation levels were higher. However, these doses are well below the level to affect human health. According to the law, the ""business operator"" is responsible for ""managing radiation dosage and the prevention of contamination"", Toshiba and Kajima said that TEPCO was responsible. But a TEPCO official made the comment: ""From the perspective of protecting workers from radiation, the business operators (that constructed the shelters) are managing radiation dosage and the prevention of contamination"" in this way suggesting that Toshiba and Kajima had to take the care for the zone management.On 26 September 2011, after the discovery of hydrogen in a pipe leading to the containment vessel of reactor no.1, NISA instructed TEPCO to check whether hydrogen was building up in reactor no. 2 and 3 as well. TEPCO announced that measurements of hydrogen would be done in reactor no. 1, before any nitrogen was injected to prevent explosions. When hydrogen would be detected at the other reactors, nitrogen injections would follow.After the discovery of hydrogen concentrations between 61 and 63 percent in pipes of the containment of reactor no. 1, nitrogen injections were started on 8 October. On 10 October TEPCO announced, that the concentrations were at that moment low enough to prevent explosions, and even if the concentration would rise again, it would not exceed 4 percent, the lowest level that would pose the risk of an explosion. On the evening of 9 October two holes were drilled into the pipe to install a filter for radioactive substances inside the containment vessel, this was 2 weeks behind the schedule TEPCO had set for itself. This filter should be in operation as soon as possible.On 19 January 2012 the interior of the primary containment vessel of reactor 2 was inspected with an industrial endoscope. This device, 8.5 millimeters in diameter, is equipped with a 360 degrees-view camera and a thermometer to measure the temperature at this spot and the cooling-water inside, in an attempt to calibrate the existing temperature-measurements that could have an error-margin of 20 degrees. The device was brought in by a hole at 2.5 meter above the floor where the vessel is located. The whole procedure lasted 70 minutes. The photos showed parts of the walls and pipes inside the containment vessel. But they were unclear and blurred, most likely due to water vapors and the radiation inside. According to TEPCO the photos showed no serious damage. The temperature measured inside was 44.7 degrees Celsius, and did not differ much from the 42.6 degrees measured outside the vessel.On 14 March 2012 for the first time after the accidents six workers were sent into the basements of reactor no. 2 and 3, to examine the suppression chambers.  Behind the door of suppression chamber in the no.2 building 160 millisieverts/hour was measured. The door to the suppression chamber in the no. 3 reactor building was damaged and could not be opened. In front of this door the radiation level measurement was 75 millisieverts/hour. For reactors to be decommissioned, access to the suppression chambers is vital for conducting repairs to the containment structures. Because the high levels of radiation, according to TEPCO this work should be done with robots, because these places could be hostile to humans. TEPCO released some video footage of the work at the suppression chambers of the No. 2 and 3 reactors.On 26 and 27 March 2012 the inside of the containment vessel of reactor 2 was inspected with a 20 meter long endoscope. With this a dosi-meter was brought into the vessel to measure the radiation levels inside. At the bottom of the primary containment structure, 60 centimeters of water was found, instead of the 3 meters expected at that place. The radiation level measured was 72.9 sieverts per hour. Because of this, the endoscope could only function a few hours at this place. For reactors number 1 and 3, no endoscopic survey was planned at that time, because the actual radiation levels at these places were too high for humans.Continued cooling of the melted reactor cores is required in order to remove excess heat. Due to damage to the integrity of the reactor vessels, radioactive water accumulated inside the reactor and turbine buildings. To decontaminate the contaminated water, TEPCO installed radioactive water treatment systems.The Japanese government had initially requested the assistance of the Russian floating water decontamination plant Landysh to process the radioactive water from the damaged reactors, but negotiations with the Russian government were a extremely slow process and it is unclear if the plant was ever sent to Fukushima. Landysh was built by Russia with funding from Japan to process liquid wastes produced during the decommissioning of nuclear submarines.As of early September 2011 the operating rate of the filtering system exceeded the target of 90 percent for the first time. 85,000 tons of water were decontaminated by September 11, with over 100,000 tons of waste-water remaining to be treated at the time. However, the nuclear waste generated by the filters had already filled almost 70 percent of the 800 cubic meters of storage space available at the time. TEPCO had to figure out how to cool the reactors with less than 15 tons of water per day in order to reduce the growth of waste-water and nuclear waste to more manageable levels. In order to remove decay heat of the severe damaged cores of Unit 1-3, TEPCO injected cooling water into the reactors. As the reactors appear to have holes around the bottom, the water dissolved the water-soluble fission products, which then accumulated in the basement of the turbine building (the adjacent diagram #2) through any leaks from the water-injected reactor buildings (#1). Since the accumulated radioactive water was a risk, TEPCO tried to transfer it.As the accumulated water in the basement (see the tunnel below diagram #2) of the turbine building of Units 2 and 3 was radioactive, TEPCO needed to remove it. They had initially planned to pump the water to the condenser (the large black vessel in diagram #1). However, TEPCO had to abandon that plan after discovering that the condensers on both units were already full of water. Pumps capable of processing 10–25 tons of water per hour were used to transfer condenser water into other storage tanks, freeing up condenser storage for the water in the basements. However, since both the storage tanks and the condensers were nearly full, TEPCO also considered using floating tankers ships as a temporary storage location for the radioactive water. Regardless of the availability of offshore storage for radioactive-contaminated water, TEPCO decided to discharge 11,500 tons of its least contaminated water (with was still approximately 100 times the legal limit for radioactivity) to the sea on April 5 in order to free up storage space. At the same time, on 5 April, TEPCO began pumping water from the condensers of units 1–3 to their respective condensation storage tanks to free room for the trench water (see below).The Fukushima Daiichi NPS has several seawater piping trenches which were originally designed to house pipes and cables running from the Unit 2–4 turbine buildings to their seaside, which doesn't directly connect to the sea. Inside the trench, radioactive contaminated water has been accumulating since the accident. Due to the risk of soil or ocean contamination from these trenches, TEPCO has been trying to remove the accumulated water in the trenches by pumping it back into the turbine buildings, as well as backfilling the trenches to reduce or prevent further incursion of contaminated water.On 5 July 2013, TEPCO found 9 kBq/L of 134Cs and 18 kBq/L of 137Cs in a sample taken from a monitoring well close to the coastline. Compared with samples taken three days earlier, the levels were 90 times higher. The cause was unknown. The monitoring well is situated close to another monitoring well that had previously leaked radioactive water into the sea in April 2011. A sample of groundwater from another well situated about 100 meters south of the first well showed that the radioactivity had risen by 18 times over the course of 4 days, with 1.7 kBq/L of strontium and other radioactive substances. A day later the readings in the first well were 11 kBq/L of 134Cs and 22 kBq/L of 137Cs, 111 times and 105 times greater than the samples of 5 July. TEPCO did not know the reasons for the higher readings, but the monitoring was to be intensified.More than a month after the groundwater contamination was discovered, TEPCO started to contain the radioactive groundwater. They assumed that the radioactivity had escaped early in the beginning of the disaster in 2011, but NRA experts had serious doubts about their assumption. According to them, other sources could not be excluded. Numerous pipes were running everywhere on the reactor grounds to cool the reactors and decontaminate the water used, and leaks could be anywhere. TEPCO's solution resulted in redirection of the groundwater flows, which could have spread the radioactive contamination further. Besides that, TEPCO had plans for pumping groundwater. At that time the turbine buildings of units 2 and 3 contained 5000 and 6000 cubic meters of radioactive water. With wells in contact with the turbine-buildings, this could spread the radioactivity into the ground. The NRA announced that it would form a task force to find the leaks and to block the flow of the groundwater to the coastline, because the NRA suspected that the groundwater was leaking into the sea.2011On March 27TEPCO announced that radioactive water had accumulated in the basement of the Unit 2 turbine building.On March 28The Japanese Nuclear Safety Commission advised TEPCO to take all possible measures to avoid the accumulated water in the Unit 2 turbine building leaking into the ground and the sea.(hereinafter called ""the JNSC advice"")On April 2TEPCO announced the outflow of fluid containing radioactive materials to the ocean from areas near the intake channel of Unit 2. The fluid source was a 20 cm crack on the concrete lateral of the pit which appeared to have been created by the earthquake. TEPCO attempted to inject fresh concrete, polymeric water absorbent, sawdust, and shredded newspapers into the crack however, this approach failed to slow the leak. After an investigation of the water flow, TEPCO began to inject sodium silicate on April 5th, and the outflow was stopped on April 6th. The total amount and radioactivity of the outflow from the crack was estimated to be approximately 520 m3 and approximately 4.7 PBq respectively.On April 17TEPCO announced the Roadmap towards Restoration from the Accident at Fukushima Daiichi Nuclear Power Station.On April 27In order to prevent the outflow of the highly radioactive water at the turbine building of Unit 2, the water was transferred to the Centralized Radiation Waste Treatment Facility since April 19th. TEPCO planned to install facilities for processing the stored water and reusing treated water to inject it into the reactors.On May 11TEPCO investigated possible leakage of radioactive water to the outside from around the intake canal of Unit 3 in response to the employees' report of water flowing into the pit via power cable pipe lines.On May 23Nuclear and Industrial Safety Agency began to use the term ""Contaminated Water"" as the water with high concentration of radioactive materials.On June 17TEPCO began the operation of the cesium adsorption apparatus (Kurion) and the decontamination apparatus (AREVA).On August 17TEPCO began the (test) operation of SARRY, which is the second cesium adsorption apparatus (TOSHIBA).On August 282 TEPCO workers at the plant were exposed to radiation by mistake while they were replacing parts of the contaminated water processing system. The next Wednesday 31 August two other workers were sprayed with highly contaminated water when the water splashed from a container with a leaking valve that did not close. It was found that they were exposed to 0.16 and .14 millisieverts. The last man wore a raincoat. No immediate symptoms were found.On December 21TEPCO announced Mid-and-long-Term Roadmap towards the Decommissioning of Fukushima Daiichi Nuclear Power Units 1-4.2012On April 5A leaking pipe was found at 1.00 AM. The leakage stopped an hour after the valves were closed. 12,000 liters water with high levels of radioactive strontium were lost, according to TEPCO much of this water escaped through a nearby sewer-system into the ocean. Investigations should reveal how much water was lost into the ocean, and how the joint could fail. A similar leakage in at the same facility happened on 26 March 2012.On September 19Nuclear Regulation Authority (NRA) was established.2013 (The year to the social problem)On March 30TEPCO began the operation of ALPS, which is the multi-nuclide removal equipment.On July 22With announcing the situation on seawater and groundwater, TEPCO admitted that contaminated groundwater had been leaking into the ocean since March 2011.On July 27TEPCO announced that extremely high levels of tritium and cesium were found in a pit containing about 5000 cubic meters water on the sea side of the Unit 2 reactor building. 8.7 MBq/liter of tritium and 2.35 GBq/liter of cesium was measured. The NRA was concerned that leaks from the pit could release high tritium levels into the sea and that there was still water flowing from the reactor into the turbine building and into the pit. However, TEPCO believed that this pollution was there from the first days in 2011, and had stayed there. Nevertheless, TEPCO would control the site for leaks, and seal the soil around the pit.On May 30The Government of Japan decided the policy to prevent the groundwater flowing in the reactor buildings. A frozen soil wall (Land-side Impermeable Wall) was scheduled for introduction to block the flow of groundwater and prevent its mixing with contaminated water.On August 19Contaminated water leakage from a flange type tank was found in the H4 area. The incident was finally evaluated by the NRA as a provisional rating Level 3 on the eight-level INES. In response to this incident, NRA recommended that TEPCO should replace the flange type tank, which is prone to leak water, with a welded type tank.On August 28A subcontractor employee was contaminated on his face, head and chest while transferring water from the damaged tank. After decontamination, 5,000 cpm were still measured on his head the readings from prior to decontamination were not released. The man was released, but ordered to have a whole-body radiation count later.On September 2It was reported that radiation near another tank was measured at 1.8 Sv/h, 18 times higher than previously thought.  TEPCO had initially recorded radiation at about 100 mSv/h, but later admitted that that was because the equipment they were using could only read measurements up to that level. The latest reading came from a more advanced device capable of measuring higher levels.  The buildup of water at the site is close to becoming unmanageable and experts say that TEPCO will soon be left with no choice but to release the water into the ocean or evaporate it.On September 3The Nuclear Emergency Response Headquarters published ""the Government’s Decision on Addressing the Contaminated Water Issue at TEPCO’s Fukushima Daiichi NPS"".On September 9TEPCO started cleaning the draining ditch at the north side of the leaking tank one day before Tokyo was selected as host of the 2020 Olympic Games. Radiation monitoring data were masked after that day for some time.On September 12Contaminated water leakage from storage tanks was found in the H4 area.Cooling the reactors with recirculated and decontaminated water from the basements proved to be a success, but as a consequence, this radioactive waste was piling up in the temporary storage facility at the plant. TEPCO decided in the first week of October to use the ""Sally"" decontamination system built by Toshiba Corporation and keep the Kurion/Areva system as back-up.On 27 September after three months operation some 4,700 drums with radioactive waste had piled up at the plant. The Kurion and Sally systems both utilized zeolites to concentrate cesium. After the zeolite was saturated, the vessels with the zeolite were turned into nuclear waste. By now, 210 Kurion-made vessels with a total of 307 cubic meters, each vessel measuring 0.9 meters in diameter and 2.3 meters in height had accumulated at the plant. The Areva-filters used sand to absorb radioactive materials and chemicals were used to reactivate the filters. In this way, 581 cubic meters of highly contaminated sludge were produced.According to Professor Akio Koyama of the Kyoto University Research Reactor Institute, the density of high-level decontaminated water was believed to contain 10 gigabecquerel per liter, but if this is condensed to polluted sludge and zeolites, this density could increase 10,000 fold. These densities could not be dealt using conventional systems.On August 16, 2011, TEPCO announced the installation of desalination equipment in the spent fuel pools of reactor 2, 3 and 4. These pools had been cooled with seawater for some time, and TEPCO feared the salt would corrode the stainless steel pipes and pool wall liners. The Unit 4 spent fuel pool was the first to have the equipment installed, the spent fuel pools of reactor 2 and 3 came next. TEPCO expected to achieve removal of 96% of the salt in the spent fuel pools within two months.On December 22, 2014, TEPCO crews completed the removal of all fuel assemblies from the spent fuel pool of reactor 4. 1331 spent fuel assemblies were moved to the ground-level common spent fuel pool, and 204 unused fuel assemblies were moved to the spent fuel pool of reactor 6 (Unit 4 was out of service for refueling at the time of the 2011 accident, so the spent fuel pool contained a number of unused new fuel assemblies).On 10 April 2011, TEPCO began using remote-controlled, unmanned heavy equipment to remove debris from around reactors 1–4. The debris and rubble, caused by hydrogen explosions at reactors 1 and 3, was impeding recovery operations both by being in the way and emitting high radioactivity. The debris will be placed into containers and kept at the plant.Because the monsoon season begins in June in Japan, it became urgent to protect the damaged reactor buildings from storms, typhoons and heavy rainfall. As a short term solution, TEPCO envisaged to apply a light cover on the remaining structures above the damaged reactors.  As of mid-June, TEPCO released its plan to use automated cranes to move structures into place over the reactor.  This strategy is an attempt to keep as many people away from the reactors as possible, while still covering the damaged reactors.On 18 March, Reuters reported that Hidehiko Nishiyama, Japan's nuclear agency spokesman when asked about burying the reactors in sand and concrete, said: ""That solution is in the back of our minds, but we are focused on cooling the reactors down."" Considered a last-ditch effort since it would not provide cooling, such a plan would require massive reinforcement under the floor, as for the Chernobyl Nuclear Power Plant sarcophagus.On 7 September 2011, TEPCO president Toshio Nishizawa said that the 4 damaged reactors will be scrapped. This announcement came at a session of the Fukushima Prefectural Assembly, which was investigating the accident at the plant. Whether the six other remaining reactors, (Daiichi 5, 6, Daini 1, 2, 3, 4) should be abolished too, would be decided based on the opinions of local municipalities.On 28 October 2011, the Japanese Atomic Energy Commission presented a timetable in a draft report titled, “how to scrap the Fukushima reactors”. Within 10 years, a start should be made with the retrieval of the melted fuel within the reactors. First, the containment vessels of reactors 1, 2 and 3 should be repaired, then all should be filled with water to prevent radiation releases. Decommissioning would take more than 30 years, because the pressure vessels of the reactor vessels are damaged also. After the accident at Three Mile Island in 1979, some 70 percent of the fuel rods had melted. There, the retrieval of the fuel was started in 1985, and completed in 1990. The work at Fukushima was expected to take significantly longer because of the far greater damage and the fact that 4 reactors would need to be decommissioned all at the same time.After discussions were started in August 2011, on 9 November, a panel of experts of Japan's Atomic Energy Commission completed a schedule for scrapping the damaged reactors - their conclusions wereThe scrapping will take 30 years or longer.First, the containment vessels needed to be repaired, then filled with water to block radiation.The reactors should be in a state of stable cold shutdown.Three years later, a start would be made to take all spent fuel from the 4 damaged reactors to a pool within the compound.Within 10 years, the removal of the melted fuel inside the reactors could begin.This scheme was partly based on the experience gained from the 1979 Three Mile Island accident. However, in Fukushima with three meltdowns at one site, the damage was much more extensive. It could take 30 years or more to remove the nuclear fuel, dismantle the reactors, and remove all the buildings.Research institutions all over the world were asked to participate in the construction of a research-site to examine the removal of fuel and other nuclear wastes. The official publication of the report was planned at the end of 2011.Since the disaster, TEPCO has installed sensors, a fabric cover over the reactors and additional filters to reduce the emission of contaminants.After the detection of radioactive xenon gas in the containment vessel of the No. 2 reactor on 1 and 2 November 2011 TEPCO was not able to determine whether this was a sustained fission process or only spontaneous fission. Therefore TEPCO installed detection devices for radioactive xenon to single out any occurrence of nuclear criticality. Next to this TEPCO installed temperature sensors to control temperature changes in the reactors, another indicator of possible critical fission reactions.On 20 September the Japanese government and TEPCO announced the installation of new filters to reduce the amount of radioactive substances released into the air. In the last week of September 2011 these filters were to be installed at reactor 1, 2 and 3. Gases out of the reactors would be decontaminated before they would be released into the air. Mid October the construction of the polyester shield over the No.1 reactor should be completed. In the first half of September the amount of radioactive substances released from the plant was about 200 megabecquerel per hour, according to TEPCO, that was about one-four millionths of the level of the initial stages of the accident in March.An effort has been undertaken to fit the three damaged reactor buildings with fabric covers and filters to limit radioactive contamination release. On 6 April 2011, sources told Kyodo News that a major construction firm was studying the idea, and that construction wouldn't ""start until June"". The plan has been criticized for potential only having ""limited effects in blocking the release of radioactive substances into the environment"". On 14 May, TEPCO announced that it had begun to clear debris to create a space to install a cover over the building of reactor 1. In June, a large crane was erected near Reactor 1 to begin construction of the fabric cover. From mid August to mid September 2011, a rectangular steel frame entirely surrounding the reactor building was constructed. Starting 9 September, the crane was used to attach polyester panels to the frame. On 20 September 2011, TEPCO announced that within three weeks they hoped to complete the construction of the polyester shield over the No.1 reactor. By that time the steel frame for the fabric cover had been completed. By 7 October, the roof of the structure was being added. On 9 October, the walls of the cover appeared to be placed, and by 13 October the roof had been completed.In June 2016, preparation work began to install a metal cover over the Unit 3 reactor building. In conjunction with this, a crane is to be installed to assist with the removal of the fuel rods from the storage pool. After inspection and cleaning, the removed fuel is expected to be stored in the site's communal storage facility. By February 2018 the dome-shaped roof had been completed in preparation of the removal of the fuel rods.Significant efforts are being taken to clean up radioactive material that escaped the plant. This effort combines washing down buildings and scraping away topsoil. It has been hampered by the volume of material to be removed and the lack of adequate storage facilities.There is also a concern that washing surfaces will merely move the radioactive material without eliminating it.After an earlier decontamination-plan only to clean all areas with radiation levels above 5 millisievert per year, had raised protests, the Japanese government revealed, on 10 October 2011, in a meeting with experts, a revised decontamination plan. This plan included:all areas with radiation levels above 1 millisievert per year would be cleaned.no-entry zones and evacuation zones designated by the government would be the responsibility of the government.the rest of the areas would be cleaned by local authorities.in areas with radiation levels above 20 millisievert per year, decontamination would be done step by step.within two years, radiation levels between 5 and 20 millisieverts should be cut down to 60%.the Japanese government would help local authorities with disposing the enormous amount of radioactive waste.On 19 December 2011 the Japanese Ministry of Environment published more details about these plans for decontamination: the work would be subsidized in 102 villages and towns. Opposition against the plan came from cattle-farmers in the prefecture Iwate and the tourist-industry in the city of Aizuwakamatsu, because of fears that cattle sales might drop or tourism would be hurt to the town, when the areas would be labeled to be contaminated. Areas with lower readings complained that their decontamination would not be funded.In a Reuters story from August 2013, it was noted ""[m]any have given up hope of ever returning to live in the shadow of the Fukushima nuclear plant. A survey in June showed that a third of the former residents of Iitate, a lush village famed for its fresh produce before the disaster, never want to move back. Half of those said they would prefer to be compensated enough to move elsewhere in Japan to farm."" In addition, despite being allowed to return home, some residents say the lack of an economy continues to make the area de facto unlivable. Compensation payments to those who have been evacuated are stopped when they are allowed to return home, however, as of August 2013 decontamination of the area has progressed more slowly than expected. There have also been revelations of additional leaks (see above: storage tanks leaking contaminated water).On 22 February 2012 TEPCO started cementing the seabed near the plant to prevent the spread of radioactive materials into the sea. Some 70000 square meters of seabed around the intake of cooling water would be covered with 60 centimeters thick cement. The work should be finished within 4 months time, and prevent the spread of contaminated mud and sand at that place for at least 50 years.On 18 December 2011 Fukushima Gov. Yuhei Sato and representatives of 11 other municipal governments near the plant were notified at a meeting at the city of Fukushima the three ministers in charge of handling the crises, Yokio Edano, minister of Economy, Trade and Industry, Goshi Hosono, nuclear disaster minister, and Tatsuo Hirano, minister in charge of reconstruction of the government plan to redesign the classification of the no-entry-zones around the Fukushima nuclear plant. From 1 April 2012 a three level system would be introduced, by the Japanese government:a) no-entry zones, with an annual radiation exposure of 50 millisieverts or moreat these places habitation would be prohibitedb) zones with annual radiation exposures between 20-50 millisievert, here former residents could return, but with restrictions.c) zones with exposures of less than 20 millisievert per yearin these zones the residents would be allowed to return to their houses.Decontamination efforts were planned in line with this newly designed order, to help the people to return to places where the radiation levels would be relatively low.Mid December 2011 the local authorities in Fukushima had spent already around 1.7 billion yen (21 million$) on the costs of decontamination-works in the cities of Fukushima and Date and the village of Kawauchi. The total clean-up costs were estimated around 420 billion yen (~ 5.2 billion$). For the clean-up only 184.3 billion yen was reserved in the September supplementary budget of prefecture Fukushima, and some funds in the central government's third supplementary budget of 2011. Whenever needed the central government would be asked for extra funding.In 2016, University of Oxford researcher and author Peter Wynn Kirby wrote that the government had allocated the equivalent of US$15 billion for the regional cleanup and described the josen (decontamination) process, with ""provisional storage areas (kari-kari-okiba) ... [and] more secure, though still temporary, storage depots (kari-okiba)"". Kirby opined the effort still would be better called ""transcontamination"" because it was moving the contaminated material around without long-term safe storage planned or executed. He also saw little progress on handling the more intense radiation waste of the destroyed power-plant site itself or on handling the larger issue of the national nuclear program's waste, particularly given the earthquake-risk of Japan relative to secure long-term storage.The Fukushima Daiichi nuclear disaster revealed the dangers of building multiple nuclear reactor units close to one another. This proximity triggered the parallel, chain-reaction accidents that led to hydrogen explosions blowing the roofs off reactor buildings and water evaporating from open-air spent fuel pools—a situation that was potentially more dangerous than the loss of reactor cooling itself. Because of the proximity of the reactors, Plant Director Masao Yoshida ""was put in the position of trying to cope simultaneously with core meltdowns at three reactors and exposed fuel pools at three units"".Tritiated waterHuman decontamination電気新聞, ed. (2011). 東日本大震災の記録 - 原子力事故と計画停電 -. (社)日本電気協会新聞部. Management of contaminated waterThe Committee on countermeasures for contaminated water treatment (2013), Preventative and Multilayered Measures for Contaminated Water Treatment at the Fukushima Daiichi Nuclear Power Station of Tokyo Electric Power Company - Through completeness of comprehensive risk management - (PDF) Tritiated Water Task Force (2016), Tritiated Water Task Force Report (PDF) METI (2016), Important Stories on Decommissioning-Fukushima Daiichi Nuclear Power Station, now and in the future (PDF) Answers to Frequently Asked Questions About Cleanup Activities at Three Mile Island, Unit 2, NUREG, 1984 空本 誠喜 (2014). 汚染水との闘い −福島第一原発・危機の深層−. ちくま新書. 筑摩書房. PM Information on contaminated water leakage at TEPCO's Fukushima Daiichi Nuclear Power Station, Prime Minister of Japan and His CabinetMOFA Information on contaminated water leakage at TEPCO’s Fukushima Daiichi Nuclear Power Station, Ministry of Foreign AffairsTEPCO News Releases, Tokyo Electric Power CompanyNRA, Japan,  Nuclear Regulation AuthorityNISA, Nuclear and Industrial Safety Agency, former organizationFukushima Diary News site of a concerned Japanese man in EuropeDecommissioning plan of Fukushima Daiichi Nuclear Power StationMid-and-Long-Term Roadmap towards the Decommissioning of TEPCO's Fukushima Daiichi Nuclear Power Station Units 1-4";environmental disaster;Fukushima disaster cleanup;0
"Global warming, also referred to as climate change, is the observed century-scale rise in the average temperature of the Earth's climate system and its related effects. Multiple lines of scientific evidence show that the climate system is warming. Many of the observed changes since the 1950s are unprecedented in the instrumental temperature record, which extends back to the mid-19th century, and in paleoclimate proxy records of climate change over thousands of years.In 2013, the Intergovernmental Panel on Climate Change (IPCC) Fifth Assessment Report concluded, ""It is extremely likely that human influence has been the dominant cause of the observed warming since the mid-20th century."" The largest human influence has been the emission of greenhouse gases such as carbon dioxide, methane, and nitrous oxide. Climate model projections summarized in the report indicated that during the 21st century, the global surface temperature is likely to rise a further 0.3 to 1.7 °C (0.5 to 3.1 °F) in the lowest emissions scenario, and 2.6 to 4.8 °C (4.7 to 8.6 °F) in the highest emissions scenario. These findings have been recognized by the national science academies of the major industrialized nations and are not disputed by any scientific body of national or international standing.Future climate change and associated impacts will differ from region to region. Anticipated effects include increasing global temperatures, rising sea levels, changing precipitation, and expansion of deserts in the subtropics. Warming is expected to be greater over land than over the oceans and greatest in the Arctic, with the continuing retreat of glaciers, permafrost, and sea ice. Other likely changes include more frequent extreme weather events such as heat waves, droughts, heavy rainfall with floods, and heavy snowfall ocean acidification and species extinctions due to shifting temperature regimes. Effects significant to humans include the threat to food security from decreasing crop yields and the abandonment of populated areas due to rising sea levels. Because the climate system has a large ""inertia"" and greenhouse gases will remain in the atmosphere for a long time, many of these effects will persist for not only decades or centuries, but for tens of thousands of years to come.Possible societal responses to global warming include mitigation by emissions reduction, adaptation to its effects, building systems resilient to its effects, and possible future climate engineering. Most countries are parties to the United Nations Framework Convention on Climate Change (UNFCCC), whose ultimate objective is to prevent dangerous anthropogenic climate change. Parties to the UNFCCC have agreed that deep cuts in emissions are required and that global warming should be limited to well below 2.0 °C (3.6 °F) compared to pre-industrial levels, with efforts made to limit warming to 1.5 °C (2.7 °F).Public reactions to global warming and concern about its effects are also increasing. A global 2015 Pew Research Center report showed that a median of 54% of all respondents asked consider it ""a very serious problem"". Significant regional differences exist, with Americans and Chinese (whose economies are responsible for the greatest annual CO2 emissions) among the least concerned.In the period from 1880 to 2012, the global average (land and ocean) surface temperature has increased by 0.85 [0.65 to 1.06] °C, multiple independently produced datasets confirm. In the period from 1906 to 2005, Earth's average surface temperature rose by 0.74±0.18 °C. The rate of warming almost doubled in the last half of that period (0.13±0.03 °C per decade, against 0.07±0.02 °C per decade). Although the popular press often reportsthe increase of the average near-surface atmospheric temperature as the measure of global warming, most of the additional energy stored in the climate system since 1970 has accumulated in the oceans. The rest has melted ice and warmed the continents and the atmosphere.Since 1979, the average temperature of the lower troposphere has increased between 0.12 and 0.135 °C (0.216 and 0.243 °F) per decade, satellite temperature measurements confirm. Climate proxies show the temperature to have been relatively stable over the one or two thousand years before 1850, with regionally varying fluctuations such as the Medieval Warm Period and the Little Ice Age.The warming evident in the instrumental temperature record is consistent with a wide range of observations, as documented by many independent scientific groups. Examples include sea level rise, widespread melting of snow and land ice, increased heat content of the oceans, increased humidity, and the earlier timing of spring events, e.g., the flowering of plants. The probability that these changes could have occurred by chance is virtually zero.Global warming refers to global averages. Because it is not a uniform phenomenon, effects can vary by region. Since 1979, global average land temperatures have increased about twice as fast as global average ocean temperatures (0.25 °C per decade against 0.13 °C per decade). Ocean temperatures increase more slowly than land temperatures because of the larger effective heat capacity of the oceans and because oceans lose more heat by evaporation. Since the beginning of industrialisation in the eighteenth century, the temperature difference between the hemispheres has increased due to feedbacks from melting of sea ice and snow in the North, and because there is more land in the Northern Hemisphere.In the past one hundred years, average arctic temperatures have been increasing at almost twice the rate of the rest of the world. At least one region — the southeastern part of the United States — has experienced cooler than normal temperatures.Although more greenhouse gases are emitted in the Northern than in the Southern Hemisphere, this fact does not contribute to the difference in warming because the major greenhouse gases persist long enough to diffuse within as well as between the hemispheres.There are different ways in which a climate can be forced to change, but because the climate system has large thermal inertia, it can take centuries — or even longer — for the climate to fully adjust. One climate commitment study concluded that if greenhouse gases were stabilized at year 2000 levels, surface temperatures would still increase by about one-half degree Celsius, and another found that if they were stabilized at 2005 levels, surface warming could exceed a whole degree Celsius. Some of this surface warming will be driven by past natural forcings which are still seeking equilibrium in the climate system. One study using a highly simplified climate model indicates these past natural forcings may account for as much as 64% of the committed 2050 surface warming and their influence will fade with time compared to the human contribution.Global temperature is subject to short-term fluctuations that overlay long-term trends and can temporarily mask them. The relative stability in surface temperature from 2002 to 2009, which has since been dubbed the global warming hiatus by the media and some scientists, is an example of such an episode. 2015 updates to account for differing methods of measuring ocean surface temperature measurements show a positive trend over the recent decade.Sixteen of the seventeen warmest years on record have occurred since 2000. While record-breaking years attract considerable public interest, individual years are less significant than the overall trend. Some climatologists have criticized the attention that the popular press gives to ""warmest year"" statistics. In particular, ocean oscillations such as the El Niño Southern Oscillation (ENSO) can cause temperatures of a given year to be abnormally warm or cold for reasons unrelated to the overall trend of climate change. Gavin Schmidt stated: ""the long-term trends or the expected sequence of records are far more important than whether any single year is a record or not.""By itself, the climate system may generate random changes in global temperatures for years to decades at a time, but long-term changes emanate only from so-called external forcings. These forcings are ""external"" to the climate system, but not necessarily external to Earth. Examples of external forcings include changes in the composition of the atmosphere (e.g., increased concentrations of greenhouse gases), solar luminosity, volcanic eruptions, and variations in Earth's orbit around the Sun.The greenhouse effect is the process by which absorption and emission of infrared radiation by gases in a planet's atmosphere warm its lower atmosphere and surface. It was proposed by Joseph Fourier in 1824, discovered in 1860 by John Tyndall, was first investigated quantitatively by Svante Arrhenius in 1896, and its scientific description was developed in the 1930s through 1960s by Guy Stewart Callendar.On Earth, an atmosphere containing naturally occurring amounts of greenhouse gases causes air temperature near the surface to be warmer by about 33 °C (59 °F) than it would be in their absence. Without the Earth's atmosphere, the Earth's average temperature would be well below the freezing temperature of water. The major greenhouse gases are water vapour, which causes about 36–70% of the greenhouse effect carbon dioxide (CO2), which causes 9–26% methane (CH4), which causes 4–9% and ozone (O3), which causes 3–7%. Clouds also affect the radiation balance through cloud forcings similar to greenhouse gases.Human activity since the Industrial Revolution has increased the amount of greenhouse gases in the atmosphere, leading to increased radiative forcing from CO2, methane, tropospheric ozone, CFCs, and nitrous oxide. According to work published in 2007, the concentrations of CO2 and methane had increased by 36% and 148% respectively since 1750. These levels are much higher than at any time during the last 800,000 years, the period for which reliable data has been extracted from ice cores. Less direct geological evidence indicates that CO2 values higher than this were last seen about 20 million years ago.Fossil fuel burning has produced about three-quarters of the increase in CO2 from human activity over the past 20 years. The rest of this increase is caused mostly by changes in land-use, particularly deforestation. Another significant non-fuel source of anthropogenic CO2 emissions is the calcination of limestone for clinker production, a chemical process which releases CO2. Estimates of global CO2 emissions in 2011 from fossil fuel combustion, including cement production and gas flaring, was 34.8 billion tonnes (9.5 ± 0.5 PgC), an increase of 54% above emissions in 1990. Coal burning was responsible for 43% of the total emissions, oil 34%, gas 18%, cement 4.9% and gas flaring 0.7%.In May 2013, it was reported that readings for CO2 taken at the world's primary benchmark site in Mauna Loa surpassed 400 ppm. According to professor Brian Hoskins, this is likely the first time CO2 levels have been this high for about 4.5 million years. Monthly global CO2 concentrations exceeded 400 ppm in March 2015, probably for the first time in several million years. On 12 November 2015, NASA scientists reported that human-made carbon dioxide continues to increase above levels not seen in hundreds of thousands of years currently, about half of the carbon dioxide released from the burning of fossil fuels is not absorbed by vegetation and the oceans and remains in the atmosphere.Over the last three decades of the twentieth century, gross domestic product per capita and population growth were the main drivers of increases in greenhouse gas emissions. CO2 emissions are continuing to rise due to the burning of fossil fuels and land-use change. Emissions can be attributed to different regions. Attributions of emissions due to land-use change are subject to considerable uncertainty.Emissions scenarios, estimates of changes in future emission levels of greenhouse gases, have been projected that depend upon uncertain economic, sociological, technological, and natural developments. In most scenarios, emissions continue to rise over the century, while in a few, emissions are reduced. Fossil fuel reserves are abundant, and will not limit carbon emissions in the 21st century.Emission scenarios, combined with modelling of the carbon cycle, have been used to produce estimates of how atmospheric concentrations of greenhouse gases might change in the future. Using the six IPCC SRES ""marker"" scenarios, models suggest that by the year 2100, the atmospheric concentration of CO2 could range between 541 and 970 ppm. This is 90–250% above the concentration in the year 1750.The popular media and the public often confuse global warming with ozone depletion, i.e., the destruction of stratospheric ozone (e.g., the ozone layer) by chlorofluorocarbons. Although there are a few areas of linkage, the relationship between the two is not strong. Reduced stratospheric ozone has had a slight cooling influence on surface temperatures, while increased tropospheric ozone has had a somewhat larger warming effect.Global dimming, a gradual reduction in the amount of global direct irradiance at the Earth's surface, was observed from 1961 until at least 1990. Solid and liquid particles known as aerosols, produced by volcanoes and human-made pollutants, are thought to be the main cause of this dimming. They exert a cooling effect by increasing the reflection of incoming sunlight. The effects of the products of fossil fuel combustion – CO2 and aerosols – have partially offset one another in recent decades, so that net warming has been due to the increase in non-CO2 greenhouse gases such as methane. Radiative forcing due to aerosols is temporally limited due to the processes that remove aerosols from the atmosphere. Removal by clouds and precipitation gives tropospheric aerosols an atmospheric lifetime of only about a week, while stratospheric aerosols can remain for a few years. Carbon dioxide has a lifetime of a century or more, and as such, changes in aerosols will only delay climate changes due to carbon dioxide. Black carbon is second only to carbon dioxide for its contribution to global warming (contribution being estimated at 17 to 20%, whereas carbon dioxide contributes 40 to 45% to global warming).In addition to their direct effect by scattering and absorbing solar radiation, aerosols have indirect effects on the Earth's radiation budget. Sulfate aerosols act as cloud condensation nuclei and thus lead to clouds that have more and smaller cloud droplets. These clouds reflect solar radiation more efficiently than clouds with fewer and larger droplets, a phenomenon known as the Twomey effect. This effect also causes droplets to be of more uniform size, which reduces growth of raindrops and makes the cloud more reflective to incoming sunlight, known as the Albrecht effect. Indirect effects are most noticeable in marine stratiform clouds, and have very little radiative effect on convective clouds. Indirect effects of aerosols represent the largest uncertainty in radiative forcing.Soot may either cool or warm Earth's climate system, depending on whether it is airborne or deposited. Atmospheric soot directly absorbs solar radiation, which heats the atmosphere and cools the surface. In isolated areas with high soot production, such as rural India, as much as 50% of surface warming due to greenhouse gases may be masked by atmospheric brown clouds. When deposited, especially on glaciers or on ice in arctic regions, the lower surface albedo can also directly heat the surface. The influences of atmospheric particles, including black carbon, are most pronounced in the tropics and sub-tropics, particularly in Asia, while the effects of greenhouse gases are dominant in the extratropics and southern hemisphere.Since 1978, solar irradiance has been measured by satellites. These measurements indicate that the Sun's radiative output has not increased since then, so the warming that occurred in the past 40 years cannot be attributed to an increase in solar energy reaching the Earth.Climate models have been used to examine the role of the Sun in recent climate change. Models are unable to reproduce the rapid warming observed in recent decades when only taking into account variations in solar output and volcanic activity. Models are, however, able to simulate the observed 20th century changes in temperature when they include all of the most important external forcings, consisting of both human influences and natural forcings.Another line of evidence for the Sun's non-attributability is the differing temperature changes at different levels in the Earth's atmosphere. According to basic physical principles, the greenhouse effect produces warming of the lower atmosphere (the troposphere), but cooling of the upper atmosphere (the stratosphere). If solar variations were responsible for the observed warming, warming of both the troposphere and the stratosphere would be expected.The tilt of the Earth’s axis and the shape of its orbit around the Sun vary slowly over tens of thousands of years. This changes climate by changing the seasonal and latitudinal distribution of incoming solar energy at Earth's surface.During the last few thousand years, this phenomenon contributed to a slow cooling trend at high latitudes of the Northern Hemisphere during summer, a trend that was reversed by greenhouse-gas-induced warming during the 20th century. Orbital cycles favorable for glaciation are not expected within the next 50,000 years.The climate system includes a range of feedbacks, which alter the response of the system to changes in external forcings. Positive feedbacks increase the response of the climate system to an initial forcing, while negative feedbacks reduce it.There are a range of feedbacks in the climate system, including water vapour, changes in ice-albedo (snow and ice cover affect how much the Earth's surface absorbs or reflects incoming sunlight), clouds, and changes in the Earth's carbon cycle (e.g., the release of carbon from soil). The main negative feedback is the energy the Earth's surface radiates into space as infrared radiation. According to the Stefan-Boltzmann law, if the absolute temperature (as measured in kelvins) doubles, radiated energy increases by a factor of 16 (2 to the 4th power).Feedbacks are an important factor in determining the sensitivity of the climate system to increased atmospheric greenhouse gas concentrations. Other factors being equal, a higher climate sensitivity means that more warming will occur for a given increase in greenhouse gas forcing. Uncertainty over the effect of feedbacks is a major reason why different climate models project different magnitudes of warming for a given forcing scenario. More research is needed to understand the role of clouds and carbon cycle feedbacks in climate projections.The IPCC projections previously mentioned span the ""likely"" range (greater than 66% probability, based on expert judgement) for the selected emissions scenarios. However, the IPCC's projections do not reflect the full range of uncertainty. The lower end of the ""likely"" range appears to be better constrained than the upper end.An observation based study on future climate change, on the soil carbon feedback, conducted since 1991 in Harvard, suggests release of about 190 petagrams of soil carbon, the equivalent of the past two decades of greenhouse gas emissions from fossil fuel burning, until 2100 from the top 1-meter of Earth's soils, due to changes in microbial communities under elevated temperatures. Climate models do not account for this possible feedback mechanism.A climate model is a representation of the physical, chemical and biological processes that affect the climate system. Such models are based on scientific disciplines such as fluid dynamics and thermodynamics as well as physical processes such as radiative transfer. The models may be used to predict a range of variables such as local air movement, temperature, clouds, and other atmospheric properties ocean temperature, salt content, and circulation ice cover on land and sea the transfer of heat and moisture from soil and vegetation to the atmosphere and chemical and biological processes, among others.Although researchers attempt to include as many processes as possible, simplifications of the actual climate system are inevitable because of the constraints of available computer power and limitations in knowledge of the climate system. Results from models can also vary due to different greenhouse gas inputs and the model's climate sensitivity. For example, the uncertainty in IPCC's 2007 projections is caused by (1) the use of multiple models with differing sensitivity to greenhouse gas concentrations, (2) the use of differing estimates of humanity's future greenhouse gas emissions, (3) any additional emissions from climate feedbacks that were not included in the models IPCC used to prepare its report, i.e., greenhouse gas releases from permafrost.The models do not assume the climate will warm due to increasing levels of greenhouse gases. Instead the models predict how greenhouse gases will interact with radiative transfer and other physical processes. Warming or cooling is thus a result, not an assumption, of the models.Clouds and their effects are especially difficult to predict. Improving the models' representation of clouds is therefore an important topic in current research. Another prominent research topic is expanding and improving representations of the carbon cycle.Models are also used to help investigate the causes of recent climate change by comparing the observed changes to those that the models project from various natural and human causes. Although these models do not unambiguously attribute the warming that occurred from approximately 1910 to 1945 to either natural variation or human effects, they do indicate that the warming since 1970 is dominated by anthropogenic greenhouse gas emissions.The physical realism of models is tested by examining their ability to simulate contemporary or past climates. Climate models produce a good match to observations of global temperature changes over the last century, but do not simulate all aspects of climate. Not all effects of global warming are accurately predicted by the climate models used by the IPCC. Observed Arctic shrinkage has been faster than that predicted. Precipitation increased proportionally to atmospheric humidity, and hence significantly faster than global climate models predict. Since 1990, sea level has also risen considerably faster than models predicted it would.The environmental effects of global warming are broad and far reaching. They include the following diverse effects:Arctic sea ice decline, sea level rise, retreat of glaciers: Global warming has led to decades of shrinking and thinning in a warm climate that has put the Arctic sea ice in a precarious position, it is now vulnerable to atmospheric anomalies. Projections of declines in Arctic sea ice vary. Recent projections suggest that Arctic summers could be ice-free (defined as ice extent less than 1 million square km) as early as 2025–2030. The sea level rise since 1993 has been estimated to have been on average 2.6 mm and 2.9 mm per year ± 0.4 mm. Additionally, sea level rise has accelerated from 1995 to 2015. Over the 21st century, the IPCC projects for a high emissions scenario, that global mean sea level could rise by 52–98 cm.Extreme weather, extreme events, tropical cyclones: Data analysis of extreme events from 1960 until 2010 suggests that droughts and heat waves appear simultaneously with increased frequency. Extremely wet or dry events within the monsoon period have increased since 1980. Projections suggest a probable increase in the frequency and severity of some extreme weather events, such as heat waves.Ecosystem changes, changes in ocean properties: In terrestrial ecosystems, the earlier timing of spring events, as well as poleward and upward shifts in plant and animal ranges, have been linked with high confidence to recent warming. It is expected that most ecosystems will be affected by higher atmospheric CO2 levels, combined with higher global temperatures. Overall, it is expected that climate change will result in the extinction of many species and reduced diversity of ecosystems. The physical effect of global warming on oceans include an increase in acidity, and a reduction of oxygen levels (ocean deoxygenation). Increases in atmospheric CO2 concentrations have led to an increase in dissolved CO2 and thus ocean acidity, measured by lower pH values. Ocean acidification threatens damage to coral reefs, fisheries, protected species, and other natural resources of value to society.Long-term effects of global warming, runaway climate change: On the timescale of centuries to millennia, the magnitude of global warming will be determined primarily by anthropogenic CO2 emissions. This is due to carbon dioxide's very long lifetime in the atmosphere. Long-term effects also include a response from the Earth's crust, due to ice melting and deglaciation, in a process called post-glacial rebound, when land masses are no longer depressed by the weight of ice. This could lead to landslides and increased seismic and volcanic activities. Tsunamis could be generated by submarine landslides caused by warmer ocean water thawing ocean-floor permafrost or releasing gas hydrates.Abrupt climate change, cold blob (North Atlantic): Climate change could result in global, large-scale changes in natural and social systems. Examples include ocean acidification caused by increased atmospheric concentrations of carbon dioxide, and the long-term melting of ice sheets, which contributes to sea level rise. Some large-scale changes could occur abruptly, i.e., over a short time period, and might also be irreversible. Examples of abrupt climate change are the rapid release of methane and carbon dioxide from permafrost, which would lead to amplified global warming. Another example is the possibility for the Atlantic Meridional Overturning Circulation to slow- or shutdown (see also shutdown of thermohaline circulation). This could trigger cooling in the North Atlantic, Europe, and North America. It would particularly affect areas such as the British Isles, France and the Nordic countries, which are warmed by the North Atlantic drift.The effects of climate change on human systems, mostly due to warming or shifts in precipitation patterns, or both, have been detected worldwide. The future social impacts of climate change will be uneven across the world. Many risks are expected to increase with higher magnitudes of global warming. All regions are at risk of experiencing negative impacts. Low-latitude, less developed areas face the greatest risk. A study from 2015 concluded that economic growth (gross domestic product) of poorer countries is much more impaired with projected future climate warming, than previously thought. In small islands and mega deltas, inundation as a result of sea level rise is expected to threaten vital infrastructure and human settlements. This could lead to issues of homelessness in countries with low-lying areas such as Bangladesh, as well as statelessness for populations in countries such as the Maldives and Tuvalu.Examples of impacts of global warming on humans include:A meta-analysis concluded in 2014 that each degree of temperature rise will increase violence by up to 20%, which includes fist fights, violent crimes, civil unrest, or wars.Estimates in 2015 based on the IPCC A1B emission scenario from additional greenhouse gases released from permafrost, found associated impact damages to the economy to be US$43 trillion.Crop production will probably be negatively affected in low latitude countries, while effects at northern latitudes may be positive or negative. Global warming of around 4.6 °C relative to pre-industrial levels could pose a large risk to global and regional food security. Production of wheat and maize globally has been impacted by climate change. While crop production has increased in some mid-latitude regions such as the UK and Northeast China, economic losses due to extreme weather events have increased globally. See also Climate change and agriculture.Generally impacts on public health will be more negative than positive. Impacts include: the effects of extreme weather, leading to injury and loss of life and indirect effects, such as undernutrition brought on by crop failures. There has been a shift from cold- to heat-related mortality in some regions as a result of warming.Livelihoods of indigenous peoples of the Arctic have been altered by climate change, and there is emerging evidence of climate change impacts on livelihoods of indigenous peoples in other regions. Regional impacts of climate change are now observable at more locations than before, on all continents and across ocean regions.The Arctic, Africa, small islands and Asian megadeltas are regions that are likely to be especially affected by future climate change. Africa is one of the most vulnerable continents to climate variability and change because of multiple existing stresses and low adaptive capacity. Existing stresses include poverty, political conflicts, and ecosystem degradation. By 2050, between 350 million and 600 million people are projected to experience increased water stress due to climate change (see Climate change in Africa). Climate variability and change is projected to severely compromise agricultural production, including access to food, across Africa.Mitigation of climate change are actions to reduce greenhouse gas emissions, or enhance the capacity of carbon sinks to absorb greenhouse gases from the atmosphere. There is a large potential for future reductions in emissions by a combination of activities, including energy conservation and increased energy efficiency the use of low-carbon energy technologies, such as renewable energy, nuclear energy, and carbon capture and storage and enhancing carbon sinks through, for example, reforestation and preventing deforestation.A 2015 report by Citibank concluded that transitioning to a low carbon economy would yield positive return on investments.Near- and long-term trends in the global energy system are inconsistent with limiting global warming at below 1.5 or 2 °C, relative to pre-industrial levels. Pledges made as part of the Cancún agreements are broadly consistent with having a likely chance (66 to 100% probability) of limiting global warming (in the 21st century) at below 3 °C, relative to pre-industrial levels.In limiting warming at below 2 °C, more stringent emission reductions in the near-term would allow for less rapid reductions after 2030. Many integrated models are unable to meet the 2 °C target if pessimistic assumptions are made about the availability of mitigation technologies.Climate change adaptation is another policy response. The adaptation may be planned, either in reaction to or anticipation of global warming, or spontaneous, i.e., without government intervention. Planned adaptation is already occurring on a limited basis. The barriers, limits, and costs of future adaptation are not fully understood. Environmental organizations and public figures have emphasized changes in the climate and the risks they entail, while promoting adaptation to changes in infrastructural needs and emissions reductions.Adaptation is especially important in developing countries since those countries are predicted to bear the brunt of the effects of global warming. That is, the capacity and potential for humans to adapt (called adaptive capacity) is unevenly distributed across different regions and populations, and developing countries generally have less capacity to adapt.Climate engineering (sometimes called geoengineering or climate intervention) is the deliberate modification of the climate. It has been investigated as a possible response to global warming, e.g. by NASA and the Royal Society. Techniques under research fall generally into the categories solar radiation management and carbon dioxide removal, although various other schemes have been suggested. A study from 2014 investigated the most common climate engineering methods and concluded they are either ineffective or have potentially severe side effects and cannot be stopped without causing rapid climate change.Most countries in the world are parties to the United Nations Framework Convention on Climate Change (UNFCCC).The ultimate objective of the Convention is to prevent dangerous human interference of the climate system.As stated in the Convention, this requires that greenhouse gas concentrations are stabilized in the atmosphere at a level where ecosystems can adapt naturally to climate change, food production is not threatened, and economic development can proceed in a sustainable fashion. The Framework Convention was agreed on in 1992, but global emissions have risen since then.During negotiations, the G77 (a lobbying group in the United Nations representing 133 developing countries) pushed for a mandate requiring developed countries to ""[take] the lead"" in reducing their emissions. This was justified on the basis that the developed countries' emissions had contributed most to the cumulation of greenhouse gases in the atmosphere, per-capita emissions (i.e., emissions per head of population) were still relatively low in developing countries, and the emissions of developing countries would grow to meet their development needs.This mandate was sustained in the Kyoto Protocol to the Framework Convention, which entered into legal effect in 2005. In ratifying the Kyoto Protocol, most developed countries accepted legally binding commitments to limit their emissions. These first-round commitments expired in 2012.United States President George W. Bush rejected the treaty on the basis that ""it exempts 80% of the world, including major population centres such as China and India, from compliance, and would cause serious harm to the US economy.""At the 15th UNFCCC Conference of the Parties, held in 2009 at Copenhagen, several UNFCCC Parties produced the Copenhagen Accord. Parties associated with the Accord (140 countries, as of November 2010) aim to limit the future increase in global mean temperature to below 2 °C. The 16th Conference of the Parties (COP16) was held at Cancún in 2010. It produced an agreement, not a binding treaty, that the Parties should take urgent action to reduce greenhouse gas emissions to meet a goal of limiting global warming to 2 °C above pre-industrial temperatures. It also recognized the need to consider strengthening the goal to a global average rise of 1.5 °C.The discussion continues in scientific articles that are peer-reviewed and assessed by scientists whowork in the relevant fields and participate in the Intergovernmental Panel on Climate Change. The scientific consensus as of 2013 stated in the IPCC Fifth Assessment Report is that it ""is extremely likely that human influence has been the dominant cause of the observed warming since the mid-20th century"".A 2008 report by the U.S. National Academy of Sciences stated that most scientists by then agreed that observed warming in recent decades was primarily caused by human activities increasing the amount of greenhouse gases in the atmosphere. In 2005 the Royal Society stated that while the overwhelming majority of scientists were in agreement on the main points, some individuals and organizations opposed to the consensus on urgent action needed to reduce greenhouse gas emissions had tried to undermine the science and work of the IPCC. National science academies have called on world leaders for policies to cut global emissions.In the scientific literature, there is a strong consensus that global surface temperatures have increased in recent decades and that the trend is caused mainly by human-induced emissions of greenhouse gases. No scientific body of national or international standing disagrees with this view. In November 2017, a second warning to humanity signed by 15,364 scientists from 184 countries stated that ""the current trajectory of potentially catastrophic climate change due to rising greenhouse gases from burning fossil fuels, deforestation, and agricultural production — particularly from farming ruminants for meat consumption"" is ""especially troubling"". A July 2017 study published in Environmental Research Letters asserts that the most significant action individuals could make to mitigate their own carbon footprint is to have fewer children, followed by living vehicle free, forgoing air travel and adopting a plant-based diet.The global warming controversy refers to a variety of disputes, substantially more pronounced in the popular media than in the scientific literature, regarding the nature, causes, and consequences of global warming. The disputed issues include the causes of increased global average air temperature, especially since the mid-20th century, whether this warming trend is unprecedented or within normal climatic variations, whether humankind has contributed significantly to it, and whether the increase is completely or partially an artefact of poor measurements. Additional disputes concern estimates of climate sensitivity, predictions of additional warming, and what the consequences of global warming will be.In the United States from about 1990 onwards, American conservative think tanks had begun challenging the legitimacy of global warming as a social problem. They challenged the scientific evidence, argued that global warming would have benefits, and asserted that proposed solutions would do more harm than good. Some people dispute aspects of climate change science. Organizations such as the libertarian Competitive Enterprise Institute, conservative commentators, and some companies such as ExxonMobil have challenged IPCC climate change scenarios, funded scientists who disagree with the scientific consensus, and provided their own projections of the economic cost of stricter controls. On the other hand, some fossil fuel companies have scaled back their efforts in recent years, or even called for policies to reduce global warming. Global oil companies have begun to acknowledge climate change exists and is caused by human activities and the burning of fossil fuels.The global warming problem came to international public attention in the late 1980s. Polling groups began to track opinions on the subject, at first mainly in the United States. The longest consistent polling, by Gallup in the US, found relatively small deviations of 10% or so from 1998 to 2015 in opinion on the seriousness of global warming, but with increasing polarization between those concerned and those unconcerned.By 2010, with 111 countries surveyed, Gallup determined that there had been a substantial decrease since 2007–2008 in the number of Americans and Europeans who viewed global warming as a serious threat. In the US, just a little over half the population (53%) viewed it as a serious concern for either themselves or their families this was 10 points below the 2008 poll (63%). Latin America had the biggest rise in concern: 73% said global warming was a serious threat to their families. This global poll also found that people were more likely to attribute global warming to human activities than to natural causes, except in the US where nearly half (47%) of the population attributed global warming to natural causes.A March–May 2013 survey by Pew Research Center for the People & the Press polled 39 countries about global threats. According to 54% of those questioned, global warming featured top of the perceived global threats.The history of climate change science began in the early 19th century when ice ages and other natural changes in paleoclimate were first suspected and the natural greenhouse effect first identified. In the late 19th century, scientists first argued that human emissions of greenhouse gases could change the climate. In the 1960s, the warming effect of carbon dioxide gas became increasingly convincing. By the 1990s, as a result of improving fidelity of computer models and observational work confirming the Milankovitch theory of the ice ages, a consensus position formed: greenhouse gases were deeply involved in most climate changes and human caused emissions were bringing discernible global warming. Since the 1990s, scientific research on climate change has included multiple disciplines and has expanded. Research during this period has been summarized in the Assessment Reports by the Intergovernmental Panel on Climate Change.In the 1950s, research suggested increasing temperatures, and a 1952 newspaper reported ""climate change"". This phrase next appeared in a November 1957 report in The Hammond Times which described Roger Revelle's research into the effects of increasing human-caused CO2 emissions on the greenhouse effect, ""a large scale global warming, with radical climate changes may result"". Both phrases were only used occasionally until 1975, when Wallace Smith Broecker published a scientific paper on the topic, ""Climatic Change: Are We on the Brink of a Pronounced Global Warming?"" The phrase began to come into common use, and in 1976 Mikhail Budyko's statement that ""a global warming up has started"" was widely reported. Other studies, such as a 1971 MIT report, referred to the human impact as ""inadvertent climate modification"", but an influential 1979 National Academy of Sciences study headed by Jule Charney followed Broecker in using global warming for rising surface temperatures, while describing the wider effects of increased CO2 as climate change.In 1986 and November 1987, NASA climate scientist James Hansen gave testimony to Congress on global warming. There were increasing heatwaves and drought problems in the summer of 1988, and when Hansen testified in the Senate on 23 June he sparked worldwide interest. He said, ""global warming has reached a level such that we can ascribe with a high degree of confidence a cause and effect relationship between the greenhouse effect and the observed warming."" Public attention increased over the summer, and global warming became the dominant popular term, commonly used both by the press and in public discourse.In a 2008 NASA article on usage, Erik M. Conway defined global warming as ""the increase in Earth’s average surface temperature due to rising levels of greenhouse gases"", while climate change was ""a long-term change in the Earth’s climate, or of a region on Earth."" As effects such as changing patterns of rainfall and rising sea levels would probably have more impact than temperatures alone, he considered global climate change a more scientifically accurate term, and like the Intergovernmental Panel on Climate Change, the NASA website would emphasize this wider context.AnthropoceneClimate change and agricultureEnvironmental impact of the coal industryGeologic temperature recordGlobal coolingGlossary of climate changeGreenhouse gas emissions accountingList of countries by carbon dioxide emissionsHolocene extinctionIndex of climate change articlesScientific opinion on climate changeNASA Goddard Institute for Space Studies – Global change researchNOAA State of the Climate Report – U.S. and global monthly state of the climate reportsClimate Change at the National Academies – repository for reportsNature Reports Climate Change – free-access web resourceMet Office: Climate Guide – UK National Weather ServiceEducational Global Climate Modelling (EdGCM) – research-quality climate change simulatorClimate Science Special Report United States 2017NASA: Climate change: How do we know?Global Climate Change: NASA's Eyes on the Earth – NASA, JPL, CaltechGlobal Climate Change Indicators – NOAANOAA Climate Services – NOAASkeptical Science: Getting skeptical about global warming skepticismUnderstanding Climate Change – Frequently Asked Questions – UCARGlobal Carbon Dioxide Circulation (NASA 13 December 2016)The World Bank – Climate Change – A 4 Degree Warmer World – We must and can avoid itClimate change tutorial by Prof. Myles Allen (Oxford), March 2018: Parts 1, 2, 3, 4, 5 (45 min. total) background & slide deck";environmental disaster;Global warming;0
"The Hanford Site is a decommissioned nuclear production complex operated by the United States federal government on the Columbia River in the U.S. state of Washington. The site has been known by many names, including Hanford Project, Hanford Works, Hanford Engineer Works and Hanford Nuclear Reservation. Established in 1943 as part of the Manhattan Project in Hanford, south-central Washington, the site was home to the B Reactor, the first full-scale plutonium production reactor in the world. Plutonium manufactured at the site was used in the first nuclear bomb, tested at the Trinity site, and in Fat Man, the bomb detonated over Nagasaki, Japan.During the Cold War, the project expanded to include nine nuclear reactors and five large plutonium processing complexes, which produced plutonium for most of the more than 60,000 weapons built for the U.S. nuclear arsenal. Nuclear technology developed rapidly during this period, and Hanford scientists produced major technological achievements. Many early safety procedures and waste disposal practices were inadequate, and government documents have confirmed that Hanford's operations released significant amounts of radioactive materials into the air and the Columbia River.The weapons production reactors were decommissioned at the end of the Cold War, and decades of manufacturing left behind 53 million US gallons (200,000 m3) of high-level radioactive waste stored within 177 storage tanks, an additional 25 million cubic feet (710,000 m3) of solid radioactive waste, and 200 square miles (520 km2) of contaminated groundwater beneath the site. In 2011, the federal agency charged with overseeing the site, the US Department of Energy (DOE), emptied 149 single-shell tanks by pumping nearly all of the liquid waste out into 28 newer double-shell tanks. DOE later found water intruding into at least 14 single-shell tanks and that one of them had been leaking about 640 US gallons (2,400 l 530 imp gal) per year into the ground since about 2010. In 2012, DOE discovered a leak also from a double-shell tank caused by construction flaws and corrosion in the bottom, and that 12 double-shell tanks have similar construction flaws. Since then, the DOE changed to monitoring single-shell tanks monthly and double-shell tanks every three years, and also changed monitoring methods. In March 2014, the DOE announced further delays in the construction of the Waste Treatment Plant, which will affect the schedule for removing waste from the tanks. Intermittent discoveries of undocumented contamination have slowed the pace and raised the cost of cleanup.In 2007, the Hanford site represented 60% of high-level radioactive waste by volume managed by the US Department of Energy and 7-9% of all nuclear waste in the United States (the DOE manages 15% of nuclear waste in the US, with the remaining 85% being commercial spent nuclear fuel). Hanford is currently the most contaminated nuclear site in the United States and is the focus of the nation's largest environmental cleanup. Besides the cleanup project, Hanford also hosts a commercial nuclear power plant, the Columbia Generating Station, and various centers for scientific research and development, such as the Pacific Northwest National Laboratory and the LIGO Hanford Observatory.On November 10, 2015, it was designated as part of the Manhattan Project National Historical Park alongside other sites in Oak Ridge and Los Alamos.The Hanford Site occupies 586 square miles (1,518 km2)—roughly equivalent to half of the total area of Rhode Island—within Benton County, Washington. This land is closed to the general public. It is a desert environment receiving under 10 inches of annual precipitation, covered mostly by shrub-steppe vegetation. The Columbia River flows along the site for approximately 50 miles (80 km), forming its northern and eastern boundary. The original site was 670 square miles (1,740 km2) and included buffer areas across the river in Grant and Franklin counties. Some of this land has been returned to private use and is now covered with orchards and irrigated fields. In 2000, large portions of the site were turned over to the Hanford Reach National Monument. The site is divided by function into three main areas. The nuclear reactors were located along the river in an area designated as the 100 Area the chemical separations complexes were located inland in the Central Plateau, designated as the 200 Area and various support facilities were located in the southeast corner of the site, designated as the 300 Area.The site is bordered on the southeast by the Tri-Cities, a metropolitan area composed of Richland, Kennewick, Pasco, and smaller communities, and home to over 230,000 residents. Hanford is a primary economic base for these cities.The confluence of the Yakima, Snake, and Columbia rivers has been a meeting place for native peoples for centuries. The archaeological record of Native American habitation of this area stretches back over ten thousand years. Tribes and nations including the Yakama, Nez Perce, and Umatilla used the area for hunting, fishing, and gathering plant foods. Hanford archaeologists have identified numerous Native American sites, including ""pit house villages, open campsites, fish farming sites, hunting/kill sites, game drive complexes, quarries, and spirit quest sites"", and two archaeological sites were listed on the National Register of Historic Places in 1976. Native American use of the area continued into the 20th century, even as the tribes were relocated to reservations. The Wanapum people were never forced onto a reservation, and they lived along the Columbia River in the Priest Rapids Valley until 1943. Settlers moved into the region in the 1860s, initially along the Columbia River south of Priest Rapids. They established farms and orchards supported by small-scale irrigation projects and railroad transportation, with small town centers at Hanford, White Bluffs, and Richland.During World War II, the S-1 Section of the federal Office of Scientific Research and Development (OSRD) sponsored an intensive research project on plutonium. The research contract was awarded to scientists at the University of Chicago Metallurgical Laboratory (Met Lab). At the time, plutonium was a rare element that had only recently been isolated in a University of California laboratory. The Met Lab researchers worked on producing chain-reacting ""piles"" of uranium to convert it to plutonium and finding ways to separate plutonium from uranium. The program was accelerated in 1942, as the United States government became concerned that scientists in Nazi Germany were developing a nuclear weapons program.In September 1942, the Army Corps of Engineers placed the newly formed Manhattan Project under the command of Brigadier General Leslie R. Groves, charging him with the construction of industrial-size plants for manufacturing plutonium and uranium. Groves recruited the DuPont Company to be the prime contractor for the construction of the plutonium production complex. DuPont recommended that it be located far away from the existing uranium production facility at Oak Ridge, Tennessee. The ideal site was described by these criteria:A large and remote tract of landA ""hazardous manufacturing area"" of at least 12 by 16 miles (19 by 26 km)Space for laboratory facilities at least 8 miles (13 km) from the nearest reactor or separations plantNo towns of more than 1,000 people closer than 20 miles (32 km) from the hazardous rectangleNo main highway, railway, or employee village closer than 10 miles (16 km) from the hazardous rectangleA clean and abundant water supplyA large electric power supplyGround that could bear heavy loads.In December 1942, Groves dispatched his assistant Colonel Franklin T. Matthias and DuPont engineers to scout potential sites. Matthias reported that Hanford was ""ideal in virtually all respects"", except for the farming towns of White Bluffs and Hanford. General Groves visited the site in January 1943 and established the Hanford Engineer Works, codenamed ""Site W"". The federal government quickly acquired the land under its war powers authority and relocated some 1,500 residents of Hanford, White Bluffs, and nearby settlements, as well as the Wanapum people, Confederated Tribes and Bands of the Yakama Nation, the Confederated Tribes of the Umatilla Indian Reservation, and the Nez Perce Tribe.The Hanford Engineer Works (HEW) broke ground in March 1943 and immediately launched a massive and technically challenging construction project. DuPont advertised for workers in newspapers for an unspecified ""war construction project"" in southeastern Washington, offering ""attractive scale of wages"" and living facilities.The construction workers (who reached a peak of 44,900 in June 1944) lived in a construction camp near the old Hanford townsite. The administrators and engineers lived in the government town established at Richland Village, which eventually had accommodation in 4,300 family units and 25 dormitories.Construction of the nuclear facilities proceeded rapidly. Before the end of the war in August 1945, the HEW built 554 buildings at Hanford, including three nuclear reactors (105-B, 105-D, and 105-F) and three plutonium processing canyons (221-T, 221-B, and 221-U), each 250 meters (820 ft) long.To receive the radioactive wastes from the chemical separations process, the HEW built ""tank farms"" consisting of 64 single-shell underground waste tanks (241-B, 241-C, 241-T, and 241-U). The project required 386 miles (621 km) of roads, 158 miles (254 km) of railway, and four electrical substations. The HEW used 780,000 cubic yards (600,000 m3) of concrete and 40,000 short tons (36,000 t) of structural steel and consumed $230 million between 1943 and 1946.The B Reactor (105-B) at Hanford was the first large-scale plutonium production reactor in the world. It was designed and built by DuPont based on an experimental design by Enrico Fermi, and originally operated at 250 megawatts (thermal). The reactor was graphite moderated and water cooled. It consisted of a 28-by-36-foot (8.5 by 11.0 m), 1,200-short-ton (1,100 t) graphite cylinder lying on its side, penetrated through its entire length horizontally by 2,004 aluminium tubes. Two hundred short tons (180 t) of uranium slugs, 1.625 inches (4.13 cm) diameter by 8 inches (20 cm) long, sealed in aluminium cans went into the tubes. Cooling water was pumped through the aluminium tubes around the uranium slugs at the rate of 30,000 US gallons (110,000 L) per minute.Construction on B Reactor began in August 1943 and was completed on September 13, 1944. The reactor went critical in late September and, after overcoming nuclear poisoning, produced its first plutonium on November 6, 1944. Plutonium was produced in the Hanford reactors when a uranium-238 atom in a fuel slug absorbed a neutron to form uranium-239. U-239 rapidly undergoes beta decay to form neptunium-239, which rapidly undergoes a second beta decay to form plutonium-239. The irradiated fuel slugs were transported by rail to three huge remotely operated chemical separation plants called ""canyons"" that were about 10 miles (16 km) away. A series of chemical processing steps separated the small amount of plutonium that was produced from the remaining uranium and the fission waste products. This first batch of plutonium was refined in the 221-T plant from December 26, 1944, to February 2, 1945, and delivered to the Los Alamos laboratory in New Mexico on February 5, 1945.The material was used in the first Trinity nuclear explosion on July 16, 1945.Two identical reactors, D Reactor and F reactor, came online in December 1944 and February 1945, respectively. By April 1945, shipments of plutonium were headed to Los Alamos every five days, and Hanford soon provided enough material for the bombs tested at Trinity and dropped over Nagasaki. Throughout this period, the Manhattan Project maintained a top secret classification. Until news arrived of the bomb dropped on Hiroshima, fewer than one percent of Hanford's workers knew they were working on a nuclear weapons project. General Groves noted in his memoirs that ""We made certain that each member of the project thoroughly understood his part in the total effort that, and nothing more.""Initially six reactors or ""piles"" were proposed, when the plutonium was to be used in the gun-type Thin Man bomb. In mid-1944 a simple gun-type bomb was found to be impractical for plutonium, and the more advanced Fat Man bomb required less plutonium. The number of piles was reduced to four and then three and the number of chemical separation plants from four to three.In the short time frame of the Manhattan Project, Hanford engineers produced many significant technological advances. As no one had ever built an industrial-scale nuclear reactor before, scientists were unsure how much heat would be generated by fission during normal operations. Seeking the greatest possible production while maintaining an adequate safety margin, DuPont engineers installed ammonia-based refrigeration systems with the D and F reactors to further chill the river water before its use as reactor coolant.Another difficulty the engineers struggled with was how to deal with radioactive contamination. Once the canyons began processing irradiated slugs, the machinery would become so radioactive that it would be unsafe for humans ever to come in contact with it. The engineers therefore had to devise methods to allow for the replacement of any component via remote control. They came up with a modular cell concept, which allowed major components to be removed and replaced by an operator sitting in a heavily shielded overhead crane. This method required early practical application of two technologies that later gained widespread use: Teflon, used as a gasket material, and closed-circuit television, used to give the crane operator a better view of the process.In September 1946, the General Electric Company assumed management of the Hanford Works under the supervision of the newly created Atomic Energy Commission. As the Cold War began, the United States faced a new strategic threat in the rise of the Soviet nuclear weapons program. In August 1947, the Hanford Works announced funding for the construction of two new weapons reactors and research to develop a new chemical separations process, entering a new phase of expansion.By 1963, the Hanford Site was home to nine nuclear reactors along the Columbia River, five reprocessing plants on the central plateau, and more than 900 support buildings and radiological laboratories around the site. Extensive modifications and upgrades were made to the original three World War II reactors, and a total of 177 underground waste tanks were built. Hanford was at its peak production from 1956 to 1965. Over the entire 40 years of operations, the site produced about 63 short tons (57 t) of plutonium, supplying the majority of the 60,000 weapons in the U.S. arsenal. Uranium-233 was also produced.In 1976, a Hanford technician named Harold McCluskey received the largest recorded dose of americium following a laboratory accident. Due to prompt medical intervention, he survived the incident and died eleven years later of natural causes.Most of the reactors were shut down between 1964 and 1971, with an average individual life span of 22 years. The last reactor, N Reactor, continued to operate as a dual-purpose reactor, being both a power reactor used to feed the civilian electrical grid via the Washington Public Power Supply System (WPPSS) and a plutonium production reactor for nuclear weapons. N Reactor operated until 1987. Since then, most of the Hanford reactors have been entombed (""cocooned"") to allow the radioactive materials to decay, and the surrounding structures have been removed and buried. The B-Reactor has not been cocooned and is accessible to the public on occasional guided tours. It was listed on the National Register of Historic Places in 1992, and some historians advocated converting it into a museum. B reactor was designated a National Historic Landmark by the National Park Service on August 19, 2008.The United States Department of Energy assumed control of the Hanford Site in 1977. Although uranium enrichment and plutonium breeding were slowly phased out, the nuclear legacy left an indelible mark on the Tri-Cities. Since World War II, the area had developed from a small farming community to a booming ""Atomic Frontier"" to a powerhouse of the nuclear-industrial complex. Decades of federal investment created a community of highly skilled scientists and engineers. As a result of this concentration of specialized skills, the Hanford Site was able to diversify its operations to include scientific research, test facilities, and commercial nuclear power production.As of  2013, operational facilities located at the Hanford Site included:The Pacific Northwest National Laboratory, owned by the Department of Energy and operated by Battelle Memorial InstituteThe Fast Flux Test Facility (FFTF), a national research facility in operation from 1980 to 1992 whose last fuel was removed in 2008LIGO's Hanford Observatory, an interferometer searching for gravitational wavesColumbia Generating Station, a commercial nuclear power plant operated by Energy Northwest.A US Navy nuclear submarine reactor dry storage site containing sealed reactor sections of 114 US Navy ships as of  2008.The Department of Energy and its contractors offer tours of the site. The tours are free, can be reserved in  advance via the department's web site, and are limited to U.S. citizens at least 18 years of age. Between 2009 and 2018, approximately 80,000 people visited the site, bringing an estimated annual tourist income of two million dollars to the surrounding area.On the morning of May 9, 2017, a twenty-foot (6 m) section of a 360-foot (110 m) tunnel caved-in. It was used to store contaminated materials and was located next to the Plutonium Uranium Extraction (PUREX) Facility in the 200 East Area in the center of the Hanford Site. All non-essential personnel were placed under a take cover alarm on the site. Some 53 truckloads (about 550 cubic yards (420 m3)) of soil were used to fill in the hole.A huge volume of water from the Columbia River was required to dissipate the heat produced by Hanford's nuclear reactors.As much as 75,000 gallons per minute was diverted from the Columbia River to cool the reactor.From 1944 to 1971, pump systems drew cooling water from the river and, after treating this water for use by the reactors, returned it to the river. Before its release into the river, the used water was held in large tanks known as retention basins for up to six hours. Longer-lived isotopes were not affected by this retention, and several terabecquerels entered the river every day. The federal government kept knowledge about these radioactive releases secret. Radiation was later measured 200 miles (320 km) downstream as far west as the Washington and Oregon coasts.The plutonium separation process resulted in the release of radioactive isotopes into the air, which were carried by the wind throughout southeastern Washington and into parts of Idaho, Montana, Oregon, and British Columbia. Downwinders were exposed to radionuclides, particularly iodine-131, with the heaviest releases during the period from 1945 to 1951. These radionuclides entered the food chain via dairy cows grazing on contaminated fields hazardous fallout was ingested by communities who consumed radioactive food and milk. Most of these airborne releases were a part of Hanford's routine operations, while a few of the larger releases occurred in isolated incidents. In 1949, an intentional release known as the ""Green Run"" released 8,000 curies of iodine-131 over two days. Another source of contaminated food came from Columbia River fish, an impact felt disproportionately by Native American communities who depended on the river for their customary diets. A U.S. government report released in 1992 estimated that 685,000 curies of radioactive iodine-131 had been released into the river and air from the Hanford site between 1944 and 1947.Beginning in the 1960s, scientists with the U.S. Public Health Service published reports about radioactivity released from Hanford, and there were protests from the health departments of Oregon and Washington. In response to an article in the Spokane Spokesman Review in September 1985, the Department of Energy announced to declassify environmental records and, in February 1986, released 19,000 pages of previously unavailable historical documents about Hanford's operations. The Washington State Department of Health collaborated with the citizen-led Hanford Health Information Network (HHIN) to publicize data about the health effects of Hanford's operations. HHIN reports concluded that residents who lived downwind from Hanford or who used the Columbia River downstream were exposed to elevated doses of radiation that placed them at increased risk for various cancers and other diseases, particularly forms of Thyroid disease. A mass tort lawsuit brought by two thousand Hanford downwinders against the federal government spent many years in the court system. In 2005, two of six plaintiffs who went to trial were awarded $500,000 in damages.In October 2015, the Department of Energy resolved the final cases.  They paid more than $60 million in legal fees and $7 million in damages.Since 2003, radioactive materials are known to be leaking from Hanford into the environment: ""The highest tritium concentration detected in riverbank springs during 2002 was 58,000 pCi/L (2,100 Bq/L) at the Hanford Townsite. The highest iodine-129 concentration of 0.19 pCi/L (0.007 Bq/L) was also found in a Hanford Townsite spring. The WHO guidelines for radionuclides in drinking-water limits levels of iodine-129 at 1 Bq/L, and tritium at 10,000 Bq/L. Concentrations of radionuclides including tritium, technetium-99, and iodine-129 in riverbank springs near the Hanford Townsite have generally been increasing since 1994. This is an area where a major groundwater plume from the 200 East Area intercepts the river ... Detected radionuclides include strontium-90, technetium-99, iodine-129, uranium-234, −235, and −238, and tritium. Other detected contaminants include arsenic, chromium, chloride, fluoride, nitrate, and sulfate.""In February 2013, Governor Jay Inslee announced that a tank storing radioactive waste at the site had been leaking liquids on average of 150 to 300 gallons per year. He said that though the leak posed no immediate health risk to the public, it should not be an excuse for not doing anything. On February 22, 2013, the Governor stated that ""6 more tanks at Hanford site"" were ""leaking radioactive waste""As of  2013, there are 177 tanks at Hanford, 149 of which have a single shell. Historically single shell tanks were used for storing radioactive liquid waste and designed to last 20 years. By 2005, some liquid waste was transferred from single shell tanks to (safer) double shell tanks. A substantial amount of residue remains in the older single shell tanks with one containing an estimated 447,000 gallons (1,700 m3) of radioactive sludge, for example. It is believed that up to six of these ""empty"" tanks are leaking. Two tanks are reportedly leaking at a rate of 300 gallons (1,136 liters) per year each, while the remaining four tanks are leaking at a rate of 15 gallons (57 liters) per year each.Since 1987, workers have reported exposure to harmful vapors after working around underground nuclear storage tanks, with no solution found. More than 40 workers in 2014 alone reported smelling vapors and became ill with ""nosebleeds, headaches, watery eyes, burning skin, contact dermatitis, increased heart rate, difficulty breathing, coughing, sore throats, expectorating, dizziness and nausea, ... Several of these workers have long-term disabilities."" Doctors checked workers and cleared them to return to work. Monitors worn by tank workers have found no samples with chemicals close to the federal limit for occupational exposure.In August 2014, OSHA ordered the facility to rehire a contractor and pay $220,000 in back wages for firing them for whistleblowing on safety concerns at the site.On November 19, 2014, Washington Attorney General Bob Ferguson said the state planned to sue the DOE and its contractor to protect workers from hazardous vapors at Hanford. A 2014 report by the DOE Savannah River National Laboratory initiated by 'Washington River Protection Solutions' found that DOE's methods to study vapor releases were inadequate, particularly, that they did not account for short but intense vapor releases. They recommended ""proactively sampling the air inside tanks to determine its chemical makeup accelerating new practices to prevent worker exposures and modifying medical evaluations to reflect how workers are exposed to vapors"".On June 25, 1988, the Hanford site was divided into four areas and proposed for inclusion on the National Priorities List. On May 15, 1989, the Washington Department of Ecology, the United States Environmental Protection Agency, and the Department of Energy entered into the Tri-Party Agreement, which provides a legal framework for environmental remediation at Hanford. As of  2014 the agencies are engaged in the world's largest environmental cleanup, with many challenges to be resolved in the face of overlapping technical, political, regulatory, and cultural interests. The cleanup effort is focused on three outcomes: restoring the Columbia River corridor for other uses, converting the central plateau to long-term waste treatment and storage, and preparing for the future. The cleanup effort is managed by the Department of Energy under the oversight of the two regulatory agencies. A citizen-led Hanford Advisory Board provides recommendations from community stakeholders, including local and state governments, regional environmental organizations, business interests, and Native American tribes. Citing the 2014 Hanford Lifecycle Scope Schedule and Cost report, the 2014 estimated cost of the remaining Hanford clean up is $113.6 billion – more than $3 billion per year for the next six years, with a lower cost projection of approximately $2 billion per year until 2046. About 11,000 workers are on site to consolidate, clean up, and mitigate waste, contaminated buildings, and contaminated soil. Originally scheduled to be complete within thirty years, the cleanup was less than half finished by 2008. Of the four areas that were formally listed as Superfund sites on October 4, 1989, only one has been removed from the list following cleanup.While major releases of radioactive material ended with the reactor shutdown in the 1970s and many of the most dangerous wastes are contained, there are continued concerns about contaminated groundwater headed toward the Columbia River and about workers' health and safety.The most significant challenge at Hanford is stabilizing the 53,000,000 US gallons (200,000,000 l 44,000,000 imp gal) of high-level radioactive waste stored in 177 underground tanks. By 1998, about a third of these tanks had leaked waste into the soil and groundwater. As of  2008, most of the liquid waste had been transferred to more secure double-shelled tanks however, 2,800,000 US gallons (11,000,000 l 2,300,000 imp gal) of liquid waste, together with 27,000,000 US gallons (100,000,000 l 22,000,000 imp gal) of salt cake and sludge, remains in the single-shelled tanks. DOE lacks information about the extent to which the 27 double-shell tanks may be susceptible to corrosion. Without determining the extent to which the factors that contributed to the leak in AY-102 were similar to the other 27 double-shell tanks, DOE cannot be sure how long its double-shell tanks can safely store waste. That waste was originally scheduled to be removed by 2018. As of  2008, the revised deadline was 2040. Nearby aquifers contain an estimated 270,000,000,000 US gallons (1.0×1012 l 2.2×1011 imp gal) of contaminated groundwater as a result of the leaks. As of  2008, 1,000,000 US gallons (3,800,000 l 830,000 imp gal) of radioactive waste is traveling through the groundwater toward the Columbia River. This waste is expected to reach the river in 12 to 50 years if cleanup does not proceed on schedule. The site includes 25 million cubic feet (710,000 m3) of solid radioactive waste.Under the Tri-Party Agreement, lower-level hazardous wastes are buried in huge lined pits that will be sealed and monitored with sophisticated instruments for many years. Disposal of plutonium and other high-level wastes is a more difficult problem that continues to be a subject of intense debate. As an example, plutonium-239 has a half-life of 24,100 years, and a decay of ten half-lives is required before a sample is considered to cease its radioactivity. In 2000, the Department of Energy awarded a $4.3 billion contract to Bechtel, a San Francisco-based construction and engineering firm, to build a vitrification plant to combine the dangerous wastes with glass to render them stable. Construction began in 2002. The plant was originally scheduled to be operational by 2011, with vitrification completed by 2028. According to a 2012 study by the General Accounting Office, there were a number of serious unresolved technical and managerial problems. As of  2013 estimated costs were $13.4 billion with commencement of operations estimated to be in 2022 and about 3 decades of operation.In May 2007, state and federal officials began closed-door negotiations about the possibility of extending legal cleanup deadlines for waste vitrification in exchange for shifting the focus of the cleanup to urgent priorities, such as groundwater remediation. Those talks stalled in October 2007. In early 2008, a $600 million cut to the Hanford cleanup budget was proposed. Washington state officials expressed concern about the budget cuts, as well as missed deadlines and recent safety lapses at the site, and threatened to file a lawsuit alleging that the Department of Energy was in violation of environmental laws. They appeared to step back from that threat in April 2008 after another meeting of federal and state officials resulted in progress toward a tentative agreement.During excavations from 2004 to 2007 a sample of purified plutonium was uncovered inside a safe in a waste trench, and has been dated to about the 1940s, making it the second-oldest sample of purified plutonium known to exist. Analyses published in 2009 concluded that the sample originated at Oak Ridge, and was one of several sent to Hanford for optimization tests of the T-Plant until Hanford could produce its own plutonium. Documents refer to such a sample, belonging to ""Watt's group"", which was disposed of in its safe when a radiation leak was suspected.Some of the radioactive waste at Hanford was supposed to be stored in the planned Yucca Mountain nuclear waste repository, but after that project was suspended, Washington State sued, joined by South Carolina. Their first suit was dismissed in July 2011. In a subsequent suit, federal authorities were ordered to either approve or reject plans for the Yucca Mountain storage site.A potential radioactive leak was reported in 2013 the clean up was estimated to have cost $40 billion with $115 billion more required.The Hanford site operations were initially directed by Colonel Franklin Matthias of the U.S. Army Corps of Engineers. Postwar the Atomic Energy Commission took over, and then the Energy Research and Development Administration. Since 1977, Hanford operations are directed by the U.S. Department of Energy. It has been operated under government contract by various private companies over the years, as summarized in the table through 2000.Plutonium Finishing Plant (PFP) – made plutonium metal for use in weaponsB Plant, S Plant, T Plant – processing, separation, and extraction of various chemicals and isotopesHealth Instruments Section – an attempt to keep workers and the environment safeREDOX Plant / C Plant – recovered wasted uranium from World War II processesExperimental Animal Farm and Aquatic Biology LaboratoryTechnical Center – radiochemistry, physics, metallurgy, biophysics, radioactive sewer, neutralization, metal fab, fuels manufacturingTank Farms – storage of liquid nuclear wasteMetal Recovery Plant / U Plant – recover uranium from tank farmsUranium Trioxide Plant (aka Uranium Oxide Plant aka UO3 Plant) – took output from other plants (i.e. liquid uranyl nitrate hexahydrate from U plant and PUREX plant), made uranium trioxide powderPlutonium-Uranium Extraction Plant / PUREX Plant – extracted useful material from spent fuel waste (also see the PUREX article)Plutonium Recycle Test Reactor (PRTR) – experimented with alternative fuel mixturesPlutonium Fuels Pilot Plant (PFPP) – see PRTR																						Lists of nuclear disasters and radioactive incidentsTimeline of nuclear weapons developmentJohn M. Findlay and Bruce Hevly. Atomic Frontier Days: Hanford and the American West (University of Washington Press 2011) 368 pages explores the history of the Hanford nuclear reservation and the tri-cities of Richland, Pasco, and Kennewick, WashingtonOfficial website Department of Energy.Washington Department of Ecology – Nuclear Waste Program State agency that regulates Hanford cleanup.U.S. Environmental Protection Agency Federal agency that regulates Hanford cleanup.";environmental disaster;Hanford Site;0
"Health is the ability of a biological system to acquire, convert, allocate, distribute, and utilize energy with maximum efficiency. The World Health Organization (WHO) defined human health in a broader sense in its 1948 constitution as ""a state of complete physical, mental and social well-being and not merely the absence of disease or infirmity."" This definition has been subject to controversy, in particular as lacking operational value, the ambiguity in developing cohesive health strategies and because of the problem created by use of the word ""complete"", which makes it practically impossible to achieve. Other definitions have been proposed, among which a recent definition that correlates health and personal satisfaction.The definition of health has evolved over time. In keeping with the biomedical perspective, early definitions of health focused on the theme of the body's ability to function health was seen as a state of normal function that could be disrupted from time to time by disease. An example of such a definition of health is: ""a state characterized by anatomic, physiologic, and psychological integrity ability to perform personally valued family, work, and community roles ability to deal with physical, biological, psychological, and social stress"". Then in 1948, in a radical departure from previous definitions, the World Health Organization (WHO) proposed a definition that aimed higher: linking health to well-being, in terms of ""physical, mental, and social well-being, and not merely the absence of disease and infirmity"". Although this definition was welcomed by some as being innovative, it was also criticized as being vague, excessively broad and was not construed as measurable. For a long time, it was set aside as an impractical ideal and most discussions of health returned to the practicality of the biomedical model.Just as there was a shift from viewing disease as a state to thinking of it as a process, the same shift happened in definitions of health. Again, the WHO played a leading role when it fostered the development of the health promotion movement in the 1980s. This brought in a new conception of health, not as a state, but in dynamic terms of resiliency, in other words, as ""a resource for living"". 1984 WHO revised the definition of health defined it as ""the extent to which an individual or group is able to realize aspirations and satisfy needs and to change or cope with the environment. Health is a resource for everyday life, not the objective of living it is a positive concept, emphasizing social and personal resources, as well as physical capacities"". Thus, health referred to the ability to maintain homeostasis and recover from insults. Mental, intellectual, emotional and social health referred to a person's ability to handle stress, to acquire skills, to maintain relationships, all of which form resources for resiliency and independent living.Since the late 1970s, the federal Healthy People Initiative has been a visible component of the United States’ approach to improving population health.  In each decade, a new version of Healthy People is issued, featuring updated goals and identifying topic areas and quantifiable objectives for health improvement during the succeeding ten years, with assessment at that point of progress or lack thereof. Progress has been limited to many objectives, leading to concerns about the effectiveness of Healthy People in shaping outcomes in the context of a decentralized and uncoordinated US health system. Healthy People 2020 gives more prominence to health promotion and preventive approaches and adds a substantive focus on the importance of addressing social determinants of health. A new expanded digital interface facilitates use and dissemination rather than bulky printed books as produced in the past. The impact of these changes to Healthy People will be determined in the coming years.Systematic activities to prevent or cure health problems and promote good health in humans are undertaken by health care providers. Applications with regard to animal health are covered by the veterinary sciences. The term ""healthy"" is also widely used in the context of many types of non-living organizations and their impacts for the benefit of humans, such as in the sense of healthy communities, healthy cities or healthy environments. In addition to health care interventions and a person's surroundings, a number of other factors are known to influence the health status of individuals, including their background, lifestyle, and economic, social conditions and spirituality these are referred to as ""determinants of health."" Studies have shown that high levels of stress can affect human health.In the first decade of the 21st century, the conceptualization of health as an ability opened the door for self-assessments to become the main indicators to judge the performance of efforts aimed at improving human health. It also created the opportunity for every person to feel healthy, even in the presence of multiple chronic diseases, or a terminal condition, and for the re-examination of determinants of health, away from the traditional approach that focuses on the reduction of the prevalence of diseases.Generally, the context in which an individual lives is of great importance for both his health status and quality of  their life. It is increasingly recognized that health is maintained and improved not only through the advancement and application of health science, but also through the efforts and intelligent lifestyle choices of the individual and society. According to the World Health Organization, the main determinants of health include the social and economic environment, the physical environment and the person's individual characteristics and behaviors.More specifically, key factors that have been found to influence whether people are healthy or unhealthy include the following:An increasing number of studies and reports from different organizations and contexts examine the linkages between health and different factors, including lifestyles, environments, health care organization and health policy, one specific health policy brought into many countries in recent years was the introduction of the sugar tax. Beverage taxes came into light with increasing concerns about obesity, particularly among youth. Sugar-sweetened beverages have become a target of anti-obesity initiatives with increasing evidence of their link to obesity.– such as the 1974 Lalonde report from Canada the Alameda County Study in California and the series of World Health Reports of the World Health Organization, which focuses on global health issues including access to health care and improving public health outcomes, especially in developing countries.The concept of the ""health field,"" as distinct from medical care, emerged from the Lalonde report from Canada. The report identified three interdependent fields as key determinants of an individual's health. These are:Lifestyle: the aggregation of personal decisions (i.e., over which the individual has control) that can be said to contribute to, or cause, illness or deathEnvironmental: all matters related to health external to the human body and over which the individual has little or no controlBiomedical: all aspects of health, physical and mental, developed within the human body as influenced by genetic make-up.The maintenance and promotion of health is achieved through different combination of physical, mental, and social well-being, together sometimes referred to as the ""health triangle."" The WHO's 1986 Ottawa Charter for Health Promotion further stated that health is not just a state, but also ""a resource for everyday life, not the objective of living. Health is a positive concept emphasizing social and personal resources, as well as physical capacities.""Focusing more on lifestyle issues and their relationships with functional health, data from the Alameda County Study suggested that people can improve their health via exercise, enough sleep, maintaining a healthy body weight, limiting alcohol use, and avoiding smoking. Health and illness can co-exist, as even people with multiple chronic diseases or terminal illnesses can consider themselves healthy.The environment is often cited as an important factor influencing the health status of individuals. This includes characteristics of the natural environment, the built environment and the social environment. Factors such as clean water and air, adequate housing, and safe communities and roads all have been found to contribute to good health, especially to the health of infants and children. Some studies have shown that a lack of neighborhood recreational spaces including natural environment leads to lower levels of personal satisfaction and higher levels of obesity, linked to lower overall health and well being. This suggests that the positive health benefits of natural space in urban neighborhoods should be taken into account in public policy and land use.Genetics, or inherited traits from parents, also play a role in determining the health status of individuals and populations. This can encompass both the predisposition to certain diseases and health conditions, as well as the habits and behaviors individuals develop through the lifestyle of their families. For example, genetics may play a role in the manner in which people cope with stress, either mental, emotional or physical. For example, obesity is a significant problem in the United States that contributes to bad mental health and causes stress in the lives of great numbers of people.  (One difficulty is the issue raised by the debate over the relative strengths of genetics and other factors interactions between genetics and environment may be of particular importance.)A number of types of health issues are common around the globe.  Disease is one of the most common. According to GlobalIssues.org, approximately 36 million people die each year from non-communicable (not contagious) disease including cardiovascular disease, cancer, diabetes and chronic lung disease (Shah, 2014).Among communicable diseases, both viral and bacterial, AIDS/HIV, tuberculosis, and malaria are the most common, causing millions of deaths every year (Shah, 2014).Another health issue that causes death or contributes to other health problems is malnutrition, especially among children.  One of the groups malnutrition affects most is young children.  Approximately 7.5 million children under the age of 5 die from malnutrition, usually brought on by not having the money to find or make food (Shah, 2014).Bodily injuries are also a common health issue worldwide.  These injuries, including broken bones, fractures, and burns can reduce a person's quality of life or can cause fatalities including infections that resulted from the injury or the severity injury in general (Moffett, 2013).Lifestyle choices are contributing factors to poor health in many cases.  These include smoking cigarettes, and can also include a poor diet, whether it is overeating or an overly constrictive diet.  Inactivity can also contribute to health issues and also a lack of sleep, excessive alcohol consumption, and neglect of oral hygiene (Moffett2013).There are also genetic disorders that are inherited by the person and can vary in how much they affect the person and when they surface (Moffett, 2013).Though the majority of these health issues are preventable, a major contributor to global ill health is the fact that approximately 1 billion people lack access to health care systems (Shah, 2014). Arguably, the most common and harmful health issue is that a great many people do not have access to quality remedies.The World Health Organization describes mental health as ""a state of well-being in which the individual realizes his or her own abilities, can cope with the normal stresses of life, can work productively and fruitfully, and is able to make a contribution to his or her community"".  Mental Health is not just the absence of mental illness.Mental illness is described as 'the spectrum of cognitive, emotional, and behavioral conditions that interfere with social and emotional well-being and the lives and productivity of people. Having a mental illness can seriously impair, temporarily or permanently, the mental functioning of a person. Other terms include: 'mental health problem', 'illness', 'disorder', 'dysfunction'.Roughly a quarter of all adults 18 and over in the US are considered diagnosable with mental illness. Mental illnesses are the leading cause of disability in the US and Canada. Examples include, schizophrenia, ADHD, major depressive disorder, bipolar disorder, anxiety disorder, post-traumatic stress disorder and autism.Many teens suffer from mental health issues in response to the pressures of society and social problems they encounter. Some of the key mental health issues seen in teens are: depression, eating disorders, and drug abuse. There are many ways to prevent these health issues from occurring such as communicating well with a teen suffering from mental health issues. Mental health can be treated and be attentive to teens' behavior. Many factors contribute to mental health problems, including:Biological factors, such as genes or brain chemistryLife experiences, such as trauma or abuseFamily history of mental health problemsAchieving and maintaining health is an ongoing process, shaped by both the evolution of health care knowledge and practices as well as personal strategies and organized interventions for staying healthy.An important way to maintain your personal health is to have a healthy diet. A healthy diet includes a variety of plant-based and animal-based foods that provide nutrients to your body. Such nutrients give you energy and keep your body running. Nutrients help build and strengthen bones, muscles, and tendons and also regulate body processes (i.e. blood pressure). The food guide pyramid is a pyramid-shaped guide of healthy foods divided into sections. Each section shows the recommended intake for each food group (i.e. Protein, Fat, Carbohydrates, and Sugars). Making healthy food choices is important because it can lower your risk of heart disease, developing some types of cancer, and it will contribute to maintaining a healthy weight.The Mediterranean diet is commonly associated with health-promoting effects due to the fact that it contains some bioactive compounds like phenolic compounds, isoprenoids and alkaloids.Physical exercise enhances or maintains physical fitness and overall health and wellness. It strengthens muscles and improves the cardiovascular system. According to the National Institute of Health (NIH) there are four types of exercise Endurance, Strength, Flexibility, and Balance. Endurance exercises are those that will elevate your heart rate including walking, jogging, running, hiking etc. Sleep is an essential component to maintaining health. In children, sleep is also vital for growth and development. Ongoing sleep deprivation has been linked to an increased risk for some chronic health problems. In addition, sleep deprivation has been shown to correlate with both increased susceptibility to illness and slower recovery times from illness. In one study, people with chronic insufficient sleep, set as six hours of sleep a night or less, were found to be four times more likely to catch a cold compared to those who reported sleeping for seven hours or more a night. Due to the role of sleep in regulating metabolism, insufficient sleep may also play a role in weight gain or, conversely, in impeding weight loss. Additionally, in 2007, the International Agency for Research on Cancer, which is the cancer research agency for the World Health Organization, declared that ""shiftwork that involves circadian disruption is probably carcinogenic to humans,"" speaking to the dangers of long-term nighttime work due to its intrusion on sleep. In 2015, the National Sleep Foundation released updated recommendations for sleep duration requirements based on age and concluded that ""Individuals who habitually sleep outside the normal range may be exhibiting signs or symptoms of serious health problems or, if done volitionally, may be compromising their health and well-being.""Health science is the branch of science focused on health. There are two main approaches to health science: the study and research of the body and health-related issues to understand how humans (and animals) function, and the application of that knowledge to improve health and to prevent and cure diseases and other physical and mental impairments. The science builds on many sub-fields, including biology, biochemistry, physics, epidemiology, pharmacology, medical sociology. Applied health sciences endeavor to better understand and improve human health through applications in areas such as health education, biomedical engineering, biotechnology and public health.Organized interventions to improve health based on the principles and procedures developed through the health sciences are provided by practitioners trained in medicine, nursing, nutrition, pharmacy, social work, psychology, occupational therapy, physical therapy and other health care professions. Clinical practitioners focus mainly on the health of individuals, while public health practitioners consider the overall health of communities and populations. Workplace wellness programs are increasingly adopted by companies for their value in improving the health and well-being of their employees, as are school health services in order to improve the health and well-being of children.Public health has been described as ""the science and art of preventing disease, prolonging life and promoting health through the organized efforts and informed choices of society, organizations, public and private, communities and individuals."" It is concerned with threats to the overall health of a community based on population health analysis. The population in question can be as small as a handful of people or as large as all the inhabitants of several continents (for instance, in the case of a pandemic). Public health has many sub-fields, but typically includes the interdisciplinary categories of epidemiology, biostatistics and health services. Environmental health, community health, behavioral health, and occupational health are also important areas of public health.The focus of public health interventions is to prevent and manage diseases, injuries and other health conditions through surveillance of cases and the promotion of healthy behavior, communities, and (in aspects relevant to human health) environments. Its aim is to prevent health problems from happening or re-occurring by implementing educational programs, developing policies, administering services and conducting research. In many cases, treating a disease or controlling a pathogen can be vital to preventing it in others, such as during an outbreak. Vaccination programs and distribution of condoms to prevent the spread of communicable diseases are examples of common preventive public health measures, as are educational campaigns to promote vaccination and the use of condoms (including overcoming resistance to such).Public health also takes various actions to limit the health disparities between different areas of the country and, in some cases, the continent or world. One issue is the access of individuals and communities to health care in terms of financial, geographical or socio-cultural constraints to accessing and using services. Applications of the public health system include the areas of maternal and child health, health services administration, emergency response, and prevention and control of infectious and chronic diseases.The great positive impact of public health programs is widely acknowledged. Due in part to the policies and actions developed through public health, the 20th century registered a decrease in the mortality rates for infants and children and a continual increase in life expectancy in most parts of the world. For example, it is estimated that life expectancy has increased for Americans by thirty years since 1900, and worldwide by six years since 1990.Personal health depends partially on the active, passive, and assisted cues people observe and adopt about their own health. These include personal actions for preventing or minimizing the effects of a disease, usually a chronic condition, through integrative care. They also include personal hygiene practices to prevent infection and illness, such as bathing and washing hands with soap brushing and flossing teeth storing, preparing and handling food safely and many others. The information gleaned from personal observations of daily living – such as about sleep patterns, exercise behavior, nutritional intake and environmental features – may be used to inform personal decisions and actions (e.g., ""I feel tired in the morning so I am going to try sleeping on a different pillow""), as well as clinical decisions and treatment plans (e.g., a patient who notices his or her shoes are tighter than usual may be having exacerbation of left-sided heart failure, and may require diuretic medication to reduce fluid overload).Personal health also depends partially on the social structure of a person's life. The maintenance of strong social relationships, volunteering, and other social activities have been linked to positive mental health and also increased longevity. One American study among seniors over age 70, found that frequent volunteering was associated with reduced risk of dying compared with older persons who did not volunteer, regardless of physical health status. Another study from Singapore reported that volunteering retirees had significantly better cognitive performance scores, fewer depressive symptoms, and better mental well-being and life satisfaction than non-volunteering retirees.Prolonged psychological stress may negatively impact health, and has been cited as a factor in cognitive impairment with aging, depressive illness, and expression of disease. Stress management is the application of methods to either reduce stress or increase tolerance to stress. Relaxation techniques are physical methods used to relieve stress. Psychological methods include cognitive therapy, meditation, and positive thinking, which work by reducing response to stress. Improving relevant skills, such as problem solving and time management skills, reduces uncertainty and builds confidence, which also reduces the reaction to stress-causing situations where those skills are applicable.In addition to safety risks, many jobs also present risks of disease, illness and other long-term health problems. Among the most common occupational diseases are various forms of pneumoconiosis, including silicosis and coal worker's pneumoconiosis (black lung disease). Asthma is another respiratory illness that many workers are vulnerable to. Workers may also be vulnerable to skin diseases, including eczema, dermatitis, urticaria, sunburn, and skin cancer. Other occupational diseases of concern include carpal tunnel syndrome and lead poisoning.As the number of service sector jobs has risen in developed countries, more and more jobs have become sedentary, presenting a different array of health problems than those associated with manufacturing and the primary sector. Contemporary problems, such as the growing rate of obesity and issues relating to stress and overwork in many countries, have further complicated the interaction between work and health.Many governments view occupational health as a social challenge and have formed public organizations to ensure the health and safety of workers. Examples of these include the British Health and Safety Executive and in the United States, the National Institute for Occupational Safety and Health, which conducts research on occupational health and safety, and the Occupational Safety and Health Administration, which handles regulation and policy relating to worker safety and health.Men's healthWomen's healthYouth healthPopulation healthPublic healthDisease burdenHealth careHealth systemMedicineHuman enhancementOne HealthHealingEnvironmental health Media related to Health at Wikimedia Commons";environmental disaster;Human health;0
"Human impact on the environment or anthropogenic impact on the environment includes changes to biophysical environments and ecosystems, biodiversity, and natural resources caused directly or indirectly by humans, including global warming, environmental degradation (such as ocean acidification), mass extinction and biodiversity loss, ecological crises, and ecological collapse. Modifying the environment to fit the needs of society is causing severe effects, which become worse as the problem of human overpopulation continues. Some human activities that cause damage (either directly or indirectly) to the environment on a global scale include human reproduction, overconsumption, overexploitation, pollution, and deforestation, to name but a few. Some of the problems, including global warming and biodiversity loss pose an existential risk to the human race, and overpopulation causes those problems.The term anthropogenic designates an effect or object resulting from human activity. The term was first used in the technical sense by Russian geologist Alexey Pavlov, and it was first used in English by British ecologist Arthur Tansley in reference to human influences on climax plant communities. The atmospheric scientist Paul Crutzen introduced the term ""Anthropocene"" in the mid-1970s. The term is sometimes used in the context of pollution emissions that are produced from human activity but also applies broadly to all major human impacts on the environment.David Attenborough described the level of human population on the planet as a multiplier of all other environmental problems. In 2013, he described humanity as ""a plague on the Earth"" that needs to be controlled by limiting population growth.Some deep ecologists, such as the radical thinker and polemicist Pentti Linkola, see human overpopulation as a threat to the entire biosphere. In 2017, over 15,000 scientists around the world issued a second warning to humanity which asserted that rapid human population growth is the ""primary driver behind many ecological and even societal threats.""Overconsumption is a situation where resource use has outpaced the sustainable capacity of the ecosystem. A prolonged pattern of overconsumption leads to environmental degradation and the eventual loss of resource bases.Humanity's overall impact on the planet is affected by many factors, not just the raw number of people. Their lifestyle (including overall affluence and resource utilization) and the pollution they generate (including carbon footprint) are equally important. In 2008, The New York Times stated that the inhabitants of the developed nations of the world consume resources like oil and metals at a rate almost 32 times greater than those of the developing world, who make up the majority of the human population.The effects of overpopulation are compounded by overconsumption. According to Paul R. Ehrlich:Rich western countries are now siphoning up the planet’s resources and destroying its ecosystems at an unprecedented rate. We want to build highways across the Serengeti to get more rare earth minerals for our cellphones. We grab all the fish from the sea, wreck the coral reefs and put carbon dioxide into the atmosphere. We have triggered a major extinction event... A world population of around a billion would have an overall pro-life effect. This could be supported for many millennia and sustain many more human lives in the long term compared with our current uncontrolled growth and prospect of sudden collapse... If everyone consumed resources at the US level – which is what the world aspires to – you will need another four or five Earths. We are wrecking our planet’s life support systems.Humanity has caused the loss of 83% of all wild mammals and half of plants   The world’s chickens are triple the weight of all the wild birds, while domesticated cattle and pigs outweigh all wild mammals by 14 to 1.The applications of technology often result in unavoidable and unexpected environmental impacts, which according to the I = PAT equation is measured as resource use or pollution generated per unit GDP. Environmental impacts caused by the application of technology are often perceived as unavoidable for several reasons. First, given that the purpose of many technologies is to exploit, control, or otherwise “improve” upon nature for the perceived benefit of humanity while at the same time the myriad of processes in nature have been optimized and are continually adjusted by evolution, any disturbance of these natural processes by technology is likely to result in negative environmental consequences. Second, the conservation of mass principle and the first law of thermodynamics (i.e., conservation of energy) dictate that whenever material resources or energy are moved around or manipulated by technology, environmental consequences are inescapable. Third, according to the second law of thermodynamics, order can be increased within a system (such as the human economy) only by increasing disorder or entropy outside the system (i.e., the environment). Thus, technologies can create “order” in the human economy (i.e., order as manifested in buildings, factories, transportation networks, communication systems, etc.) only at the expense of increasing “disorder” in the environment. According to a number of studies, increased entropy is likely to be correlated to negative environmental impacts.The environmental impact of agriculture varies based on the wide variety of agricultural practices employed around the world. Ultimately, the environmental impact depends on the production practices of the system used by farmers. The connection between emissions into the environment and the farming system is indirect, as it also depends on other climate variables such as rainfall and temperature.There are two types of indicators of environmental impact: ""means-based"", which is based on the farmer's production methods, and ""effect-based"", which is the impact that farming methods have on the farming system or on emissions to the environment. An example of a means-based indicator would be the quality of groundwater that is affected by the amount of nitrogen applied to the soil. An indicator reflecting the loss of nitrate to groundwater would be effect-based.The environmental impact of agriculture involves a variety of factors from the soil, to water, the air, animal and soil diversity, plants, and the food itself. Some of the environmental issues that are related to agriculture are climate change, deforestation, genetic engineering, irrigation problems, pollutants, soil degradation, and waste.The environmental impact of fishing can be divided into issues that involve the availability of fish to be caught, such as overfishing, sustainable fisheries, and fisheries management and issues that involve the impact of fishing on other elements of the environment, such as by-catch and destruction of habitat such as coral reefs.These conservation issues are part of marine conservation, and are addressed in fisheries science programs. There is a growing gap between how many fish are available to be caught and humanity’s desire to catch them, a problem that gets worse as the world population grows.Similar to other environmental issues, there can be conflict between the fishermen who depend on fishing for their livelihoods and fishery scientists who realize that if future fish populations are to be sustainable then some fisheries must reduce or even close.The journal Science published a four-year study in November 2006, which predicted that, at prevailing trends, the world would run out of wild-caught seafood in 2048. The scientists stated that the decline was a result of overfishing, pollution and other environmental factors that were reducing the population of fisheries at the same time as their ecosystems were being degraded. Yet again the analysis has met criticism as being fundamentally flawed, and many fishery management officials, industry representatives and scientists challenge the findings, although the debate continues. Many countries, such as Tonga, the United States, Australia and New Zealand, and international management bodies have taken steps to appropriately manage marine resources.The environmental impact of irrigation includes the changes in quantity and quality of soil and water as a result of irrigation and the ensuing effects on natural and social conditions at the tail-end and downstream of the irrigation scheme.The impacts stem from the changed hydrological conditions owing to the installation and operation of the scheme.An irrigation scheme often draws water from the river and distributes it over the irrigated area. As a hydrological result it is found that:the downstream river discharge is reducedthe evaporation in the scheme is increasedthe groundwater recharge in the scheme is increasedthe level of the water table risesthe drainage flow is increased.These may be called direct effects.Effects on soil and water quality are indirect and complex, and subsequent impacts on natural, ecological and socio-economic conditions are intricate.  In some, but not all instances, water logging and soil salinization can result.  However, irrigation can also be used, together with soil drainage, to overcome soil salinization by leaching excess salts from the vicinity of the root zone.Irrigation can also be done extracting groundwater by (tube)wells. As a hydrological result it is found that the level of the water descends. The effects may be water mining, land/soil subsidence, and, along the coast, saltwater intrusion.Irrigation projects can have large benefits, but the negative side effects are often overlooked.Agricultural irrigation technologies such as high powered water pumps, dams, and pipelines are responsible for the large-scale depletion of fresh water resources such as aquifers, lakes, and rivers.  As a result of this massive diversion of freshwater, lakes, rivers, and creeks are running dry, severely altering or stressing surrounding ecosystems, and contributing to the extinction of many aquatic species.Lal and Stewart estimated global loss of agricultural land by degradation and abandonment at 12 million hectares per year.    In contrast, according to Scherr, GLASOD (Global Assessment of Human-Induced Soil Degradation, under the UN Environment Programme) estimated that 6 million hectares of agricultural land per year had been lost to soil degradation since the mid-1940s, and she noted that this magnitude is similar to earlier estimates by Dudal and by Rozanov et al.  Such losses are attributable not only to soil erosion, but also to salinization, loss of nutrients and organic matter, acidification, compaction, water logging and subsidence.  Human-induced land degradation tends to be particularly serious in dry regions. Focusing on soil properties, Oldeman estimated that about 19 million square kilometers of global land area had been degraded Dregne and Chou, who included degradation of vegetation cover as well as soil, estimated about 36 million square kilometers degraded in the world’s dry regions. Despite estimated losses of agricultural land, the amount of arable land used in crop production globally increased by about 9% from 1961 to 2012, and is estimated to have been 1.396 billion hectares in 2012.Global average soil erosion rates are thought to be high, and erosion rates on conventional cropland generally exceed estimates of soil production rates, usually by more than an order of magnitude. In the US, sampling for erosion estimates by the US NRCS (Natural Resources Conservation Service) is statistically based, and estimation uses the Universal Soil Loss Equation and Wind Erosion Equation. For 2010, annual average soil loss by sheet, rill and wind erosion on non-federal US land was estimated to be 10.7 t/ha on cropland and 1.9 t/ha on pasture land the average soil erosion rate on US cropland had been reduced by about 34% since 1982. No-till and low-till practices have become increasingly common on North American cropland used for production of grains such as wheat and barley. On uncultivated cropland, the recent average total soil loss has been 2.2 t/ha per year.  In comparison with agriculture using conventional cultivation, it has been suggested that, because no-till agriculture produces erosion rates much closer to soil production rates, it could provide a foundation for sustainable agriculture.Environmental impacts associated with meat production include use of fossil energy, water and land resources, greenhouse gas emissions, and in some instances, rainforest clearing, water pollution and species endangerment, among other adverse effects. Steinfeld et al. of the FAO estimated that 18% of global anthropogenic GHG (greenhouse gas) emissions (estimated as 100-year carbon dioxide equivalents) are associated in some way with livestock production.  A more recent FAO analysis estimated that all agriculture, including the livestock sector, in 2011 accounted for 12% of global anthropogenic GHG emissions expressed as 100-year carbon dioxide equivalents. Similarly, the Intergovernmental Panel on Climate Change has estimated that about 10 to 12% of global anthropogenic GHG emissions (expressed as 100-year carbon dioxide equivalents) were assignable to all of agriculture, including the livestock sector, in 2005  and again in 2010.  The percentage assignable to livestock would be some fraction of the percentage for agriculture.  The amount assignable to meat production would be some fraction of that assigned to livestock.  FAO data indicate that meat accounted for 26% of global livestock product tonnage in 2011. However, many estimates use different sectoral assignment of some emissions. Environmental specialists Jeff Anhang and Robert Goodland with the IFC and World Bank, have put the GHG associated with livestock at 51%, pointing out the FAO report failed to account for the 8,769 metric tons of respiratory CO2 produced each year, undercounted methane production and land use associated with livestock, and failed to properly categorize emissions related to the slaughtering, processing, packaging, storing and transporting of animals and animal products.Globally, enteric fermentation (mostly in ruminant livestock) accounts for about 27% of anthropogenic methane emissions,  Despite methane’s 100-year global warming potential, recently estimated at 28 without and 34 with climate carbon feedbacks, methane emission is currently contributing relatively little to global warming.  Over the decade 2000 through 2009, atmospheric methane content increased by an average of only 6 Tg per year (because nearly all natural and anthropogenic methane emission was offset by degradation), while atmospheric carbon dioxide increased by nearly 15,000 Tg per year.  At the currently estimated rate of methane degradation, slight reduction of anthropogenic methane emissions, to about 98% of that decade’s average, would be expected to result in no further increase of atmospheric methane content.  Although reduction of methane emissions would have a rapid effect on warming, the expected effect would be small. Other anthropogenic GHG emissions associated with livestock production include carbon dioxide from fossil fuel consumption (mostly for production, harvesting and transport of feed), and nitrous oxide emissions associated with use of nitrogenous fertilizers, growing of nitrogen-fixing legume vegetation and manure management. Management practices that can mitigate GHG emissions from production of livestock and feed have been identified.Livestock production, including feed production and grazing, uses about 30% of the earth’s ice-free terrestrial surface: about 26% for grazing and about 4% for other feed production.  The intensity and duration of grazing use vary greatly  and these, together with terrain, vegetation and climate, influence the nature and importance of grazing’s environmental impact, which can range from severe to negligible, and in some cases (as noted below) beneficial.  Excessive use of vegetation by grazing can be especially conducive to land degradation in dry areas.Considerable water use is associated with meat production, mostly because of water used in production of vegetation that provides feed.  There are several published estimates of water use associated with livestock and meat production, but the amount of water use assignable to such production is seldom estimated.  For example, “green water” use is evapotranspirational use of soil water that has been provided directly by precipitation and “green water” has been estimated to account for 94% of global beef cattle production’s “water footprint”, and on rangeland, as much as 99.5% of the water use associated with beef production is “green water”.  However, it would be misleading simply to assign that associated rangeland green water use to beef production, partly because that evapotranspirational use occurs even in the absence of cattle. Even when cattle are present, most of that associated water use can be considered assignable to production of terrestrial environmental values, because it produces root and residue biomass important for erosion control, stabilization of soil structure, nutrient cycling, carbon sequestration, support of numerous primary consumers, many of which support higher trophic levels, etc.  Withdrawn water (from surface and groundwater sources) is used for livestock watering, and in some cases is also used for irrigation of forage and feed crops.  Whereas all irrigation in the US (including loss in conveyance) is estimated to account for about 38% of US withdrawn freshwater use, irrigation water for production of livestock feed and forage has been estimated to account for about 9% other withdrawn freshwater use for the livestock sector (for drinking, washdown of facilities, etc.) is estimated at about 0.7%.  Because of the preponderance of non-meat products from the livestock sector only some fraction of this water use is assignable to meat production.Impairment of water quality by manure and other substances in runoff and infiltrating water is a concern, especially where intensive livestock production is carried out.  In the US, in a comparison of 32 industries, the livestock industry was found to have a relatively good record of compliance with environmental regulations pursuant to the Clean Water Act and Clean Air Act, but pollution issues from large livestock operations can sometimes be serious where violations occur.  Various measures have been suggested by the US Environmental Protection Agency, among others, which can help reduce livestock damage to streamwater quality and riparian environments.Data of a USDA study indicate that, in 2002, about 0.6% of non-solar energy use in the United States was accounted for by production of meat-producing livestock and poultry.  This estimate included embodied energy used in production, such as energy used in manufacture and transport of fertilizer for feed production.  (Non-solar energy is specified, because solar energy is used in such processes as photosynthesis and hay-drying.)Changes in livestock production practices influence the environmental impact of meat production, as illustrated by some beef data.  In the US beef production system, practices prevailing in 2007 are estimated to have involved 8.6% less fossil fuel use, 16.3% less greenhouse gas emissions (estimated as 100-year carbon dioxide equivalents), 12.1% less withdrawn water use and 33.0% less land use, per unit mass of beef produced, than in 1977.  From 1980 to 2012 in the US, while population increased by 38%, the small ruminant inventory decreased by 42%, the cattle-and-calves inventory decreased by 17%, and methane emissions from livestock decreased by 18%  yet despite the reduction in cattle numbers, US beef production increased over that period.Some impacts of meat-producing livestock may be considered environmentally beneficial.  These include waste reduction by conversion of human-inedible crop residues to food, use of livestock as an alternative to herbicides for control of invasive and noxious weeds and other vegetation management, use of animal manure as fertilizer as a substitute for those synthetic fertilizers that require considerable fossil fuel use for manufacture, grazing use for wildlife habitat enhancement, and carbon sequestration in response to grazing practices, among others. Conversely, according to some studies appearing in peer-reviewed journals the growing demand for meat is contributing to significant biodiversity loss as it is a significant driver of deforestation and habitat destruction.Palm oil, produced from the oil palm, is a basic source of income for many farmers in Southeast Asia, Central and West Africa, and Central America. It is locally used as a cooking oil, exported for use in many commercial food and personal care products and is converted into biofuel. It produces up to 10 times more oil per unit area as soyabeans, rapeseed or sunflowers. Oil palms produce 38% of vegetable oil output on 5% of the world’s vegetable-oil farmland. Palm oil is under increasing scrutiny in relation to its effects on the environment.Introductions of species, particularly plants into new areas, by whatever means and for whatever reasons have brought about major and permanent changes to the environment over large areas.  Examples include the introduction of Caulerpa taxifolia into the Mediterranean, the introduction of oat species into the California grasslands, and the introduction of privet, kudzu, and purple loosestrife to North America.  Rats, cats, and goats have radically altered biodiversity in many islands.  Additionally, introductions have resulted in genetic changes to native fauna where interbreeding has taken place, as with buffalo with domestic cattle, and wolves with domestic dogs.The environmental impact of energy harvesting and consumption is diverse. In recent years there has been a trend towards the increased commercialization of various renewable energy sources.In the real world, consumption of fossil fuel resources leads to global warming and climate change. However, little change is being made in many parts of the world. If the peak oil theory proves true, more explorations of viable alternative energy sources, could be more friendly to the environment.Rapidly advancing technologies can achieve a transition of energy generation, water and waste management, and food production towards better environmental and energy usage practices using methods of systems ecology and industrial ecology.The environmental impact of biodiesel includes energy use, greenhouse gas emissions and some other kinds of pollution.  A joint life cycle analysis by the US Department of Agriculture and the US Department of Energy found that substituting 100% biodiesel for petroleum diesel in buses reduced life cycle consumption of petroleum by 95%.  Biodiesel reduced net emissions of carbon dioxide by 78.45%, compared with petroleum diesel.  In urban buses, biodiesel reduced particulate emissions 32 percent, carbon monoxide emissions 35 percent, and emissions of sulfur oxides 8%, relative to life cycle emissions associated with use of petroleum diesel.  Life cycle emissions of hydrocarbons were 35% higher and emission of various nitrogen oxides (NOx) were 13.5% higher with biodiesel.  Life cycle analyses by the Argonne National Laboratory have indicated reduced fossil energy use and reduced greenhouse gas emissions with biodiesel, compared with petroleum diesel use.  Biodiesel derived from various vegetable oils (e.g. canola or soybean oil), is readily biodegradable in the environment compared with petroleum diesel.The environmental impact of coal mining and -burning is diverse. Legislation passed by the US Congress in 1990 required the United States Environmental Protection Agency (EPA) to issue a plan to alleviate toxic air pollution from coal-fired power plants. After delay and litigation, the EPA now has a court-imposed deadline of March 16, 2011, to issue its report.The environmental impact of electricity generation is significant because modern society uses large amounts of electrical power. This power is normally generated at power plants that convert some other kind of energy into electricity. Each such system has advantages and disadvantages, but many of them pose environmental concerns.The environmental impact of nuclear power results from the nuclear fuel cycle processes including mining, processing, transporting and storing fuel and radioactive fuel waste. Released radioisotopes pose a health danger to human populations, animals and plants as radioactive particles enter organisms through various transmission routes.Radiation is a carcinogen and causes numerous effects on living organisms and systems. The environmental impacts of nuclear power plant disasters such as the Chernobyl disaster, the Fukushima Daiichi nuclear disaster and the Three Mile Island accident, among others, persist indefinitely, though several other factors contributed to these events including improper management of fail safe systems and natural disasters putting uncommon stress on the generators. The radioactive decay rate of particles varies greatly, dependent upon the nuclear properties of a particular isotope. Radioactive Plutonium-244 has a half-life of 80.8 million years, which indicates the time duration required for half of a given sample to decay, though very little plutonium-244 is produced in the nuclear fuel cycle and lower half-life materials have lower activity thus giving off less dangerous radiation.The environmental impact of the oil shale industry includes the consideration of issues such as land use, waste management, and water and air pollution caused by the extraction and processing of oil shale. Surface mining of oil shale deposits causes the usual environmental impacts of open-pit mining. In addition, the combustion and thermal processing generate waste material, which must be disposed of, and harmful atmospheric emissions, including carbon dioxide, a major greenhouse gas. Experimental in-situ conversion processes and carbon capture and storage technologies may reduce some of these concerns in future, but may raise others, such as the pollution of groundwater.The environmental impact of petroleum is often negative because it is toxic to almost all forms of life. Petroleum, a common word for oil or natural gas, is closely linked to virtually all aspects of present society, especially for transportation and heating for both homes and for commercial activities.The environmental impact of reservoirs is coming under ever increasing scrutiny as the world demand for water and energy increases and the number and size of reservoirs increases.Dams and the reservoirs can be used to supply drinking water, generate hydroelectric power, increasing the water supply for irrigation, provide recreational opportunities and flood control. However, adverse environmental and sociological impacts have also been identified during and after many reservoir constructions. Although the impact varies greatly between different dams and reservoirs, common criticisms include preventing sea-run fish from reaching their historical mating grounds, less access to water downstream, and a smaller catch for fishing communities in the area. Advances in technology have provided solutions to many negative impacts of dams but these advances are often not viewed as worth investing in if not required by law or under the threat of fines. Whether reservoir projects are ultimately beneficial or detrimental—to both the environment and surrounding human populations— has been debated since the 1960s and probably long before that. In 1960 the construction of Llyn Celyn and the flooding of Capel Celyn provoked political uproar which continues to this day. More recently, the construction of Three Gorges Dam and other similar projects throughout Asia, Africa and Latin America have generated considerable environmental and political debate.Compared to the environmental impact of traditional energy sources, the environmental impact of wind power is relatively minor. Wind powered electricity generation consumes no fuel, and emits no air pollution, unlike fossil fuel power sources. The energy consumed to manufacture and transport the materials used to build a wind power plant is equal to the new energy produced by the plant within a few months. While a wind farm may cover a large area of land, many land uses such as agriculture are compatible, with only small areas of turbine foundations and infrastructure made unavailable for use.There are reports of bird and bat mortality at wind turbines, as there are around other artificial structures. The scale of the ecological impact may or may not be significant, depending on specific circumstances. Prevention and mitigation of wildlife fatalities, and protection of peat bogs, affect the siting and operation of wind turbines.There are conflicting reports about the effects of noise on people who live very close to a wind turbine.Artificial light at night is one of the most obvious physical changes that humans have made to the biosphere, and is the easiest form of pollution to observe from space. The main environmental impacts of artificial light are due to light's use as an information source (rather than an energy source). The hunting efficiency of visual predators generally increases under artificial light, changing predator prey interactions. Artificial light also affects dispersal, orientation, migration, and hormone levels, resulting in disrupted circadian rhythms.The environmental impact of cleaning agents is diverse. In recent years, measures have been taken to reduce these effects.Nanotechnology's environmental impact can be split into two aspects: the potential for nanotechnological innovations to help improve the environment, and the possibly novel type of pollution that nanotechnological materials might cause if released into the environment. As nanotechnology is an emerging field, there is great debate regarding to what extent industrial and commercial use of nanomaterials will affect organisms and ecosystems.The environmental impact of paint is diverse. Traditional painting materials and processes can have harmful effects on the environment, including those from the use of lead and other additives. Measures can be taken to reduce environmental impact, including accurately estimating paint quantities so that wastage is minimized, use of paints, coatings, painting accessories and techniques that are environmentally preferred. The United States Environmental Protection Agency guidelines and Green Star ratings are some of the standards that can be applied.The environmental impact of paper is significant, which has led to changes in industry and behaviour at both business and personal levels. With the use of modern technology such as the printing press and the highly mechanised harvesting of wood, paper has become a cheap commodity. This has led to a high level of consumption and waste. With the rise in environmental awareness due to the lobbying by environmental organizations and with increased government regulation there is now a trend towards sustainability in the pulp and paper industry.Some scientists suggest that by 2050 there could be more plastic than fish in the oceans.The environmental impact of pesticides is often greater than what is intended by those who use them. Over 98% of sprayed insecticides and 95% of herbicides reach a destination other than their target species, including nontarget species, air, water, bottom sediments, and food. Pesticide contaminates land and water when it escapes from production sites and storage tanks, when it runs off from fields, when it is discarded, when it is sprayed aerially, and when it is sprayed into water to kill algae.The amount of pesticide that migrates from the intended application area is influenced by the particular chemical's properties: its propensity for binding to soil, its vapor pressure, its water solubility, and its resistance to being broken down over time. Factors in the soil, such as its texture, its ability to retain water, and the amount of organic matter contained in it, also affect the amount of pesticide that will leave the area. Some pesticides contribute to global warming and the depletion of the ozone layer.The environmental impact of pharmaceuticals and personal care products (PPCPs) is largely speculative. PPCPs are substances used by individuals for personal health or cosmetic reasons and the products used by agribusiness to boost growth or health of livestock. PPCPs have been detected in water bodies throughout the world. The effects of these chemicals on humans and the environment are not yet known, but to date there is no scientific evidence that they affect human health.The environmental impact of mining includes erosion, formation of sinkholes, loss of biodiversity, and contamination of soil, groundwater and surface water by chemicals from mining processes. In some cases, additional forest logging is done in the vicinity of mines to increase the available room for the storage of the created debris and soil. Besides creating environmental damage, the contamination resulting from leakage of chemicals also affect the health of the local population. Mining companies in some countries are required to follow environmental and rehabilitation codes, ensuring the area mined is returned to close to its original state. Some mining methods may have significant environmental and public health effects.The environmental impact of transport is significant because it is a major user of energy, and burns most of the world's petroleum. This creates air pollution, including nitrous oxides and particulates, and is a significant contributor to global warming through emission of carbon dioxide, for which transport is the fastest-growing emission sector. By subsector, road transport is the largest contributor to global warming.Environmental regulations in developed countries have reduced the individual vehicles emission however, this has been offset by an increase in the number of vehicles, and more use of each vehicle. Some pathways to reduce the carbon emissions of road vehicles considerably have been studied. Energy use and emissions vary largely between modes, causing environmentalists to call for a transition from air and road to rail and human-powered transport, and increase transport electrification and energy efficiency.Other environmental impacts of transport systems include traffic congestion and automobile-oriented urban sprawl, which can consume natural habitat and agricultural lands. By reducing transportation emissions globally, it is predicted that there will be significant positive effects on Earth's air quality, acid rain, smog and climate change.The health impact of transport emissions is also of concern. A recent survey of the studies on the effect of traffic emissions on pregnancy outcomes has linked exposure to emissions to adverse effects on gestational duration and possibly also intrauterine growth.The environmental impact of aviation occurs because aircraft engines emit noise, particulates, and gases which contribute to climate change and global dimming. Despite emission reductions from automobiles and more fuel-efficient and less polluting turbofan and turboprop engines, the rapid growth of air travel in recent years contributes to an increase in total pollution attributable to aviation. In the EU, greenhouse gas emissions from aviation increased by 87% between 1990 and 2006. Among other factors leading to this phenomenon are the increasing number of hypermobile travellers and social factors that are making air travel commonplace, such as frequent flyer programs.There is an ongoing debate about possible taxation of air travel and the inclusion of aviation in an emissions trading scheme, with a view to ensuring that the total external costs of aviation are taken into account.The environmental impact of roads includes the local effects of highways (public roads) such as on noise, light pollution, water pollution, habitat destruction/disturbance and local air quality and the wider effects including climate change from vehicle emissions. The design, construction and management of roads, parking and other related facilities as well as the design and regulation of vehicles can change the impacts to varying degrees.The environmental impact of shipping includes greenhouse gas emissions and oil pollution. In 2007, carbon dioxide emissions from shipping were estimated at 4 to 5% of the global total, and estimated by the International Maritime Organisation (IMO) to rise by up to 72% by 2020 if no action is taken. There is also a potential for introducing invasive species into new areas through shipping, usually by attaching themselves to the ship's hull.The First Intersessional Meeting of the IMO Working Group on Greenhouse Gas Emissions from Ships took place in Oslo, Norway on 23–27 June 2008. It was tasked with developing the technical basis for the reduction mechanisms that may form part of a future IMO regime to control greenhouse gas emissions from international shipping, and a draft of the actual reduction mechanisms themselves, for further consideration by IMO’s Marine Environment Protection Committee (MEPC).General military spending and military activities have marked environmental effects. The United States military is considered one of the worst polluters in the world, responsible for over 39,000 sites contaminated with hazardous materials. Several studies have also found a strong positive correlation between higher military spending and higher carbon emissions where increased military spending has a larger effect on increasing carbon emissions in the Global North than in the Global South. Military activities also affect land use and are extremely resource-intensive.The military does not solely have negative effects on the environment. There are several examples of militaries aiding in land management, conservation, and greening of an area. Additionally, certain military technologies have proven extremely helpful for conservationists and environmental scientists.As well as the cost to human life and society, there is a significant environmental impact of war. Scorched earth methods during, or after war have been in use for much of recorded history but with modern technology war can cause a far greater devastation on the environment. Unexploded ordnance can render land unusable for further use or make access across it dangerous or fatal.Human activity is causing environmental degradation, which is the deterioration of the environment through depletion of resources such as air, water and soil the destruction of ecosystems habitat destruction the extinction of wildlife and pollution. It is defined as any change or disturbance to the environment perceived to be deleterious or undesirable. As indicated by the I=PAT equation, environmental impact (I) or degradation is caused by the combination of an already very large and increasing human population (P), continually increasing economic growth or per capita affluence (A), and the application of resource-depleting and polluting technology (T).Biodiversity generally refers to the variety and variability of life on Earth, and is represented by the number of different species there are on the planet. Since its introduction, Homo sapiens (the human species) has been killing off entire species either directly (such as through hunting) or indirectly (such as by destroying habitats), causing the extinction of species at an alarming rate. Humans are the cause of the current mass extinction, called the Holocene extinction, driving extinctions to 100 to 1000 times the normal background rate. Though most experts agree that human beings have accelerated the rate of species extinction, some scholars have postulated without humans, the biodiversity of the Earth would grow at an exponential rate rather than decline. The Holocene extinction continues, with meat consumption, overfishing, ocean acidification and the amphibian crisis being a few broader examples of an almost universal, cosmopolitan decline in biodiversity. Human overpopulation (and continued population growth) along with profligate consumption are considered to be the primary drivers of this rapid decline. A 2017 statement by 15,364 scientists from 184 countries warned that, among other things, this sixth extinction event unleashed by humanity could annihilate many current life forms and consign them to extinction by the end of this century.Defaunation is the loss of animals from ecological communities.It is estimated that more than 50 percent of all wildlife has been lost in the last 40 years. It is estimated that by 2020, 68% of the world's wildlife will be lost. In South America, there is believed to be a 70 percent loss. A May 2018 study published in PNAS found that 83% of wild mammals, 80% of marine mammals, 50% of plants and 15% of fish have been lost since the dawn of human civilization. Currently, livestock make up 60% of all mammals on earth, followed by humans (36%) and wild mammals (4%).Because of human overpopulation, coral reefs are dying around the world. In particular, coral mining, pollution (organic and non-organic), overfishing, blast fishing and the digging of canals and access into islands and bays are serious threats to these ecosystems. Coral reefs also face high dangers from pollution, diseases, destructive fishing practices and warming oceans. In order to find answers for these problems, researchers study the various factors that impact reefs. The list of factors is long, including the ocean's role as a carbon dioxide sink, atmospheric changes, ultraviolet light, ocean acidification, biological virus, impacts of dust storms carrying agents to far flung reefs, pollutants, algal blooms and others. Reefs are threatened well beyond coastal areas.General estimates show approximately 10% world's coral reefs are already dead. It is estimated that about 60% of the world's reefs are at risk due to destructive, human-related activities. The threat to the health of reefs is particularly strong in Southeast Asia, where 80% of reefs are endangered.Global warming is the result of increasing atmospheric carbon dioxide concentrations which is caused primarily by the combustion of fossil energy sources such as petroleum, coal, and natural gas, and to an unknown extent by destruction of forests, increased methane, volcanic activity and cement production. Such massive alteration of the global carbon cycle has only been possible because of the availability and deployment of advanced technologies, ranging in application from fossil fuel exploration, extraction, distribution, refining, and combustion in power plants and automobile engines and advanced farming practices. Livestock contributes to climate change both through the production of greenhouse gases and through destruction of carbon sinks such as rain-forests. According to the 2006 United Nations/FAO report, 18% of all greenhouse gas emissions found in the atmosphere are due to livestock. The raising of livestock and the land needed to feed them has resulted in the destruction millions of acres of Rainforest and as global demand for meat rises, so too will the demand for land. Ninety-one percent of all rainforest land deforested since 1970 is now used for livestock. Potential negative environmental impacts caused by increasing atmospheric carbon dioxide concentrations are rising global air temperatures, altered hydrogeological cycles resulting in more frequent and severe droughts, storms, and floods, as well as sea level rise and ecosystem disruption.Land degradation is a process in which the value of the biophysical environment is affected by a combination of human-induced processes acting upon the land. It is viewed as any change or disturbance to the land perceived to be deleterious or undesirable. Natural hazards are excluded as a cause however human activities can indirectly affect phenomena such as floods and bush fires.This is considered to be an important topic of the 21st century due to the implications land degradation has upon agronomic productivity, the environment, and its effects on food security. It is estimated that up to 40% of the world's agricultural land is seriously degraded.Of particular concern is N2O, which has an average atmospheric lifetime of 114–120 years, and is 300 times more effective than CO2 as a greenhouse gas. NOx produced by industrial processes, automobiles and agricultural fertilization and NH3 emitted from soils (i.e., as an additional byproduct of nitrification) and livestock operations are transported to downwind ecosystems, influencing N cycling and nutrient losses. Six major effects of NOx and NH3 emissions have been identified:decreased atmospheric visibility due to ammonium aerosols (fine particulate matter [PM])elevated ozone concentrationsozone and PM affects human health (e.g. respiratory diseases, cancer)increases in radiative forcing and global climate changedecreased agricultural productivity due to ozone depositionecosystem acidification and eutrophication.Human impacts upon the environment, such as pollution and global warming, in turn affect human health.Diamond, Jared. Collapse: How Societies Choose to Fail or Succeed, Penguin Books, 2005 and 2011 (ISBN 9780241958681).Goudie, Andrew (2006). The human impact on the natural environment: past, present, and future. Wiley-Blackwell. ISBN 9781405127042. Ivanova, Diana Stadler, Konstantin Steen-Olsen, Kjartan Wood, Richard Vita, Gibran Tukker, Arnold Hertwich, Edgar G. (18 December 2015). ""Environmental Impact Assessment of Household Consumption"". Journal of Industrial Ecology. 20 (3): 526–536. doi:10.1111/jiec.12371. Linkola, Pentti (2011). Can Life Prevail? Arktos Media. ISBN 1907166637Lovelock, James (2009). The Vanishing Face of Gaia. Basic Books. ISBN 0465019072The Garden of Our Neglect: How Humans Shape the Evolution of Other Species July 5, 2012 Scientific AmericanSutherland W. et al. (2015). What Works in Conservation, Open Book Publishers, ISBN 9781783741571.Climate Science Special Report – U.S. Global Change Research ProgramThe Sixth Extinction on YouTube (PBS Digital Studios, November 17, 2014)Human activities that harm the Environment (Energy Physics)www.worldometers.infoEquation: Human Impact on Climate Change (2017) & Yale University";environmental disaster;Human impact on the environment;0
"The International Nuclear and Radiological Event Scale (INES) was introduced in 1990 by the International Atomic Energy Agency (IAEA) in order to enable prompt communication of safety-significant information in case of nuclear accidents.The scale is intended to be logarithmic, similar to the moment magnitude scale that is used to describe the comparative magnitude of earthquakes. Each increasing level represents an accident approximately ten times more severe than the previous level. Compared to earthquakes, where the event intensity can be quantitatively evaluated, the level of severity of a man-made disaster, such as a nuclear accident, is more subject to interpretation. Because of the difficulty of interpreting, the INES level of an incident is assigned well after the incident occurs. Therefore, the scale has a very limited ability to assist in disaster-aid deployment.As INES ratings are not assigned by a central body, high-profile nuclear incidents are sometimes assigned INES ratings by the operator, by the formal body of the country, but also by scientific institutes, international authorities or other experts which may lead to confusion as to the actual severity.A number of criteria and indicators are defined to assure coherent reporting of nuclear events by different official authorities. There are seven nonzero levels on the INES scale: three incident-levels and four accident-levels. There is also a level 0.The level on the scale is determined by the highest of three scores: off-site effects, on-site effects, and defence in depth degradation.There are also events of no safety relevance, characterized as ""out of scale"".Examples:17 November 2002, Natural Uranium Oxide Fuel Plant at the Nuclear Fuel Complex in Hyderabad, India: A chemical explosion at a fuel fabrication facility.29 September 1999: H.B. Robinson, United States: A tornado sighting within the protected area of the nuclear power plant.5 March 1999: San Onofre, United States: Discovery of suspicious item, originally thought to be a bomb, in nuclear power plant.Deficiencies in the existing INES have emerged through comparisons between the 1986 Chernobyl disaster, which had severe and widespread consequences to humans and the environment, and the 2011 Fukushima Daiichi nuclear accident, which caused no fatalities and comparatively small (10%) release of radiological material into the environment. The Fukushima Daiichi nuclear accident was originally rated as INES 5, but then upgraded to INES 7 (the highest level) when the events of units 1, 2 and 3 were combined into a single event and the combined release of radiological material was the determining factor for the INES rating. In light of that, nuclear experts say that the INES emergency scale is very likely to be revisited.One study found that the INES scale of the IAEA is highly inconsistent, and the scores provided by the IAEA incomplete, with many events not having an INES rating. Further, the actual accident damage values do not reflect the INES scores. A quantifiable, continuous scale might be preferable to the INES, in the same way that the antiquated Mercalli scale for earthquake magnitudes was superseded by the continuous physically-based Richter scale.The following arguments have been proposed: firstly, the scale is essentially a discrete qualitative ranking, not defined beyond event level 7. Secondly, it was designed as a public relations tool, not an objective scientific scale. Thirdly, its most serious shortcoming is that it conflates magnitude and intensity. An alternative nuclear accident magnitude scale (NAMS) was proposed by British nuclear safety expert David Smythe to address these issues.The Nuclear Accident Magnitude Scale (NAMS) is an alternative to INES, proposed by David Smythe in 2011 as a response to the Fukushima Daiichi nuclear disaster.  There were some concerns that INES was used in a confusing manner, and NAMS was intended to address the perceived INES shortcomings.As Smythe pointed out, the INES scale ends at 7 a more severe accident than Fukushima in 2011 or Chernobyl in 1986 cannot be measured by that scale. In addition, it is not continuous, not allowing a fine-grained comparison of nuclear incidents and accidents. But then, the most pressing item identified by Smythe is that INES conflates magnitude with intensity a distinction long made by seismologists to describe earthquakes. In that area, magnitude describes the physical energy released by an earthquake, while the intensity focuses on the effects of the earthquake. In analogy, a nuclear incident with a high magnitude (e.g. a core meltdown) may not result in an intense radioactive contamination, as the incident at the Swiss research reactor in Lucens shows – but yet it resides in INES category 5, together with the Windscale fire of 1957, which has caused significant contamination outside of the facility.The definition of the NAMS scale is:NAMS = log10(20 × R)with R being the radioactivity being released in terabecquerels, calculated as the equivalent dose of iodine-131. Furthermore, only the atmospheric release affecting the area outside the nuclear facility is considered for calculating the NAMS, giving a NAMS score of 0 to all incidents which do not affect the outside. The factor of 20 assures that both the INES and the NAMS scales reside in a similar range, aiding a comparison between accidents. An atmospheric release of any radioactivity will only occur in the INES categories 4 to 7, while NAMS does not have such a limitation.The NAMS scale does still not take into account the radioactive contamination of liquids such as an ocean, sea, river or groundwater pollution in proximity to any nuclear power plant. An estimation of its magnitude seems to be related to the problematic definition of a radiological equivalence between different type of involved isotopes and the variety of paths by which activity might eventually be ingested, e.g. eating fish or through the food chain.Nuclear Events Web-based System (NEWS), IAEAInternational Nuclear Event Scale factsheet, IAEA""International Nuclear Event Scale, User's manual"" (PDF). Archived from the original (PDF) on 19 March 2011. Retrieved 19 March 2011.  International Nuclear Event Scale, User's manual, IAEA, 2008";environmental disaster;International Nuclear Event Scale;0
"The International Standard Book Number (ISBN) is a unique numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The initial ISBN configuration of recognition was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines and the International Standard Music Number (ISMN) covers for musical scores.The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin, for the booksellers and stationers WHSmith and others in 1965. The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker (regarded as the ""Father of the ISBN"") and in 1968 in the United States by Emery Koltay (who later became director of the U.S. ISBN agency R.R. Bowker).The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108. The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.An SBN may be converted to an ISBN by prefixing the digit ""0"". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has ""SBN 340 01381 8"" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8 the check digit does not need to be re-calculated.Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with ""Bookland"" European Article Number EAN-13s.An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):for a 13-digit ISBN, a prefix element – a GS1 prefix: so far 978 or 979 have been made available by GS1,the registration group element (language-sharing country group, individual country or territory),the registrant element,the publication element, anda checksum character or check digit.A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.A full directory of ISBN agencies is available on the International ISBN Agency website. Partial listing:Australia: the commercial library services agency Thorpe-BowkerBrazil: The National Library of BrazilCanada (English): Library and Archives Canada, a government agencyCanada (French): Bibliothèque et Archives nationales du QuébecColombia: Cámara Colombiana del Libro, an NGOHong Kong: Books Registration Office (BRO), under the Hong Kong Public LibrariesIndia: The Raja Rammohun Roy National Agency for ISBN (Book Promotion and Copyright Division), under Department of Higher Education, a constituent of the Ministry of Human Resource DevelopmentIsrael: The Israel Center for LibrariesItaly: EDISER srl, owned by Associazione Italiana Editori (Italian Publishers Association)Maldives: The National Bureau of Classification (NBC)Malta: The National Book Council (Maltese: Il-Kunsill Nazzjonali tal-Ktieb)Morocco: The National Library of MoroccoNew Zealand: The National Library of New ZealandPakistan: National Library of PakistanPhilippines: National Library of the PhilippinesSouth Africa:National Library of South AfricaTurkey: General Directorate of Libraries and Publications, a branch of the Ministry of CultureUnited Kingdom and Republic of Ireland:Nielsen Book Services Ltd, part of Nielsen Holdings N.V.United States: R.R. Bowker.The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979). Registration group identifiers have primarily been allocated within the 978 prefix element. The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries 2 for French-speaking countries 3 for German-speaking countries 4 for Japan 5 for Russian-speaking countries and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976. Books published in rare languages typically have longer group identifiers.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN. The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN however, most bookstores only handle ISBN bearing publications.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes. Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements. Here are some sample ISBN-10 codes, illustrating block length variations.English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with ""0"" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an ""X"".The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.For example, for an ISBN-10 of 0-306-40615-2:                                                                        s                                                            =                (                0                ×                10                )                +                (                3                ×                9                )                +                (                0                ×                8                )                +                (                6                ×                7                )                +                (                4                ×                6                )                +                (                0                ×                5                )                +                (                6                ×                4                )                +                (                1                ×                3                )                +                (                5                ×                2                )                +                (                2                ×                1                )                                                                                                  =                0                +                27                +                0                +                42                +                24                +                0                +                24                +                3                +                10                +                2                                                                                                  =                132                =                12                ×                11                                                          {\displaystyle {\begin{aligned}s&12\times 11\end{aligned}}}  Formally, using modular arithmetic, we can say:                    (        10                  x                      1                          +        9                  x                      2                          +        8                  x                      3                          +        7                  x                      4                          +        6                  x                      5                          +        5                  x                      6                          +        4                  x                      7                          +        3                  x                      8                          +        2                  x                      9                          +                  x                      10                          )        ≡        0                            (          mod                    11          )                .              {\displaystyle (10x_{1}+9x_{2}+8x_{3}+7x_{4}+6x_{5}+5x_{6}+4x_{7}+3x_{8}+2x_{9}+x_{10})\equiv 0{\pmod {11}}.}  It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:                                                                        s                                                            =                (                0                ×                1                )                +                (                3                ×                2                )                +                (                0                ×                3                )                +                (                6                ×                4                )                +                (                4                ×                5                )                +                (                0                ×                6                )                +                (                6                ×                7                )                +                (                1                ×                8                )                +                (                5                ×                9                )                +                (                2                ×                10                )                                                                                                  =                0                +                6                +                0                +                24                +                20                +                0                +                42                +                8                +                45                +                20                                                                                                  =                165                =                15                ×                11                                                          {\displaystyle {\begin{aligned}s&15\times 11\end{aligned}}}  Formally, we can say:                    (                  x                      1                          +        2                  x                      2                          +        3                  x                      3                          +        4                  x                      4                          +        5                  x                      5                          +        6                  x                      6                          +        7                  x                      7                          +        8                  x                      8                          +        9                  x                      9                          +        10                  x                      10                          )        ≡        0                            (          mod                    11          )                .              {\displaystyle (x_{1}+2x_{2}+3x_{3}+4x_{4}+5x_{5}+6x_{6}+7x_{7}+8x_{8}+9x_{9}+10x_{10})\equiv 0{\pmod {11}}.}  The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.)The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:                                                                        s                                                            =                (                0                ×                10                )                +                (                3                ×                9                )                +                (                0                ×                8                )                +                (                6                ×                7                )                +                (                4                ×                6                )                +                (                0                ×                5                )                +                (                6                ×                4                )                +                (                1                ×                3                )                +                (                5                ×                2                )                                                                                                  =                130                                                          {\displaystyle {\begin{aligned}s&130\end{aligned}}}  Adding 2 to 130 gives a multiple of 11 (132 = 12 x 11) − this is the only number between 0 and 10 which does so. Therefore, the check digit has to be 2, and the complete sequence is ISBN 0-306-40615-2. The value                               x                      10                                {\displaystyle x_{10}}   required to satisfy this condition might be 10 if so, an 'X' should be used.Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first ""modulo 11"" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:                                                                        s                                                            =                (                11                −                (                (                (                0                ×                10                )                +                (                3                ×                9                )                +                (                0                ×                8                )                +                (                6                ×                7                )                +                (                4                ×                6                )                +                (                0                ×                5                )                +                (                6                ×                4                )                +                (                1                ×                3                )                +                (                5                ×                2                )                )                                                  mod                                                                                        11                )                                                  mod                                                                                        11                                                                                                  =                (                11                −                (                0                +                27                +                0                +                42                +                24                +                0                +                24                +                3                +                10                )                                                  mod                                                                                        11                )                                                  mod                                                                                        11                                                                                                  =                (                11                −                (                130                                                  mod                                                                                        11                )                )                                                  mod                                                                                        11                                                                                                  =                (                11                −                (                9                )                )                                                  mod                                                                                        11                                                                                                  =                (                2                )                                                  mod                                                                                        11                                                                                                  =                2                                                          {\displaystyle {\begin{aligned}s&2\end{aligned}}}  Thus the check digit is 2.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.The 2005 edition of the International ISBN Agency's official manual describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.Formally, using modular arithmetic, we can say:                    (                  x                      1                          +        3                  x                      2                          +                  x                      3                          +        3                  x                      4                          +                  x                      5                          +        3                  x                      6                          +                  x                      7                          +        3                  x                      8                          +                  x                      9                          +        3                  x                      10                          +                  x                      11                          +        3                  x                      12                          +                  x                      13                          )        ≡        0                            (          mod                    10          )                .              {\displaystyle (x_{1}+3x_{2}+x_{3}+3x_{4}+x_{5}+3x_{6}+x_{7}+3x_{8}+x_{9}+3x_{10}+x_{11}+3x_{12}+x_{13})\equiv 0{\pmod {10}}.}  The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:s = 9×1 + 7×3 + 8×1 + 0×3 + 3×1 + 0×3 + 6×1 + 4×3 + 0×1 + 6×3 + 1×1 + 5×3  =   9 +  21 +   8 +   0 +   3 +   0 +   6 +  12 +   0 +  18 +   1 +  15  = 9393 / 10 = 9 remainder 310 –  3 = 7Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.In general, the ISBN-13 check digit is calculated as follows.Let                    r        =                              (                          10        −                              (                                    x                      1                          +        3                  x                      2                          +                  x                      3                          +        3                  x                      4                          +        ⋯        +                  x                      11                          +        3                  x                      12                                                )                                            mod                                                10                              )                          .              {\displaystyle r={\big (}10-{\big (}x_{1}+3x_{2}+x_{3}+3x_{4}+\cdots +x_{11}+3x_{12}{\big )}\,{\bmod {\,}}10{\big )}.}  Then                              x                      13                          =                              {                                                            r                                                                                                            r                  <                  10                                                                              0                                                                                                            r                  =                  10.                                                                                      {\displaystyle x_{13}10.\end{cases}}}  This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1  9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).The conversion is quite simple as one only needs to prefix ""978"" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it that failure causes book identification problems for libraries, booksellers, and readers. For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase ""Cancelled ISBN"". However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine. OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Only the term ""ISBN"" should be used the terms ""eISBN"" and ""e-ISBN"" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic ""eISBN"" which encompasses all the e-book formats for a title.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13 they may have a separate barcode encoding five digits for the currency and the recommended retail price. For 10 digit ISBNs, the number ""978"", the Bookland ""country code"", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007. As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an ""M"" letter the bar code represents the ""M"" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0 979-1 to 979-9 will be used by ISBN.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.ASIN (Amazon Standard Identification Number)CODEN (serial publication identifier currently used by libraries replaced by the ISSN for new works)DOI (Digital Object Identifier)ESTC (English Short Title Catalogue)ETTN (Electronic Textbook Track Number)ISAN (International Standard Audiovisual Number)ISMN (International Standard Music Number)ISWC (International Standard Musical Work Code)ISRC (International Standard Recording Code)ISSN (International Standard Serial Number)ISTC (International Standard Text Code)ISWN (International Standard Wine Number)LCCN (Library of Congress Control Number)List of group-0 ISBN publisher codesList of group-1 ISBN publisher codesOCLC number (Online Computer Library Center number)Registration authorityBICI (Book Item and Component Identifier)SICI (Serial Item and Contribution Identifier)Special:Booksources, Wikipedia's ISBN search pageVD 16 (Verzeichnis der im deutschen Sprachbereich erschienenen Drucke des 16. Jahrhunderts)(in English: Bibliography of Books Printed in the German Speaking Countries of the Sixteenth Century)VD 17 (Verzeichnis der im deutschen Sprachraum erschienenen Drucke des 17. Jahrhunderts)(in English: Bibliography of Books Printed in the German Speaking Countries of the Seventeenth Century)ISO 2108:2005International ISBN Agency—coordinates and supervises the worldwide use of the ISBN systemNumerical List of Group Identifiers—List of language/region prefixesFree conversion tool: ISBN-10 to ISBN-13 & ISBN-13 to ISBN-10 from the ISBN agency. Also shows correct hyphenation & verifies if ISBNs are valid or not.""Implementation guidelines"" (PDF). Archived from the original (PDF) on 12 September 2004  (51.0 KB) for the 13-digit ISBN codeBooks at Curlie (based on DMOZ)""Are You Ready for ISBN-13?"". R. R. Bowker LLC. RFC 3187—Using International Standard Book Numbers as Uniform Resource Names (URN)";environmental disaster;International Standard Book Number;0
This is an alphabetical list of environmental issues, harmful aspects of human activity on the biophysical environment. As such, they relate to the anthropogenic effects on the natural environment, which are loosely divided into causes, effects and mitigation, noting that effects are interconnected and can cause new effects.Human overpopulation — Biocapacity • Carrying capacity • Exploitation • Industrialisation • I = PAT • Land degradation • Land reclamation • Optimum population • Overshoot (population) • Population density • Population dynamics • Population growth • Projections of population growth • Total fertility rate • Urbanization • Waste • Water conflict • Water scarcity • OverdraftingHydrology — Environmental impacts of reservoirs • Tile drainage • Hydrology (agriculture) • Flooding• Landslide •Intensive farming — Agricultural subsidy • Environmental effects of meat production • Intensive animal farming • Intensive crop farming • Irrigation • Monoculture • Nutrient pollution • Overgrazing • Pesticide drift • Plasticulture • Slash and burn • Tile drainageLand use — Built environment • Desertification • Habitat fragmentation • Habitat destruction • Land degradation — Land pollution • Lawn-environmental concerns • Trail ethics • Urban heat island • Urban sprawlNanotechnology — Impact of nanotechnologyNatural disastersNuclear issues — Nuclear fallout • Nuclear meltdown • Nuclear power • Nuclear weapons • Nuclear and radiation accidents • Nuclear safety • High-level radioactive waste management •Ocean trashClimate change — Global warming • Global dimming • Fossil fuels • Sea level rise • Greenhouse gas • Ocean acidification • Shutdown of thermohaline circulation • Environmental impact of the coal industry • Urban Heat Islands • FloodingEnvironmental degradation — Habitat destruction • Invasive speciesEnvironmental health — Air quality • Asthma • Birth defect • Developmental disability • endocrine disruptors • Environmental impact of the coal industry • Environmental impact of nanotechnology • Electromagnetic field • Electromagnetic radiation and health • Indoor air quality • Lead poisoning • Leukemia • Nanotoxicology • Nature deficit disorder • One Health • Sick Building Syndrome • Environmental impact of hydraulic fracturingEnvironmental issues with energy — Environmental impact of the coal industry • Environmental impact of the energy industry • Environmental impact of hydraulic fracturingEnvironmental issues with war - Agent Orange • Depleted Uranium • Military Superfund site (Category only)•Scorched earth • War and environmental law • Unexploded ordnanceOverpopulation — Burial • Overpopulation in companion animals • Tragedy of the commons • Gender Imbalance in Developing Countries • Sub-replacement fertility levels in developed countriesMutation breeding — Genetic pollutionSynthetic biologyGenetically modified food - Genetically modified crops - Genetically modified livestockPollution — Nonpoint source pollution • Point source pollutionAir pollution — Environmental impact of the coal industry • Environmental impact of hydraulic fracturing • Indoor air quality • Smog • Tropospheric ozone • Volatile organic compound Atmospheric particulate matter CFC • Biological effects of UV exposureLight pollution • Visual pollutionNoise pollutionSoil pollution — Alkali soil • Brownfield • Residual Sodium Carbonate Index • Soil conservation • Soil erosion • Soil contamination • Soil salination • Superfund • Superfund sitesWater pollution — Acid rain • Agricultural runoff • Algal bloom • Environmental impact of the coal industry • Environmental impact of hydraulic fracturing• Eutrophication • Fish kill • Groundwater pollution • Groundwater recharge • Marine debris • Marine pollution • Mercury in fish • Microplastics • Nutrient pollution • Ocean acidification • Ocean dumping • Oil spills• Soda lake • Ship pollution • Thermal pollution • Urban runoff • WastewaterSpace debris • Interplanetary contamination * Ozone depletionResource depletion — Exploitation of natural resources • Overdrafting (groundwater) •OverexploitationConsumerism — Consumer capitalism • Planned obsolescence • Over-consumptionFishing — Blast fishing • Bottom trawling • Cyanide fishing • Ghost nets • Illegal, unreported and unregulated fishing • Overfishing • Shark finning • WhalingLogging — Clearcutting • Deforestation • Illegal loggingMining — Acid mine drainage • Environmental impact of hydraulic fracturing • Mountaintop removal mining • Slurry impoundmentsWater (depletion) — Anoxic waters • Aral Sea • California Water Wars • Dead Sea • Lake Chad • Water scarcityToxicants — Agent Orange • Asbestos • Beryllium • Bioaccumulation • Biomagnification • Chlorofluorocarbons (CFCs) • Cyanide • DDT • Endocrine disruptors • Explosives • Environmental impact of the coal industry • Herbicides • Hydrocarbons • Perchlorate • Pesticides • Persistent organic pollutant • PBBs • PBDEs • Toxic heavy metals • PCB • Dioxin • Polycyclic aromatic hydrocarbons • Radioactive contamination • Volatile organic compoundsWaste — Electronic waste • Great Pacific Garbage Patch • Illegal dumping • Incineration • Litter • Waste disposal incidents • Marine debris • Medical waste • Landfill • Leachate • Toxic waste • Environmental impact of the coal industry • Exporting of hazardous wasteConservationEcosystems — Anoxic waters • Biodiversity • Biosecurity • Coral bleaching • Black carbon • Edge effect • Habitat destruction • Organic farming • Habitat fragmentation • In-situ leachFishing — Blast fishing • Bottom trawling • By-catch • Cetacean bycatch • Gillnetting • Illegal, unreported and unregulated fishing • Environmental effects of fishing • Marine pollution • Overfishing • WhalingForests — Clearcutting • Deforestation • Illegal logging • Trail ethicsNatural resources — Resource depletion • Exploitation of natural resources • Steady-state economySpecies — Endangered species • Genetic diversity • Habitat destruction • Holocene extinction • Invasive species • Poaching • Pollinator decline • Species extinction • Threshold host density • Wildlife trade • Wildlife diseaseEnergy conservation — Efficient energy use - Carfree_city - Local foodRenewable energy — Renewable energy commercializationRecreation — Protected areasWater conservationDisaster mitigationEnvironmental law - Environmental crime • Environmental justice • Polluter pays principle • Precautionary principle • Regulatory capture - Trail ethicsEnvironmentalismEnvironmental racismEnvironmental racism in EuropeGlobal issuesIndex of environmental articlesList of conservation topicsList of environmental disastersList of environmental organizationsList of population concern organizationsList of sustainability topicsLists of environmental topicsList of environmental organisations topicsEnvironmental Issues and Research Topics | AurumScience.comEnvironmental Issues | GlobalIssues.comHuman Activities that affect the Environment | The Energy PhysicsEnvironmental Threats That We are Going to Face | The Energy Physics (causes and effects)Millennium Ecosystem Assessment;environmental disaster;List of environmental issues;0
"A natural disaster is a major adverse event resulting from natural processes of the Earth examples are floods, hurricanes, tornadoes, volcanic eruptions, earthquakes, tsunamis, and other geologic processes. A natural disaster can cause loss of life or property damage, and typically leaves some economic damage in its wake, the severity of which depends on the affected population's resilience, or ability to recover and also on the infrastructure available.An adverse event will not rise to the level of a disaster if it occurs in an area without vulnerable population. In a vulnerable area, however, such as Nepal during the 2015 earthquake, an earthquake can have disastrous consequences and leave lasting damage, which can require years to repair.A landslide is described as an outward and downward slope movement of an abundance of slope-forming materials including rock, soil, artificial, or even a combination of these things.During World War I, an estimated 40,000 to 80,000 soldiers died as a result of avalanches during the mountain campaign in the Alps at the Austrian-Italian front. Many of the avalanches were caused by artillery fire.An earthquake is the result of a sudden release of energy in the Earth's crust that creates seismic waves.  At the Earth's surface, earthquakes manifest themselves by vibration, shaking, and sometimes displacement of the ground. Earthquakes are caused by slippage within geological faults. The underground point of origin of the earthquake is called the seismic focus. The point directly above the focus on the surface is called the epicenter. Earthquakes by themselves rarely kill people or wildlife. It is usually the secondary events that they trigger such as building collapse, fires, tsunamis (seismic sea waves) and volcanoes. Many of these could possibly be avoided by better construction, safety systems, early warning and planning.When natural erosion or human mining makes the ground too weak to support the structures built on it, the ground can collapse and produce a sinkhole. For example, the 2010 Guatemala City sinkhole which killed fifteen people was caused when heavy rain from Tropical Storm Agatha, diverted by leaking pipes into a pumice bedrock, led to the sudden collapse of the ground beneath a factory building.Volcanoes can cause widespread destruction and consequent disaster in several ways. The effects include the volcanic eruption itself that may cause harm following the explosion of the volcano or falling rocks. Secondly, lava may be produced during the eruption of a volcano, and so as it leaves the volcano the lava destroys many buildings, plants and animals due to its extreme heat. Thirdly, volcanic ash, generally meaning the cooled ash, may form a cloud, and settle thickly in nearby locations. When mixed with water this forms a concrete-like material. In sufficient quantities, ash may cause roofs to collapse under its weight but even small quantities will harm humans if inhaled. Since the ash has the consistency of ground glass it causes abrasion damage to moving parts such as engines. The main killer of humans in the immediate surroundings of a volcanic eruption is the pyroclastic flows, which consist of a cloud of hot volcanic ash which builds up in the air above the volcano and rushes down the slopes when the eruption no longer supports the lifting of the gases. It is believed that Pompeii was destroyed by a pyroclastic flow. A lahar is a volcanic mudflow or landslide. The 1953 Tangiwai disaster was caused by a lahar, as was the 1985 Armero tragedy in which the town of Armero was buried and an estimated 23,000 people were killed.Volcanoes rated at 8 (the highest level) on the Volcanic Explosivity Index are known as supervolcanoes. According to the Toba catastrophe theory, 75,000 to 80,000 years ago a supervolcanic eruption at what is now Lake Toba in Sumatra reduced the human population to 10,000 or even 1,000 breeding pairs, creating a bottleneck in human evolution, and killed three-quarters of all plant life in the northern hemisphere. However, there is considerable debate regarding the veracity of this theory. The main danger from a supervolcano is the immense cloud of ash, which has a disastrous global effect on climate and temperature for many years.A violent, sudden and destructive change either in the quality of Earth's water or in the distribution or movement of water on land below the surface or in the atmosphere.A flood is an overflow of water that 'submerges' land. The EU Floods Directive defines a flood as a temporary covering by water of land  which is usually not covered by water. In the sense of 'flowing water', the word may also be applied to the inflow of the tides. Flooding may result from the volume of water within a body of water, such as a river or lake, which overflows, causing some of the water to escape its usual boundaries. While the size of a lake or other body of water will vary with seasonal changes in precipitation and snow melt, it is not a significant flood unless the water covers land used by man, like a village, city or other inhabited area, roads, expanses of farmland, etc.A limnic eruption occurs when a gas, usually CO2, suddenly erupts from deep lake water, posing the threat of suffocating wildlife, livestock and humans. Such an eruption may also cause tsunamis in the lake as the rising gas displaces water. Scientists believe landslides, volcanic activity, or explosions can trigger such an eruption. To date, only two limnic eruptions have been observed and recorded. In 1984, in Cameroon, a limnic eruption in Lake Monoun caused the deaths of 37 nearby residents, and at nearby Lake Nyos in 1986 a much larger eruption killed between 1,700 and 1,800 people by asphyxiation.A tsunami (plural: tsunamis or tsunami from Japanese: 津波, lit. ""harbour wave"" English pronunciation: /tsuːˈnɑːmi/), also known as a seismic sea wave or as a tidal wave, is a series of waves in a water body caused by the displacement of a large volume of water, generally in an ocean or a large lake. Tsunamis can be caused by undersea earthquakes such as the 2004 Boxing Day tsunami, or by landslides such as the one in 1958 at Lituya Bay, Alaska, or by volcanic eruptions such as the ancient eruption of Santorini. On March 11, 2011, a tsunami occurred near Fukushima, Japan and spread through the Pacific.Blizzards are severe winter storms characterized by heavy snow and strong winds. When high winds stir up snow that has already fallen, it is known as a ground blizzard. Blizzards can impact local economic activities, especially in regions where snowfall is rare. The Great Blizzard of 1888 affected the United States, when many tons of wheat crops were destroyed, and in Asia, 2008 Afghanistan blizzard and the 1972 Iran blizzard were also significant events. The 1993 Superstorm originated in the Gulf of Mexico and traveled north, causing damage in 26 states as well as Canada and leading to more than 300 deaths.Cyclone, tropical cyclone, hurricane, and typhoon are different names for the same phenomenon, which is a cyclonic storm system that forms over the oceans. The determining factor on which term is used is based on where they originate. In the Atlantic and Northeast Pacific, the term ""hurricane"" is used in the Northwest Pacific it is referred to as a ""typhoon"" and ""cyclones"" occur in the South Pacific and Indian Ocean.The deadliest hurricane ever was the 1970 Bhola cyclone the deadliest Atlantic hurricane was the Great Hurricane of 1780 which devastated Martinique, St. Eustatius and Barbados. Another notable hurricane is Hurricane Katrina, which devastated the Gulf Coast of the United States in 2005.Drought is the unusual dryness of soil caused by levels of rainfall significantly below average over a prolonged period. Hot dry winds, shortage of water, high temperatures and consequent evaporation of moisture from the ground can also contribute to conditions of drought. Droughts result in crop failure and shortages of water.Well-known historical droughts include the 1997–2009 Millennium Drought in Australia led to a water supply crisis across much of the country. As a result, many desalination plants were built for the first time (see list). In 2011, the State of Texas lived under a drought emergency declaration for the entire calendar year and severe economic losses.  The drought caused the Bastrop fires.Severe storms, dust clouds, and volcanic eruptions can generate lightning. Apart from the damage typically associated with storms, such as winds, hail, and flooding, the lightning itself can damage buildings, ignite fires and kill by direct contact. Especially deadly lightning incidents include a 2007 strike in Ushari Dara, a remote mountain village in northwestern Pakistan, that killed 30 people, the crash of LANSA Flight 508 which killed 91 people, and a fuel explosion in Dronka, Egypt caused by lightning in 1994 which killed 469. Most lightning deaths occur in the poor countries of America and Asia, where lightning is common and adobe mud brick housing provides little protection.Hailstorms are precipitation in the form of ice, with the ice not melting before it hits the ground. A particularly damaging hailstorm hit Munich, Germany, on July 12, 1984, causing about $2 billion in insurance claims.A heat wave is a period of unusually and excessively hot weather. The worst heat wave in recent history was the European Heat Wave of 2003. A summer heat wave in Victoria, Australia, created conditions which fuelled the massive bushfires in 2009. Melbourne experienced three days in a row of temperatures exceeding 40 °C (104 °F) with some regional areas sweltering through much higher temperatures. The bushfires, collectively known as ""Black Saturday"", were partly the act of arsonists. The 2010 Northern Hemisphere summer resulted in severe heat waves, which killed over 2,000 people. It resulted in hundreds of wildfires which caused widespread air pollution, and burned thousands of square miles of forest.A tornado is a violent and dangerous rotating column of air that is in contact with both the surface of the Earth and a cumulonimbus cloud, or the base of a cumulus cloud in rare cases. It is also referred to as a twister or a cyclone, although the word cyclone is used in meteorology in a wider sense, to refer to any closed low pressure circulation. Tornadoes come in many shapes and sizes, but are typically in the form of a visible condensation funnel, whose narrow end touches the Earth and is often encircled by a cloud of debris and dust.  Most tornadoes have wind speeds less than 110 miles per hour (177 km/h), are approximately 250 feet (80 m) across, and travel a few miles (several kilometers) before dissipating. The most extreme tornadoes can attain wind speeds of more than 300 mph (480 km/h), stretch more than two miles (3 km) across, and stay on the ground for dozens of miles (perhaps more than 100 km).Wildfires are large fires which often start in wildland areas. Common causes include lightning and drought but wildfires may also be started by human negligence or arson. They can spread to populated areas and can thus be a threat to humans and property, as well as wildlife. Notable cases of wildfires were the 1871 Peshtigo Fire in the United States, which killed at least 1700 people, and the 2009 Victorian bushfires in Australia.Asteroids that impact the Earth have led to several major extinction events, including one which created the Chicxulub crater 64.9 million years ago and which is associated with the demise of the dinosaurs. Scientists estimate that the likelihood of death for a living human from a global impact event is comparable to the probability of death from an airliner crash.No human death has been definitively attributed to an impact event, but the 1490 Ch'ing-yang event in which over 10,000 people may have died has been linked to a meteor shower. Even asteroids and comets that burn up in the atmosphere can cause significant destruction on the ground due to the air burst explosion: notable air bursts include the Tunguska event in June 1908, which devastated large areas of Siberian countryside, and the Chelyabinsk meteor on 15 February 2013, which caused widespread property damage in the city of Chelyabinsk and injured 1,491.A solar flare is a phenomenon where the Sun suddenly releases a great amount of solar radiation, much more than normal. Solar flares are unlikely to cause any direct injury, but can destroy electrical equipment. The potential of solar storms to cause disaster was seen during the 1859 Carrington event, which disrupted the telegraph network, and the March 1989 geomagnetic storm which blacked out Quebec. Some major known solar flares include the X20 event on August 16, 1989, and a similar flare on April 2, 2001. The most powerful flare ever recorded occurred on November 4, 2003 (estimated at between X40 and X45).International law, for example Geneva Conventions defines International Red Cross and Red Crescent Movement the Convention on the Rights of Persons with Disabilities, requires that ""States shall take, in accordance with their obligations under international law, including international humanitarian law and international human rights law, all necessary measures to ensure the protection and safety of persons with disabilities in situations of risk, including the occurrence of natural disaster."" And further United Nations Office for the Coordination of Humanitarian Affairs is formed by General Assembly Resolution 44/182. People displaced due to natural disasters are currently protected under international law (Guiding Principles of International Displacement, Campala Convention of 2009).Natural disasters can also affect political relations with countries and vice versa. Violent conflicts within states can exacerbate the impact of natural disasters by weakening the ability of states, communities and individuals to provide disaster relief. Natural disasters can also worsen ongoing conflicts within states by weakening the capacity of states to fight rebels. In developed countries like the US, studies find that incumbents lose votes when the electorate perceives them as responsible for a poor disaster response.Between 1995 and 2015, according to the UN’s disaster-monitoring system, the greatest number of natural disasters occurred in America, China and India.In 2012, there were 905 natural disasters worldwide, 93% of which were weather-related disasters. Overall costs were US$170 billion and insured losses $70 billion. 2012 was a moderate year. 45% were meteorological (storms), 36% were hydrological (floods), 12% were climatological (heat waves, cold waves, droughts, wildfires) and 7% were geophysical events (earthquakes and volcanic eruptions). Between 1980 and 2011 geophysical events accounted for 14% of all natural catastrophes.Studies on natural events require complete historical records and strategies related to obtaining and storing reliable records, allowing for both critical interpretation and validation of the sources. Under this point of view the irreplaceable role of traditional repositories (archives) can be supplemented by the use of such web sources as eBay.Act of GodCivil defenseDisaster risk reductionEmergency managementEmergency sanitationEnvironmental disasterEnvironmental emergencyList of countries by natural disaster riskList of environmental disastersList of natural disasters by death tollWorld Conference on Disaster Risk Reduction""World Bank's Hazard Risk Management"". World Bank. ""Billion-dollar Weather and Climate Disasters"". NCDC. ""Disaster News Network"". Retrieved 2006-11-05.  US news site focused on disaster-related news.""EM-DAT International Disaster Database"". Archived from the original on 2008-08-11. Retrieved 2006-11-05.  Includes country profiles, disaster profiles and a disaster list.""Global Disaster Alert and Coordination System"". European Commission and United Nations website initiative. ""Natural Disaster and Extreme Weather. Searchable Information Center"". Ebrary. Natural hazard research from Bushfire and Natural Hazards CRC";environmental disaster;Natural disaster;0
"The natural environment encompasses all living and non-living things occurring naturally, meaning in this case not artificial. The term is most often applied to the Earth or some parts of Earth. This environment encompasses the interaction of all living species, climate, weather, and natural resources that affect human survival and economic activity.The concept of the natural environment can be distinguished as components:Complete ecological units that function as natural systems without massive civilized human intervention, including all vegetation, microorganisms, soil, rocks, atmosphere, and natural phenomena that occur within their boundaries and their nature.Universal natural resources and physical phenomena that lack clear-cut boundaries, such as air, water, and climate, as well as energy, radiation, electric charge, and magnetism, not originating from civilized human actionsIn contrast to the natural environment is the built environment. In such areas where man has fundamentally transformed landscapes such as urban settings and agricultural land conversion, the natural environment is greatly modified into a simplified human environment.  Even acts which seem less extreme, such as building a mud hut or a photovoltaic system in the desert, the modified environment becomes an artificial one.  Though many animals build things to provide a better environment for themselves, they are not human, hence beaver dams, and the works of Mound-building termites, are thought of as natural.People seldom find absolutely natural environments on Earth, and naturalness usually varies in a continuum, from  100% natural in one extreme to 0% natural in the other. More precisely, we can consider the different aspects or components of an environment, and see that their degree of naturalness is not uniform. If, for instance, in an agricultural field, the mineralogic composition and the structure of its soil are similar to those of an undisturbed forest soil, but the structure is quite different.Natural environment is often used as a synonym for habitat. For instance, when we say that the natural environment of giraffes is the savanna.Earth science generally recognizes 4 spheres, the lithosphere, the hydrosphere, the atmosphere, and the biosphere as correspondent to rocks, water, air, and life respectively. Some scientists include, as part of the spheres of the Earth, the cryosphere (corresponding to ice) as a distinct portion of the hydrosphere, as well as the pedosphere (corresponding to soil) as an active and intermixed sphere. Earth science (also known as geoscience, the geosciences or the Earth Sciences), is an all-embracing term for the sciences related to the planet Earth. There are four major disciplines in earth sciences, namely geography, geology, geophysics and geodesy. These major disciplines use physics, chemistry, biology, chronology and mathematics to build a qualitative and quantitative understanding of the principal areas or spheres of Earth.The Earth's crust, or lithosphere, is the outermost solid surface of the planet and is chemically and mechanically different from underlying mantle. It has been generated greatly by igneous processes in which magma  cools and solidifies to form solid rock. Beneath the lithosphere lies the mantle which is heated by the decay of radioactive elements. The mantle though solid is in a state of rheic convection. This convection process causes the lithospheric plates to move, albeit slowly. The resulting process is known as plate tectonics. Volcanoes result primarily from the melting of subducted crust material or of rising mantle at mid-ocean ridges and mantle plumes.Most water is found in one or another natural kind of body of water.An ocean is a major body of saline water, and a component of the hydrosphere. Approximately 71% of the Earth's surface (an area of some 362 million square kilometers) is covered by ocean, a continuous body of water that is customarily divided into several principal oceans and smaller seas. More than half of this area is over 3,000 meters (9,800 ft) deep. Average oceanic salinity is around 35 parts per thousand (ppt) (3.5%), and nearly all seawater has a salinity in the range of 30 to 38 ppt. Though generally recognized as several 'separate' oceans, these waters comprise one global, interconnected body of salt water often referred to as the World Ocean or global ocean.  The deep seabeds are more than half the Earth's surface, and are among the least-modified natural environments.  The major oceanic divisions are defined in part by the continents, various archipelagos, and other criteria: these divisions are (in descending order of size) the Pacific Ocean, the Atlantic Ocean, the Indian Ocean, the Southern Ocean and the Arctic Ocean.A river is a natural watercourse, usually freshwater, flowing toward an ocean, a lake, a sea or another river.  A few rivers simply flow into the ground and dry up completely before reaching another body of water. The water in a river is usually in a channel, made up of a stream bed between banks. In larger rivers there is also a wider floodplain shaped by waters over-topping the channel. Flood plains may be very wide in relation to the size of the river channel. Rivers are a part of the hydrological cycle. Water within a river is generally collected from precipitation through surface runoff, groundwater recharge, springs, and the release of water stored in glaciers and snowpacks.Small rivers may also be termed by several other names, including stream, creek and brook.  Their current is confined within a bed and stream banks.  Streams play an important corridor role in connecting fragmented habitats and thus in conserving biodiversity. The study of streams and waterways in general is known as surface hydrology.A lake (from Latin lacus) is a terrain feature, a body of water that is localized to the bottom of basin.  A body of water is considered a lake when it is inland, is not part of an ocean, and is larger and deeper than a pond.Natural lakes on Earth are generally found in mountainous areas, rift zones, and areas with ongoing or recent glaciation. Other lakes are found in endorheic basins or along the courses of mature rivers. In some parts of the world, there are many lakes because of chaotic drainage patterns left over from the last Ice Age. All lakes are temporary over geologic time scales, as they will slowly fill in with sediments or spill out of the basin containing them.A pond is a body of standing water, either natural or man-made, that is usually smaller than a lake. A wide variety of man-made bodies of water are classified as ponds, including water gardens designed for aesthetic ornamentation, fish ponds designed for commercial fish breeding, and solar ponds designed to store thermal energy. Ponds and lakes are distinguished from streams by their current speed. While currents in streams are easily observed, ponds and lakes possess thermally driven micro-currents and moderate wind driven currents. These features distinguish a pond from many other aquatic terrain features, such as stream pools and tide pools.The atmosphere of the Earth serves as a key factor in sustaining the planetary ecosystem. The thin layer of gases that envelops the Earth is held in place by the planet's gravity. Dry air consists of 78% nitrogen, 21% oxygen, 1% argon and other inert gases, such as carbon dioxide. The remaining gases are often referred to as trace gases, among which are the greenhouse gases such as water vapor, carbon dioxide, methane, nitrous oxide, and ozone. Filtered air includes trace amounts of many other chemical compounds. Air also contains a variable amount of water vapor and suspensions of water droplets and ice crystals seen as clouds. Many natural substances may be present in tiny amounts in an unfiltered air sample, including dust, pollen and spores, sea spray, volcanic ash, and meteoroids. Various industrial pollutants also may be present, such as chlorine (elementary or in compounds), fluorine compounds, elemental mercury, and sulphur compounds such as sulphur dioxide [SO2].The ozone layer of the Earth's atmosphere plays an important role in depleting the amount of ultraviolet (UV) radiation that reaches the surface. As DNA is readily damaged by UV light, this serves to protect life at the surface. The atmosphere also retains heat during the night, thereby reducing the daily temperature extremes.Earth's atmosphere can be divided into five main layers. These layers are mainly determined by whether temperature increases or decreases with altitude. From highest to lowest, these layers are:Exosphere: The outermost layer of Earth's atmosphere extends from the exobase upward, mainly composed of hydrogen and helium.Thermosphere: The top of the thermosphere is the bottom of the exosphere, called the exobase. Its height varies with solar activity and ranges from about 350–800 km (220–500 mi 1,150,000–2,620,000 ft). The International Space Station orbits in this layer, between 320 and 380 km (200 and 240 mi).Mesosphere: The mesosphere extends from the stratopause to 80–85 km (50–53 mi 262,000–279,000 ft). It is the layer where most meteors burn up upon entering the atmosphere.Stratosphere: The stratosphere extends from the tropopause to about 51 km (32 mi 167,000 ft). The stratopause, which is the boundary between the stratosphere and mesosphere, typically is at 50 to 55 km (31 to 34 mi 164,000 to 180,000 ft).Troposphere: The troposphere begins at the surface and extends to between 7 km (23,000 ft) at the poles and 17 km (56,000 ft) at the equator, with some variation due to weather. The troposphere is mostly heated by transfer of energy from the surface, so on average the lowest part of the troposphere is warmest and temperature decreases with altitude. The tropopause is the boundary between the troposphere and stratosphere.Other layersWithin the five principal layers determined by temperature are several layers determined by other properties.The ozone layer is contained within the stratosphere. It is mainly located in the lower portion of the stratosphere from about 15–35 km (9.3–21.7 mi 49,000–115,000 ft), though the thickness varies seasonally and geographically. About 90% of the ozone in our atmosphere is contained in the stratosphere.The ionosphere, the part of the atmosphere that is ionized by solar radiation, stretches from 50 to 1,000 km (31 to 621 mi 160,000 to 3,280,000 ft) and typically overlaps both the exosphere and the thermosphere. It forms the inner edge of the magnetosphere.The homosphere and heterosphere: The homosphere includes the troposphere, stratosphere, and mesosphere. The upper part of the heterosphere is composed almost completely of hydrogen, the lightest element.The planetary boundary layer is the part of the troposphere that is nearest the Earth's surface and is directly affected by it, mainly through turbulent diffusion.The potential dangers of global warming are being increasingly studied by a wide global consortium of scientists. These scientists are increasingly concerned about the potential long-term effects of global warming on our natural environment and on the planet. Of particular concern is how climate change and global warming caused by anthropogenic, or human-made releases of greenhouse gases, most notably carbon dioxide, can act interactively, and have adverse effects upon the planet, its natural environment and humans' existence. It is clear the planet is warming, and warming rapidly.–This warming is also responsible for the extinction of natural habitats, which in turn leads to a reduction in wildlife population.The most recent report from the Intergovernmental Panel on Climate Change (the group of the leading climate scientists in the world) concluded that the earth will warm anywhere from 2.7 to almost 11 degrees Fahrenheit (1.5 to 6 degrees Celsius) between 1990 and 2100.Efforts have been increasingly focused on the mitigation of greenhouse gases that are causing climatic changes, on developing adaptative strategies to global warming, to assist humans, other animal, and plant species, ecosystems, regions and nations in adjusting to the effects of global warming. Some examples of recent collaboration to address climate change and global warming include:The United Nations Framework Convention Treaty and convention on Climate Change, to stabilize greenhouse gas concentrations in the atmosphere at a level that would prevent dangerous anthropogenic interference with the climate system.The Kyoto Protocol, which is the protocol to the international Framework Convention on Climate Change treaty, again with the objective of reducing greenhouse gases in an effort to prevent anthropogenic climate change.The Western Climate Initiative, to identify, evaluate, and implement collective and cooperative ways to reduce greenhouse gases in the region, focusing on a market-based cap-and-trade system.A significantly profound challenge is to identify the natural environmental dynamics in contrast to environmental changes not within natural variances. A common solution is to adapt a static view neglecting natural variances to exist. Methodologically, this view could be defended when looking at processes which change slowly and short time series, while the problem arrives when fast processes turns essential in the object of the study.Climate encompasses the statistics of temperature, humidity, atmospheric pressure, wind, rainfall, atmospheric particle count and numerous other meteorological elements in a given region over long periods of time. Climate can be contrasted to weather, which is the present condition of these same elements over periods up to two weeks.Climates can be classified according to the average and typical ranges of different variables, most commonly temperature and precipitation. The most commonly used classification scheme is the one originally developed by Wladimir Köppen. The Thornthwaite system, in use since 1948, incorporates evapotranspiration in addition to temperature and precipitation information and is used in studying animal species diversity and potential impacts of climate changes.Weather is a set of all the phenomena occurring in a given atmospheric area at a given time. Most weather phenomena occur in the troposphere, just below the stratosphere. Weather refers, generally, to day-to-day temperature and precipitation activity, whereas climate is the term for the average atmospheric conditions over longer periods of time. When used without qualification, ""weather"" is understood to be the weather of Earth.Weather occurs due to density (temperature and moisture) differences between one place and another. These differences can occur due to the sun angle at any particular spot, which varies by latitude from the tropics. The strong temperature contrast between polar and tropical air gives rise to the jet stream. Weather systems in the mid-latitudes, such as extratropical cyclones, are caused by instabilities of the jet stream flow. Because the Earth's axis is tilted relative to its orbital plane, sunlight is incident at different angles at different times of the year. On the Earth's surface, temperatures usually range ±40 °C (100 °F to −40 °F) annually. Over thousands of years, changes in the Earth's orbit have affected the amount and distribution of solar energy received by the Earth and influence long-term climateSurface temperature differences in turn cause pressure differences. Higher altitudes are cooler than lower altitudes due to differences in compressional heating. Weather forecasting is the application of science and technology to predict the state of the atmosphere for a future time and a given location. The atmosphere is a chaotic system, and small changes to one part of the system can grow to have large effects on the system as a whole. Human attempts to control the weather have occurred throughout human history, and there is evidence that civilized human activity such as agriculture and industry has inadvertently modified weather patterns.Evidence suggests that life on Earth has existed for about 3.7 billion years. All known life forms share fundamental molecular mechanisms, and based on these observations, theories on the origin of life attempt to find a mechanism explaining the formation of a primordial single cell organism from which all life originates. There are many different hypotheses regarding the path that might have been taken from simple organic molecules via pre-cellular life to protocells and metabolism.Although there is no universal agreement on the definition of life, scientists generally accept that the biological manifestation of life is characterized by organization, metabolism, growth, adaptation, response to stimuli and reproduction. Life may also be said to be simply the characteristic state of organisms. In biology, the science of living organisms, ""life"" is the condition which distinguishes active organisms from inorganic matter, including the capacity for growth, functional activity and the continual change preceding death.A diverse variety of living organisms (life forms) can be found in the biosphere on Earth, and properties common to these organisms—plants, animals, fungi, protists, archaea, and bacteria—are a carbon- and water-based cellular form with complex organization and heritable genetic information. Living organisms undergo metabolism, maintain homeostasis, possess a capacity to grow, respond to stimuli, reproduce and, through natural selection, adapt to their environment in successive generations. More complex living organisms can communicate through various means.An ecosystem (also called as environment) is a natural unit consisting of all plants, animals and micro-organisms (biotic factors) in an area functioning together with all of the non-living physical (abiotic) factors of the environment.Central to the ecosystem concept is the idea that living organisms are continually engaged in a highly interrelated set of relationships with every other element constituting the environment in which they exist. Eugene Odum, one of the founders of the science of ecology, stated: ""Any unit that includes all of the organisms (ie: the ""community"") in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity, and material cycles (i.e.: exchange of materials between living and nonliving parts) within the system is an ecosystem."" The human ecosystem concept is then grounded in the deconstruction of the human/nature dichotomy, and the emergent premise that all species are ecologically integrated with each other, as well as with the abiotic constituents of their biotope.A greater number or variety of species or biological diversity of an ecosystem may contribute to greater resilience of an ecosystem, because there are more species present at a location to respond to change and thus ""absorb"" or reduce its effects. This reduces the effect before the ecosystem's structure is fundamentally changed to a different state. This is not universally the case and there is no proven relationship between the species diversity of an ecosystem and its ability to provide goods and services on a sustainable level.The term ecosystem can also pertain to human-made environments, such as human ecosystems and human-influenced ecosystems, and can describe any situation where there is relationship between living organisms and their environment. Fewer areas on the surface of the earth today exist free from human contact, although some genuine wilderness areas continue to exist without any forms of human intervention.Biomes are terminologically similar to the concept of ecosystems, and are climatically and geographically defined areas of ecologically similar climatic conditions on the Earth, such as communities of plants, animals, and soil organisms, often referred to as ecosystems. Biomes are defined on the basis of factors such as plant structures (such as trees, shrubs, and grasses), leaf types (such as broadleaf and needleleaf), plant spacing (forest, woodland, savanna), and climate. Unlike ecozones, biomes are not defined by genetic, taxonomic, or historical similarities. Biomes are often identified with particular patterns of ecological succession and climax vegetation.Global biogeochemical cycles are critical to life, most notably those of water, oxygen, carbon, nitrogen and phosphorus.The nitrogen cycle is the transformation of nitrogen and nitrogen-containing compounds in nature. It is a cycle which includes gaseous components.The water cycle, is the continuous movement of water on, above, and below the surface of the Earth. Water can change states among liquid, vapour, and ice at various places in the water cycle. Although the balance of water on Earth remains fairly constant over time, individual water molecules can come and go.The carbon cycle is the biogeochemical cycle by which carbon is exchanged among the biosphere, pedosphere, geosphere, hydrosphere, and atmosphere of the Earth.The oxygen cycle is the movement of oxygen within and between its three main reservoirs: the atmosphere, the biosphere, and the lithosphere. The main driving factor of the oxygen cycle is photosynthesis, which is responsible for the modern Earth's atmospheric composition and life.The phosphorus cycle is the movement of phosphorus through the lithosphere, hydrosphere, and biosphere. The atmosphere does not play a significant role in the movements of phosphorus, because phosphorus and phosphorus compounds are usually solids at the typical ranges of temperature and pressure found on Earth.Wilderness is generally defined as a natural environment on Earth that has not been significantly modified by human activity. The WILD Foundation goes into more detail, defining wilderness as: ""The most intact, undisturbed wild natural areas left on our planet - those last truly wild places that humans do not control and have not developed with roads, pipelines or other industrial infrastructure."" Wilderness areas and protected parks are considered important for the survival of certain species, ecological studies, conservation, solitude, and recreation. Wilderness is deeply valued for cultural, spiritual, moral, and aesthetic reasons. Some nature writers believe wilderness areas are vital for the human spirit and creativity.The word, ""wilderness"", derives from the notion of wildness in other words that which is not controllable by humans. The word's etymology is from the Old English wildeornes, which in turn derives from wildeor meaning wild beast (wild + deor = beast, deer). From this point of view, it is the wildness of a place that makes it a wilderness. The mere presence or activity of people does not disqualify an area from being ""wilderness."" Many ecosystems that are, or have been, inhabited or influenced by activities of people may still be considered ""wild."" This way of looking at wilderness includes areas within which natural processes operate without very noticeable human interference.Wildlife includes all non-domesticated plants, animals and other organisms. Domesticating wild plant and animal species for human benefit has occurred many times all over the planet, and has a major impact on the environment, both positive and negative. Wildlife can be found in all ecosystems. Deserts, rain forests, plains, and other areas—including the most developed urban sites—all have distinct forms of wildlife. While the term in popular culture usually refers to animals that are untouched by civilized human factors, most scientists agree that wildlife around the world is (now) impacted by human activities.It is the common understanding of natural environment that underlies environmentalism — a broad political, social, and philosophical movement that advocates various actions and policies in the interest of protecting what nature remains in the natural environment, or restoring or expanding the role of nature in this environment. While true wilderness is increasingly rare, wild nature (e.g., unmanaged forests, uncultivated grasslands, wildlife, wildflowers) can be found in many locations previously inhabited by humans.Goals for the benefit of people and natural systems, commonly expressed by environmental scientists and environmentalists include:Elimination of pollution and toxicants in air, water, soil, buildings, manufactured goods, and food.Preservation of biodiversity and protection of endangered species.Conservation and sustainable use of resources such as water, land, air, energy, raw materials, and natural resources.Halting human-induced global warming, which represents pollution, a threat to biodiversity, and a threat to human populations.Shifting from fossil fuels to renewable energy in electricity, heating and cooling, and transportation, which addresses pollution, global warming, and sustainability.  This may include public transportation and distributed generation, which have benefits for traffic congestion and electric reliability.Establishment of nature reserves for recreational purposes and ecosystem preservation.Sustainable and less polluting waste management including waste reduction (or even zero waste), reuse, recycling, composting, waste-to-energy, and anaerobic digestion of sewage sludge.In some cultures the term environment is meaningless because there is no separation between people and what they view as the natural world, or their surroundings. Specifically in the United States, many native cultures do not recognize the ""environment"", or see themselves as environmentalists. Media related to Environment at Wikimedia CommonsUNEP - United Nations Environment ProgrammeBBC - Science and Nature.Science.gov - Environment & Environmental Quality";environmental disaster;Natural environment;0
"Nauru (Nauruan: Naoero,  nah-OO-roo or  NAH-roo), officially the Republic of Nauru (Nauruan: Repubrikin Naoero) and formerly known as Pleasant Island, is an island country in Micronesia, a subregion of Oceania, in the Central Pacific. Its nearest neighbour is Banaba Island in Kiribati, 300 kilometres (186 mi) to the east. It further lies northwest of Tuvalu, north of the Solomon Islands, east-northeast of Papua New Guinea, southeast of the Federated States of Micronesia and south of the Marshall Islands. With 11,347 residents in a 21-square-kilometre (8.1 sq mi) area, Nauru is the smallest state in the South Pacific, smallest republic and third smallest state by area in the world, behind only Vatican City and Monaco.Settled by people from Micronesia and Polynesia c. 1000 BC, Nauru was annexed and claimed as a colony by the German Empire in the late 19th century. After World War I, Nauru became a League of Nations mandate administered by Australia, New Zealand and the United Kingdom. During World War II, Nauru was occupied by Japanese troops, who were bypassed by the Allied advance across the Pacific. After the war ended, the country entered into UN trusteeship. Nauru gained its independence in 1968.Nauru is a phosphate rock island with rich deposits near the surface, which allowed easy strip mining operations. It has some remaining phosphate resources which, as of  2011, are not economically viable for extraction. Nauru boasted the highest per-capita income enjoyed by any sovereign state in the world during the late 1960s and early 1970s. When the phosphate reserves were exhausted, and the island's environment had been seriously harmed by mining, the trust that had been established to manage the island's wealth diminished in value. To earn income, Nauru briefly became a tax haven and illegal money laundering centre. From 2001 to 2008, and again from 2012, it accepted aid from the Australian Government in exchange for hosting the Nauru Regional Processing Centre. As a result of heavy dependence on Australia, many sources have identified Nauru as a client state of Australia.Nauru was first inhabited by Micronesians and Polynesians at least 3,000 years ago. There were traditionally 12 clans or tribes on Nauru, which are represented in the 12-pointed star on the country's flag. Traditionally, Nauruans traced their descent matrilineally. Inhabitants practised aquaculture: they caught juvenile ibija fish, acclimatised them to fresh water, and raised them in the Buada Lagoon, providing a reliable source of food. The other locally grown components of their diet included coconuts and pandanus fruit. The name ""Nauru"" may derive from the Nauruan word Anáoero, which means ""I go to the beach.""The British sea captain John Fearn, a whale hunter, became the first Westerner to visit Nauru, in 1798, calling it ""Pleasant Island"". From around 1830, Nauruans had contact with Europeans from whaling ships and traders who replenished their supplies (particularly fresh water) at Nauru.Around this time, deserters from European ships began to live on the island. The islanders traded food for alcoholic palm wine and firearms. The firearms were used during the 10-year Nauruan Tribal War that began in 1878.After an agreement with Great Britain, Nauru was annexed by Germany in 1888 and incorporated into Germany's Marshall Islands Protectorate for administrative purposes. The arrival of the Germans ended the civil war, and kings were established as rulers of the island. The most widely known of these was King Auweyida. Christian missionaries from the Gilbert Islands arrived in 1888. The German settlers called the island Nawodo or Onawero. The Germans ruled Nauru for almost three decades. Robert Rasch, a German trader who married a Nauruan woman, was the first administrator, appointed in 1890.Phosphate was discovered on Nauru in 1900 by the prospector Albert Fuller Ellis. The Pacific Phosphate Company began to exploit the reserves in 1906 by agreement with Germany, exporting its first shipment in 1907. In 1914, following the outbreak of World War I, Nauru was captured by Australian troops. In 1919 it was agreed by the Allied and Associated Powers that His Britannic Majesty should be the administering authority under a League of Nations mandate. The Nauru Island Agreement made in 1919 between the governments of the United Kingdom, Australia and New Zealand provided for the administration of the island and for working of the phosphate deposits by an intergovernmental British Phosphate Commission (BPC). The terms of the League of Nations Mandate were drawn up in 1920.The island experienced an influenza epidemic in 1920, with a mortality rate of 18% among native Nauruans.In 1923, the League of Nations gave Australia a trustee mandate over Nauru, with the United Kingdom and New Zealand as co-trustees. On 6 and 7 December 1940, the German auxiliary cruisers Komet and Orion sank five supply ships in the vicinity of Nauru. Komet then shelled Nauru's phosphate mining areas, oil storage depots, and the shiploading cantilever.Japanese troops occupied Nauru on 25 August 1942. The Japanese built an airfield which was bombed for the first time on 25 March 1943, preventing food supplies from being flown to Nauru. The Japanese deported 1,200 Nauruans to work as labourers in the Chuuk islands, which was also occupied by Japan. Nauru, which had been bypassed and left to ""wither on the vine"" by American forces, was finally liberated on 13 September 1945, when commander Hisayaki Soeda surrendered the island to the Australian Army and the Royal Australian Navy.This surrender was accepted by Brigadier J. R. Stevenson, who represented Lieutenant General Vernon Sturdee, the commander of the First Australian Army, on board the warship HMAS Diamantina. Arrangements were made to repatriate from Chuuk the 737 Nauruans who survived Japanese captivity there. They were returned to Nauru by the BPC ship Trienza in January 1946.In 1947, a trusteeship was established by the United Nations, with Australia, New Zealand, and the United Kingdom as trustees. Under those arrangements, the UK, Australia and New Zealand were a joint administering authority. The Nauru Island Agreement provided for the first Administrator to be appointed by Australia for 5 years, leaving subsequent appointments to be decided by the three governments. However, in practice, administrative power was exercised by Australia alone.Nauru became self-governing in January 1966, and following a two-year constitutional convention it became independent in 1968 under founding president Hammer DeRoburt. In 1967, the people of Nauru purchased the assets of the British Phosphate Commissioners, and in June 1970 control passed to the locally owned Nauru Phosphate Corporation. Income from the mines gave Nauruans one of the highest standards of living in the world. In 1989, Nauru took legal action against Australia in the International Court of Justice over Australia's administration of the island, in particular Australia's failure to remedy the environmental damage caused by phosphate mining. Certain Phosphate Lands: Nauru v. Australia led to an out-of-court settlement to rehabilitate the mined-out areas of Nauru.Nauru is a 21 square kilometres (8 sq mi) oval-shaped island in the southwestern Pacific Ocean, 55.95 kilometres (35 mi) south of the Equator. The island is surrounded by a coral reef, which is exposed at low tide and dotted with pinnacles. The presence of the reef has prevented the establishment of a seaport, although channels in the reef allow small boats access to the island. A fertile coastal strip 150 to 300 metres (490 to 980 ft) wide lies inland from the beach.Coral cliffs surround Nauru's central plateau. The highest point of the plateau, called the Command Ridge, is 71 metres (233 ft) above sea level.The only fertile areas on Nauru are on the narrow coastal belt, where coconut palms flourish. The land around Buada Lagoon supports bananas, pineapples, vegetables, pandanus trees, and indigenous hardwoods, such as the tomano tree.Nauru was one of three great phosphate-rock islands in the Pacific Ocean, along with Banaba (Ocean Island), in Kiribati, and Makatea, in French Polynesia. The phosphate reserves on Nauru are now almost entirely depleted. Phosphate mining in the central plateau has left a barren terrain of jagged limestone pinnacles up to 15 metres (49 ft) high. Mining has stripped and devastated about 80% of Nauru's land area, and has also affected the surrounding Exclusive Economic Zone 40% of marine life is estimated to have been killed by silt and phosphate runoff.There are limited natural sources of fresh water on Nauru. Rooftop storage tanks collect rainwater. The islanders are mostly dependent on three desalination plants housed at Nauru's Utilities Agency.Nauru's climate is hot and very humid year round because of its proximity to the equator and the ocean. Nauru is hit by monsoon rains between November and February, but usually no cyclones. Annual rainfall is highly variable and is influenced by the El Niño–Southern Oscillation, with several significant recorded droughts. The temperature on Nauru ranges between 26 and 35 °C (79 and 95 °F) during the day and between 22 and 34 °C (72 and 93 °F) at night.Fauna is sparse on the island, because of a lack of vegetation and the consequences of phosphates mining. Many indigenous birds have disappeared or become rare owing to destruction of their habitat. There are about 60 recorded vascular plant species native to the island, none of which is endemic. Coconut farming, mining, and introduced species have seriously disturbed the native vegetation.There are no native land mammals, but there are native insects, land crabs, and birds, including the endemic Nauru reed warbler. The Polynesian rat, cats, dogs, pigs, and chickens have been introduced to Nauru from ships. The diversity of the reef marine life makes fishing a popular activity for tourists on the island also popular are SCUBA diving and snorkelling.The president of Nauru is Baron Waqa, who heads a 19-member unicameral parliament. The country is a member of the United Nations, the Commonwealth of Nations, the Asian Development Bank and the Pacific Islands Forum. Nauru also participates in the Commonwealth and Olympic Games. Recently Nauru became a member country of the International Renewable Energy Agency (IRENA). The Republic of Nauru became the 189th member of the International Monetary Fund in April 2016.Nauru is a republic with a parliamentary system of government. The president is both head of state and head of government. A 19-member unicameral parliament is elected every three years. The parliament elects the president from its members, and the president appoints a cabinet of five to six members.Nauru does not have any formal structure for political parties, and candidates typically stand for office as independents fifteen of the 19 members of the current Parliament are independents. Four parties that have been active in Nauruan politics are the Nauru Party, the Democratic Party, Nauru First, and the Centre Party. However, alliances within the government are often formed on the basis of extended family ties rather than party affiliation.From 1992 to 1999, Nauru had a local government system known as the Nauru Island Council (NIC). This nine-member council was designed to provide municipal services. The NIC was dissolved in 1999 and all assets and liabilities became vested in the national government. Land tenure on Nauru is unusual: all Nauruans have certain rights to all land on the island, which is owned by individuals and family groups. Government and corporate entities do not own any land, and they must enter into a lease arrangement with landowners to use land. Non-Nauruans cannot own land on the island.Nauru had 17 changes of administration between 1989 and 2003. Bernard Dowiyogo died in office in March 2003 and Ludwig Scotty was elected as the president, later being re-elected to serve a full term in October 2004. Following a vote of no confidence on 19 December 2007, Scotty was replaced by Marcus Stephen. Stephen resigned in November 2011, and Freddie Pitcher became President. Sprent Dabwido then filed a motion of no confidence in Pitcher, resulting in him becoming president. Following parliamentary elections in 2013, Baron Waqa was elected president.Its Supreme Court, headed by the Chief Justice, is paramount on constitutional issues. Other cases can be appealed to the two-judge Appellate Court. Parliament cannot overturn court decisions, but Appellate Court rulings can be appealed to the High Court of Australia. In practice this rarely happens. Lower courts consist of the District Court and the Family Court, both of which are headed by a Resident Magistrate, who also is the Registrar of the Supreme Court. There are two other quasi-courts: the Public Service Appeal Board and the Police Appeal Board, both of which are presided over by the Chief Justice.Following independence in 1968, Nauru joined the Commonwealth of Nations as a Special Member it became a full member in 2000. The country was admitted to the Asian Development Bank in 1991 and to the United Nations in 1999. Nauru is a member of the Pacific Islands Forum, the South Pacific Regional Environment Programme, the South Pacific Commission, and the South Pacific Applied Geoscience Commission. The American Atmospheric Radiation Measurement Program operates a climate-monitoring facility on the island.Nauru has no armed forces, though there is a small police force under civilian control. Australia is responsible for Nauru's defence under an informal agreement between the two countries. The September 2005 Memorandum of Understanding between Australia and Nauru provides the latter with financial aid and technical assistance, including a Secretary of Finance to prepare the budget, and advisers on health and education. This aid is in return for Nauru's housing of asylum seekers while their applications for entry into Australia are processed. Nauru uses the Australian dollar as its official currency.Nauru has used its position as a member of the United Nations to gain financial support from both Taiwan (ROC) and China (PRC) by changing its recognition from one to the other under the One-China policy. On 21 July 2002, Nauru signed an agreement to establish diplomatic relations with the PRC, accepting $130 million from the PRC for this action. In response, the ROC severed diplomatic relations with Nauru two days later. Nauru later re-established links with the ROC on 14 May 2005, and diplomatic ties with the PRC were officially severed on 31 May 2005. However, the PRC continues to maintain a representative office on Nauru.In 2008, Nauru recognised Kosovo as an independent country, and in 2009 Nauru became the fourth country, after Russia, Nicaragua, and Venezuela, to recognise Abkhazia, a breakaway region of Georgia. Russia was reported to be giving Nauru $50 million in humanitarian aid as a result of this recognition. On 15 July 2008, the Nauruan government announced a port refurbishment programme, financed with US$9 million of development aid received from Russia. The Nauru government claims this aid is not related to its recognising Abkhazia and South Ossetia.A significant portion of Nauru's income has been in the form of aid from Australia. In 2001, the MV Tampa, a Norwegian ship that had rescued 438 refugees from a stranded 20-metre-long boat, was seeking to dock in Australia. In what became known as the Tampa affair, the ship was refused entry and boarded by Australian troops. The refugees were eventually loaded onto Royal Australian Navy vessel HMAS Manoora and taken to Nauru to be held in detention facilities which later became part of the Howard government's Pacific Solution. Nauru operated two detention centres known as State House and Topside for these refugees in exchange for Australian aid. By November 2005, only two refugees, Mohammed Sagar and Muhammad Faisal, remained on Nauru from those first sent there in 2001, with Sagar finally resettling in early 2007. The Australian government sent further groups of asylum-seekers to Nauru in late 2006 and early 2007. The refugee centre was closed in 2008, but, following the Australian government's re-adoption of the Pacific Solution in August 2012, it has re-opened it. Amnesty International has described the conditions of the refugees of war living in Nauru, as ""horror"".Nauru is divided into fourteen administrative districts which are grouped into eight electoral constituencies and are further divided into various villages. The most populous district is Denigomodu with a total of 1,804 residents, out of which 1,497 reside in NPC settlement called Location. The following table shows population size by district as per 2011 census.The Nauruan economy peaked in the early 1980s, as it was dependent almost entirely on the phosphate deposits that originate from the droppings of sea birds. There are few other resources, and most necessities are imported. Small-scale mining is still conducted by RONPhos, formerly known as the Nauru Phosphate Corporation. The government places a percentage of RONPhos's earnings into the Nauru Phosphate Royalties Trust. The Trust manages long-term investments, which were intended to support the citizens once the phosphate reserves were exhausted.Because of mismanagement, the Trust's fixed and current assets were reduced considerably and may never fully recover. The failed investments included financing Leonardo the Musical in 1993. The Mercure Hotel in Sydney and Nauru House in Melbourne were sold in 2004 to finance debts and Air Nauru's only Boeing 737 was repossessed in December 2005. Normal air service resumed after the aircraft was replaced with a Boeing 737–300 airliner in June 2006. In 2005, the corporation sold its property asset in Melbourne, the vacant Savoy Tavern site, for $7.5 million.The value of the Trust is estimated to have shrunk from A$1.3 billion in 1991 to A$138 million in 2002. Nauru currently lacks money to perform many of the basic functions of government for example, the National Bank of Nauru is insolvent. The CIA World Factbook estimated a GDP per capita of $5,000 in 2005. The Asian Development Bank 2007 economic report on Nauru estimated GDP per capita at $2,400 to $2,715. The United Nations (2013) estimates the GDP per capita to 15,211 and ranks it 51 on its GDP per capita country list.There are no personal taxes in Nauru. The unemployment rate is estimated to be 90%, and of those who have jobs, the government employs 95%. The Asian Development Bank notes that, although the administration has a strong public mandate to implement economic reforms, in the absence of an alternative to phosphate mining, the medium-term outlook is for continued dependence on external assistance. Tourism is not a major contributor to the economy.In the 1990s, Nauru became a tax haven and offered passports to foreign nationals for a fee. The inter-governmental Financial Action Task Force on Money Laundering (FATF) identified Nauru as one of 15 ""non-cooperative"" countries in its fight against money laundering. During the 1990s, it was possible to establish a licensed bank in Nauru for only $25,000 with no other requirements. Under pressure from FATF, Nauru introduced anti-avoidance legislation in 2003, after which foreign hot money left the country. In October 2005, after satisfactory results from the legislation and its enforcement, FATF lifted the non-cooperative designation.From 2001 to 2007, the Nauru detention centre provided a significant source of income for the country. The Nauruan authorities reacted with concern to its closure by Australia. In February 2008, the Foreign Affairs minister, Dr Kieren Keke, stated that the closure would result in 100 Nauruans losing their jobs, and would affect 10 percent of the island's population directly or indirectly: ""We have got a huge number of families that are suddenly going to be without any income. We are looking at ways we can try and provide some welfare assistance but our capacity to do that is very limited. Literally we have got a major unemployment crisis in front of us."" The detention centre was re-opened in August 2012.In July 2017 the Organisation for Economic Co-operation and Development (OECD) upgraded its rating of Nauru's standards of tax transparency. Previously Nauru had been listed alongside fourteen other countries that had failed to show that they could comply with international tax transparency standards and regulations. The OECD subsequently put Nauru through a fast-tracked compliance process and the country was given a ""largely compliant"" rating.The Nauru 2017/18 budget, delivered by Minister for Finance David Adeang, forecasted $128.7 million in revenues and $128.6 million in expenditures and projected modest economic growth for the nation over the next two years.Nauru had 11,347 residents as of July 2016, making it the second smallest sovereign state after Vatican City. The population was previously larger, but in 2006 1,500 people left the island during a repatriation of immigrant workers from Kiribati and Tuvalu. The repatriation was motivated by large force reductions in phosphate mining. Nauru is the least populous country in Oceania.58% of people in Nauru are ethnically Nauruan, 26% are other Pacific Islander, 8% are European, and 8% are Han Chinese. Nauruans descended from Polynesian and Micronesian seafarers. Two of the 12 original tribal groups became extinct in the 20th century.The official language of Nauru is Nauruan, a distinct Pacific island language, which is spoken by 96% of ethnic Nauruans at home.English is widely spoken and is the language of government and commerce, as Nauruan is not common outside of the country.The main religion practised on the island is Christianity (two-thirds Protestant, one-third Roman Catholic). The Constitution provides for freedom of religion. The government has restricted the religious practices of The Church of Jesus Christ of Latter-day Saints and the Jehovah's Witnesses, most of whom are foreign workers employed by the government-owned Nauru Phosphate Corporation. The Catholics are pastorally served by the Roman Catholic Diocese of Tarawa and Nauru, with see at Tarawa in Kiribati.Angam Day, held on 26 October, celebrates the recovery of the Nauruan population after the two World Wars and the 1920 influenza epidemic. The displacement of the indigenous culture by colonial and contemporary Western influences is significant. Few of the old customs have been preserved, but some forms of traditional music, arts and crafts, and fishing are still practised.There are no daily news publications on Nauru, although there is one fortnightly publication, Mwinen Ko. There is a state-owned television station, Nauru Television (NTV), which broadcasts programs from New Zealand and Australia, and a state-owned non-commercial radio station, Radio Nauru, which carries programs from Radio Australia and the BBC.Australian rules football is the most popular sport in Nauru – it and weightlifting are considered the country's national sports. There is an Australian Rules football league with eight teams. Other sports popular in Nauru include volleyball, netball, fishing and tennis. Nauru participates in the Commonwealth Games and has participated in the Summer Olympic Games in weightlifting and judo.Nauru's national basketball team competed at the 1969 Pacific Games, where it defeated the Solomon Islands and Fiji.Rugby sevens popularity has increased over the last two years, so much they have a national team.Nauru competed in the 2015 Oceania Sevens Championship in New Zealand.Independence Day is celebrated on 31 January.Literacy on Nauru is 96 percent. Education is compulsory for children from six to sixteen years old, and two more non-compulsory years are offered (years 11 and 12). The island has three primary schools and two secondary schools, the latter being Nauru College and Nauru Secondary School. There is a campus of the University of the South Pacific on Nauru. Before this campus was built in 1987, students would study either by distance or abroad. Since 2011, the University of New England, Australia has established a presence on the island with around 30 Nauruan teachers studying for an associate degree in education. These students will continue onto the degree to complete their studies. This project is led by Associate Professor Pep Serow and funded by the Australian Department of Foreign Affairs and Trade.The previous community public library had been destroyed in a fire. As of  1999 a new one had not yet been built, and no bookmobile services are available as of that year. Sites with libraries include the University of the South Pacific campus, Nauru Secondary, Kayser College, and Aiwo Primary.Life expectancy on Nauru in 2009 was 60.6 years for males and 68.0 years for females.By measure of mean body mass index (BMI) Nauruans are the most overweight people in the world 97% of men and 93% of women are overweight or obese. In 2012 the obesity rate was 71.7%. Obesity in the Pacific islands is common.Nauru has the world's highest level of type 2 diabetes, with more than 40% of the population affected. Other significant dietary-related problems on Nauru include kidney disease and heart disease.Index of Nauru-related articlesOutline of NauruVisa policy of Nauru This article incorporates public domain material from the United States Department of State document ""U.S. Relations With Nauru"". This article incorporates public domain material from the CIA World Factbook website https://www.cia.gov/library/publications/the-world-factbook/index.html.Gowdy, John M McDaniel, Carl N (2000). Paradise for Sale: A Parable of Nature. University of California Press. ISBN 978-0-520-22229-8. Government of Nauru""Nauru"". The World Factbook. Central Intelligence Agency. Nauru at Curlie (based on DMOZ) Wikimedia Atlas of NauruNauru from UCB Libraries GovPubsNauru profile from the BBC News Online";environmental disaster;Nauru;0
"A nuclear weapon is an explosive device that derives its destructive force from nuclear reactions, either fission (fission bomb) or from a combination of fission and fusion reactions (thermonuclear bomb). Both bomb types release large quantities of energy from relatively small amounts of matter. The first test of a fission (""atomic"") bomb released an amount of energy approximately equal to 20,000 tons of TNT (84 TJ). The first thermonuclear (""hydrogen"") bomb test released energy approximately equal to 10 million tons of TNT (42 PJ). A thermonuclear weapon weighing little more than 2,400 pounds (1,100 kg) can release energy equal to more than 1.2 million tons of TNT (5.0 PJ). A nuclear device no larger than traditional bombs can devastate an entire city by blast, fire, and radiation. Since they are weapons of mass destruction, the proliferation of nuclear weapons is a focus of international relations policy.Nuclear weapons have been used twice in war, both times by the United States against Japan near the end of World War II. On August 6, 1945, the U.S. Army Air Forces detonated a uranium gun-type fission bomb nicknamed ""Little Boy"" over the Japanese city of Hiroshima three days later, on August 9, the U.S. Army Air Forces detonated a plutonium implosion-type fission bomb nicknamed ""Fat Man"" over the Japanese city of Nagasaki.  These bombings caused injuries that resulted in the deaths of approximately 200,000 civilians and military personnel. The ethics of these bombings and their role in Japan's surrender are subjects of debate.Since the atomic bombings of Hiroshima and Nagasaki, nuclear weapons have been detonated over two thousand times for testing and demonstration. Only a few nations possess such weapons or are suspected of seeking them. The only countries known to have detonated nuclear weapons—and acknowledge possessing them—are (chronologically by date of first test) the United States, the Soviet Union (succeeded as a nuclear power by Russia), the United Kingdom, France, China, India, Pakistan, and North Korea. Israel is believed to possess nuclear weapons, though, in a policy of deliberate ambiguity, it does not acknowledge having them. Germany, Italy, Turkey, Belgium and the Netherlands are nuclear weapons sharing states. South Africa is the only country to have independently developed and then renounced and dismantled its nuclear weapons.The Treaty on the Non-Proliferation of Nuclear Weapons aims to reduce the spread of nuclear weapons, but its effectiveness has been questioned, and political tensions remained high in the 1970s and 1980s. Modernisation of weapons continues to this day.There are two basic types of nuclear weapons: those that derive the majority of their energy from nuclear fission reactions alone, and those that use fission reactions to begin nuclear fusion reactions that produce a large amount of the total energy output.All existing nuclear weapons derive some of their explosive energy from nuclear fission reactions. Weapons whose explosive output is exclusively from fission reactions are commonly referred to as atomic bombs or atom bombs (abbreviated as A-bombs). This has long been noted as something of a misnomer, as their energy comes from the nucleus of the atom, just as it does with fusion weapons.In fission weapons, a mass of fissile material (enriched uranium or plutonium) is forced into supercriticality—allowing an exponential growth of nuclear chain reactions—either by shooting one piece of sub-critical material into another (the ""gun"" method) or by compressing using explosive lenses a sub-critical sphere of material using chemical explosives to many times its original density (the ""implosion"" method). The latter approach is considered more sophisticated than the former, and only the latter approach can be used if the fissile material is plutonium.A major challenge in all nuclear weapon designs is to ensure that a significant fraction of the fuel is consumed before the weapon destroys itself. The amount of energy released by fission bombs can range from the equivalent of just under a ton to upwards of 500,000 tons (500 kilotons) of TNT (4.2 to 2.1×108 GJ).All fission reactions generate fission products, the remains of the split atomic nuclei. Many fission products are either highly radioactive (but short-lived) or moderately radioactive (but long-lived), and as such, they are a serious form of radioactive contamination if not fully contained. Fission products are the principal radioactive component of nuclear fallout.The most commonly used fissile materials for nuclear weapons applications have been uranium-235 and plutonium-239. Less commonly used has been uranium-233. Neptunium-237 and some isotopes of americium may be usable for nuclear explosives as well, but it is not clear that this has ever been implemented, and their plausible use in nuclear weapons is a matter of dispute.The other basic type of nuclear weapon produces a large proportion of its energy in nuclear fusion reactions. Such fusion weapons are generally referred to as thermonuclear weapons or more colloquially as hydrogen bombs (abbreviated as H-bombs), as they rely on fusion reactions between isotopes of hydrogen (deuterium and tritium). All such weapons derive a significant portion of their energy from fission reactions used to ""trigger"" fusion reactions, and fusion reactions can themselves trigger additional fission reactions.Only six countries—United States, Russia, United Kingdom, China, France, and India—have conducted thermonuclear weapon tests. (Whether India has detonated a ""true"" multi-staged thermonuclear weapon is controversial.) North Korea claims to have tested a fusion weapon as of January 2016, though this claim is disputed. Thermonuclear weapons are considered much more difficult to successfully design and execute than primitive fission weapons. Almost all of the nuclear weapons deployed today use the thermonuclear design because it is more efficient.Thermonuclear bombs work by using the energy of a fission bomb to compress and heat fusion fuel. In the Teller-Ulam design, which accounts for all multi-megaton yield hydrogen bombs, this is accomplished by placing a fission bomb and fusion fuel (tritium, deuterium, or lithium deuteride) in proximity within a special, radiation-reflecting container. When the fission bomb is detonated, gamma rays and X-rays emitted first compress the fusion fuel, then heat it to thermonuclear temperatures. The ensuing fusion reaction creates enormous numbers of high-speed neutrons, which can then induce fission in materials not normally prone to it, such as depleted uranium. Each of these components is known as a ""stage"", with the fission bomb as the ""primary"" and the fusion capsule as the ""secondary"". In large, megaton-range hydrogen bombs, about half of the yield comes from the final fissioning of depleted uranium.Virtually all thermonuclear weapons deployed today use the ""two-stage"" design described above, but it is possible to add additional fusion stages—each stage igniting a larger amount of fusion fuel in the next stage. This technique can be used to construct thermonuclear weapons of arbitrarily large yield, in contrast to fission bombs, which are limited in their explosive force. The largest nuclear weapon ever detonated, the Tsar Bomba of the USSR, which released an energy equivalent of over 50 megatons of TNT (210 PJ), was a three-stage weapon. Most thermonuclear weapons are considerably smaller than this, due to practical constraints from missile warhead space and weight requirements.Fusion reactions do not create fission products, and thus contribute far less to the creation of nuclear fallout than fission reactions, but because all thermonuclear weapons contain at least one fission stage, and many high-yield thermonuclear devices have a final fission stage, thermonuclear weapons can generate at least as much nuclear fallout as fission-only weapons.There are other types of nuclear weapons as well. For example, a boosted fission weapon is a fission bomb that increases its explosive yield through a small number of fusion reactions, but it is not a fusion bomb. In the boosted bomb, the neutrons produced by the fusion reactions serve primarily to increase the efficiency of the fission bomb. There are two types of boosted fission bomb: internally boosted, in which a deuterium-tritium mixture is injected into the bomb core, and externally boosted, in which concentric shells of lithium-deuteride and depleted uranium are layered on the outside of the fission bomb core.Some nuclear weapons are designed for special purposes a neutron bomb is a thermonuclear weapon that yields a relatively small explosion but a relatively large amount of neutron radiation such a device could theoretically be used to cause massive casualties while leaving infrastructure mostly intact and creating a minimal amount of fallout. The detonation of any nuclear weapon is accompanied by a blast of neutron radiation. Surrounding a nuclear weapon with suitable materials (such as cobalt or gold) creates a weapon known as a salted bomb. This device can produce exceptionally large quantities of long-lived radioactive contamination. It has been conjectured that such a device could serve as a ""doomsday weapon"" because such a large quantity of radioactivities with half-lives of decades, lifted into the stratosphere where winds would distribute it around the globe, would make all life on the planet extinct.In connection with the Strategic Defense Initiative, research into the nuclear pumped laser was conducted under the DOD program Project Excalibur but this did not result in a working weapon. The concept involves the tapping of the energy of an exploding nuclear bomb to power a single-shot laser which is directed at a distant target.During the Starfish Prime high-altitude nuclear test in 1962, an unexpected effect was produced which is called a nuclear electromagnetic pulse. This is an intense flash of electromagnetic energy produced by a rain of high energy electrons which in turn are produced by a nuclear bomb's gamma rays. This flash of energy can permanently destroy or disrupt electronic equipment if insufficiently shielded. It has been proposed to use this effect to disable an enemy's military and civilian infrastructure as an adjunct to other nuclear or conventional military operations against that enemy. Because the effect is produced by high altitude nuclear detonations, it can produce damage to electronics over a wide, even continental, geographical area.Research has been done into the possibility of pure fusion bombs: nuclear weapons that consist of fusion reactions without requiring a fission bomb to initiate them. Such a device might provide a simpler path to thermonuclear weapons than one that required development of fission weapons first, and pure fusion weapons would create significantly less nuclear fallout than other thermonuclear weapons, because they would not disperse fission products. In 1998, the United States Department of Energy divulged that the United States had, ""...made a substantial investment"" in the past to develop pure fusion weapons, but that, ""The U.S. does not have and is not developing a pure fusion weapon"", and that, ""No credible design for a pure fusion weapon resulted from the DOE investment"".Antimatter, which consists of particles resembling ordinary matter particles in most of their properties but having opposite electric charge, has been considered as a trigger mechanism for nuclear weapons. A major obstacle is the difficulty of producing antimatter in large enough quantities, and there is no evidence that it is feasible beyond the military domain. However, the U.S. Air Force funded studies of the physics of antimatter in the Cold War, and began considering its possible use in weapons, not just as a trigger, but as the explosive itself. A fourth generation nuclear weapon design is related to, and relies upon, the same principle as antimatter-catalyzed nuclear pulse propulsion.Most variation in nuclear weapon design is for the purpose of achieving different yields for different situations, and in manipulating design elements to attempt to minimize weapon size.The system used to deliver a nuclear weapon to its target is an important factor affecting both nuclear weapon design and nuclear strategy. The design, development, and maintenance of delivery systems are among the most expensive parts of a nuclear weapons program they account, for example, for 57% of the financial resources spent by the United States on nuclear weapons projects since 1940.The simplest method for delivering a nuclear weapon is a gravity bomb dropped from aircraft this was the method used by the United States against Japan. This method places few restrictions on the size of the weapon. It does, however, limit attack range, response time to an impending attack, and the number of weapons that a country can field at the same time. With miniaturization, nuclear bombs can be delivered by both strategic bombers and tactical fighter-bombers. This method is the primary means of nuclear weapons delivery the majority of U.S. nuclear warheads, for example, are free-fall gravity bombs, namely the B61.More preferable from a strategic point of view is a nuclear weapon mounted on a missile, which can use a ballistic trajectory to deliver the warhead over the horizon. Although even short-range missiles allow for a faster and less vulnerable attack, the development of long-range intercontinental ballistic missiles (ICBMs) and submarine-launched ballistic missiles (SLBMs) has given some nations the ability to plausibly deliver missiles anywhere on the globe with a high likelihood of success.More advanced systems, such as multiple independently targetable reentry vehicles (MIRVs), can launch multiple warheads at different targets from one missile, reducing the chance of a successful missile defense. Today, missiles are most common among systems designed for delivery of nuclear weapons. Making a warhead small enough to fit onto a missile, though, can be difficult.Tactical weapons have involved the most variety of delivery types, including not only gravity bombs and missiles but also artillery shells, land mines, and nuclear depth charges and torpedoes for anti-submarine warfare. An atomic mortar has been tested by the United States. Small, two-man portable tactical weapons (somewhat misleadingly referred to as suitcase bombs), such as the Special Atomic Demolition Munition, have been developed, although the difficulty of combining sufficient yield with portability limits their military utility.Nuclear warfare strategy is a set of policies that deal with preventing or fighting a nuclear war. The policy of trying to prevent an attack by a nuclear weapon from another country by threatening nuclear retaliation is known as the strategy of nuclear deterrence. The goal in deterrence is to always maintain a second strike capability (the ability of a country to respond to a nuclear attack with one of its own) and potentially to strive for first strike status (the ability to destroy an enemy's nuclear forces before they could retaliate). During the Cold War, policy and military theorists considered the sorts of policies that might prevent a nuclear attack, and they developed game theory models that could lead to stable deterrence conditions.Different forms of nuclear weapons delivery (see above) allow for different types of nuclear strategies. The goals of any strategy are generally to make it difficult for an enemy to launch a pre-emptive strike against the weapon system and difficult to defend against the delivery of the weapon during a potential conflict. This can mean keeping weapon locations hidden, such as deploying them on submarines or land mobile transporter erector launchers whose locations are difficult to track, or it can mean protecting weapons by burying them in hardened missile silo bunkers. Other components of nuclear strategies included using missile defenses to destroy the missiles before they land, or implementing civil defense measures using early-warning systems to evacuate citizens to safe areas before an attack.Weapons designed to threaten large populations or to deter attacks are known as strategic weapons. Nuclear weapons for use on a battlefield in military situations are called tactical weapons.Critics of nuclear war strategy often suggest that a nuclear war between two nations would result in mutual annihilation. From this point of view, the significance of nuclear weapons is to deter war because any nuclear war would escalate out of mutual distrust and fear, resulting in mutually assured destruction. This threat of national, if not global, destruction has been a strong motivation for anti-nuclear weapons activism.Critics from the peace movement and within the military establishment have questioned the usefulness of such weapons in the current military climate. According to an advisory opinion issued by the International Court of Justice in 1996, the use of (or threat of use of) such weapons would generally be contrary to the rules of international law applicable in armed conflict, but the court did not reach an opinion as to whether or not the threat or use would be lawful in specific extreme circumstances such as if the survival of the state were at stake.Another deterrence position is that nuclear proliferation can be desirable. In this case, it is argued that, unlike conventional weapons, nuclear weapons deter all-out war between states, and they succeeded in doing this during the Cold War between the U.S. and the Soviet Union. In the late 1950s and early 1960s, Gen. Pierre Marie Gallois of France, an adviser to Charles de Gaulle, argued in books like The Balance of Terror: Strategy for the Nuclear Age (1961) that mere possession of a nuclear arsenal was enough to ensure deterrence, and thus concluded that the spread of nuclear weapons could increase international stability. Some prominent neo-realist scholars, such as Kenneth Waltz and John Mearsheimer, have argued, along the lines of Gallois, that some forms of nuclear proliferation would decrease the likelihood of total war, especially in troubled regions of the world where there exists a single nuclear-weapon state. Aside from the public opinion that opposes proliferation in any form, there are two schools of thought on the matter: those, like Mearsheimer, who favored selective proliferation, and Waltz, who was somewhat more non-interventionist. Interest in proliferation and the stability-instability paradox that it generates continues to this day, with ongoing debate about indigenous Japanese and South Korean nuclear deterrent against North Korea.The threat of potentially suicidal terrorists possessing nuclear weapons (a form of nuclear terrorism) complicates the decision process. The prospect of mutually assured destruction might not deter an enemy who expects to die in the confrontation. Further, if the initial act is from a stateless terrorist instead of a sovereign nation, there might not be a nation or specific target to retaliate against. It has been argued, especially after the September 11, 2001 attacks, that this complication calls for a new nuclear strategy, one that is distinct from that which gave relative stability during the Cold War. Since 1996, the United States has had a policy of allowing the targeting of its nuclear weapons at terrorists armed with weapons of mass destruction.Robert Gallucci argues that although traditional deterrence is not an effective approach toward terrorist groups bent on causing a nuclear catastrophe, Gallucci believes that ""the United States should instead consider a policy of expanded deterrence, which focuses not solely on the would-be nuclear terrorists but on those states that may deliberately transfer or inadvertently lead nuclear weapons and materials to them. By threatening retaliation against those states, the United States may be able to deter that which it cannot physically prevent."".Graham Allison makes a similar case, arguing that the key to expanded deterrence is coming up with ways of tracing nuclear material to the country that forged the fissile material. ""After a nuclear bomb detonates, nuclear forensics cops would collect debris samples and send them to a laboratory for radiological analysis. By identifying unique attributes of the fissile material, including its impurities and contaminants, one could trace the path back to its origin."" The process is analogous to identifying a criminal by fingerprints. ""The goal would be twofold: first, to deter leaders of nuclear states from selling weapons to terrorists by holding them accountable for any use of their weapons second, to give leaders every incentive to tightly secure their nuclear weapons and materials.""Because they are weapons of mass destruction, the proliferation and possible use of nuclear weapons are important issues in international relations and diplomacy. In most countries, the use of nuclear force can only be authorized by the head of government or head of state. Despite controls and regulations governing nuclear weapons, there is an inherent danger of ""accidents, mistakes, false alarms, blackmail, theft, and sabotage"".In the late 1940s, lack of mutual trust prevented the United States and the Soviet Union from making progress on arms control agreements. The Russell–Einstein Manifesto was issued in London on July 9, 1955, by Bertrand Russell in the midst of the Cold War. It highlighted the dangers posed by nuclear weapons and called for world leaders to seek peaceful resolutions to international conflict. The signatories included eleven pre-eminent intellectuals and scientists, including Albert Einstein, who signed it just days before his death on April 18, 1955. A few days after the release, philanthropist Cyrus S. Eaton offered to sponsor a conference—called for in the manifesto—in Pugwash, Nova Scotia, Eaton's birthplace. This conference was to be the first of the Pugwash Conferences on Science and World Affairs, held in July 1957.By the 1960s steps were taken to limit both the proliferation of nuclear weapons to other countries and the environmental effects of nuclear testing. The Partial Nuclear Test Ban Treaty (1963) restricted all nuclear testing to underground nuclear testing, to prevent contamination from nuclear fallout, whereas the Treaty on the Non-Proliferation of Nuclear Weapons (1968) attempted to place restrictions on the types of activities signatories could participate in, with the goal of allowing the transference of non-military nuclear technology to member countries without fear of proliferation.In 1957, the International Atomic Energy Agency (IAEA) was established under the mandate of the United Nations to encourage development of peaceful applications of nuclear technology, provide international safeguards against its misuse, and facilitate the application of safety measures in its use. In 1996, many nations signed the Comprehensive Nuclear-Test-Ban Treaty, which prohibits all testing of nuclear weapons. A testing ban imposes a significant hindrance to nuclear arms development by any complying country. The Treaty requires the ratification by 44 specific states before it can go into force as of 2012, the ratification of eight of these states is still required.Additional treaties and agreements have governed nuclear weapons stockpiles between the countries with the two largest stockpiles, the United States and the Soviet Union, and later between the United States and Russia. These include treaties such as SALT II (never ratified), START I (expired), INF, START II (never ratified), SORT, and New START, as well as non-binding agreements such as SALT I and the Presidential Nuclear Initiatives of 1991. Even when they did not enter into force, these agreements helped limit and later reduce the numbers and types of nuclear weapons between the United States and the Soviet Union/Russia.Nuclear weapons have also been opposed by agreements between countries. Many nations have been declared Nuclear-Weapon-Free Zones, areas where nuclear weapons production and deployment are prohibited, through the use of treaties. The Treaty of Tlatelolco (1967) prohibited any production or deployment of nuclear weapons in Latin America and the Caribbean, and the Treaty of Pelindaba (1964) prohibits nuclear weapons in many African countries. As recently as 2006 a Central Asian Nuclear Weapon Free Zone was established amongst the former Soviet republics of Central Asia prohibiting nuclear weapons.In 1996, the International Court of Justice, the highest court of the United Nations, issued an Advisory Opinion concerned with the ""Legality of the Threat or Use of Nuclear Weapons"". The court ruled that the use or threat of use of nuclear weapons would violate various articles of international law, including the Geneva Conventions, the Hague Conventions, the UN Charter, and the Universal Declaration of Human Rights. Given the unique, destructive characteristics of nuclear weapons, the International Committee of the Red Cross calls on States to ensure that these weapons are never used, irrespective of whether they consider them lawful or not.Additionally, there have been other, specific actions meant to discourage countries from developing nuclear arms. In the wake of the tests by India and Pakistan in 1998, economic sanctions were (temporarily) levied against both countries, though neither were signatories with the Nuclear Non-Proliferation Treaty. One of the stated casus belli for the initiation of the 2003 Iraq War was an accusation by the United States that Iraq was actively pursuing nuclear arms (though this was soon discovered not to be the case as the program had been discontinued). In 1981, Israel had bombed a nuclear reactor being constructed in Osirak, Iraq, in what it called an attempt to halt Iraq's previous nuclear arms ambitions in 2007, Israel bombed another reactor being constructed in Syria.In 2013, Mark Diesendorf said that governments of France, India, North Korea, Pakistan, UK, and South Africa have used nuclear power and/or research reactors to assist nuclear weapons development or to contribute to their supplies of nuclear explosives from military reactors.Nuclear disarmament refers to both the act of reducing or eliminating nuclear weapons and to the end state of a nuclear-free world, in which nuclear weapons are eliminated.Beginning with the 1963 Partial Test Ban Treaty and continuing through the 1996 Comprehensive Test Ban Treaty, there have been many treaties to limit or reduce nuclear weapons testing and stockpiles. The 1968 Nuclear Non-Proliferation Treaty has as one of its explicit conditions that all signatories must ""pursue negotiations in good faith"" towards the long-term goal of ""complete disarmament"". The nuclear weapon states have largely treated that aspect of the agreement as ""decorative"" and without force.Only one country—South Africa—has ever fully renounced nuclear weapons they had independently developed. The former Soviet republics of Belarus, Kazakhstan, and Ukraine returned Soviet nuclear arms stationed in their countries to Russia after the collapse of the USSR.Proponents of nuclear disarmament say that it would lessen the probability of nuclear war, especially accidentally. Critics of nuclear disarmament say that it would undermine the present nuclear peace and deterrence and would lead to increased global instability. Various American elder statesmen, who were in office during the Cold War period, have been advocating the elimination of nuclear weapons. These officials include Henry Kissinger, George Shultz, Sam Nunn, and William Perry. In January 2010, Lawrence M. Krauss stated that ""no issue carries more importance to the long-term health and security of humanity than the effort to reduce, and perhaps one day, rid the world of nuclear weapons"".In the years after the end of the Cold War, there have been numerous campaigns to urge the abolition of nuclear weapons, such as that organized by the Global Zero movement, and the goal of a ""world without nuclear weapons"" was advocated by United States President Barack Obama in an April 2009 speech in Prague. A CNN poll from April 2010 indicated that the American public was nearly evenly split on the issue.Some analysts have argued that nuclear weapons have made the world relatively safer, with peace through deterrence and through the stability–instability paradox, including in south Asia. Kenneth Waltz has argued that nuclear weapons have helped keep an uneasy peace, and further nuclear weapon proliferation might even help avoid the large scale conventional wars that were so common before their invention at the end of World War II. But former Secretary Henry Kissinger says there is a new danger, which cannot be addressed by deterrence: ""The classical notion of deterrence was that there was some consequences before which aggressors and evildoers would recoil. In a world of suicide bombers, that calculation doesn’t operate in any comparable way"". George Shultz has said, ""If you think of the people who are doing suicide attacks, and people like that get a nuclear weapon, they are almost by definition not deterrable"".The UN Office for Disarmament Affairs (UNODA) is a department of the United Nations Secretariat established in January 1998 as part of the United Nations Secretary-General Kofi Annan's plan to reform the UN as presented in his report to the General Assembly in July 1997.Its goal is to promote nuclear disarmament and non-proliferation and the strengthening of the disarmament regimes in respect to other weapons of mass destruction, chemical and biological weapons. It also promotes disarmament efforts in the area of conventional weapons, especially land mines and small arms, which are often the weapons of choice in contemporary conflicts.Even before the first nuclear weapons had been developed, scientists involved with the Manhattan Project were divided over the use of the weapon. The role of the two atomic bombings of the country in Japan's surrender and the U.S.'s ethical justification for them has been the subject of scholarly and popular debate for decades. The question of whether nations should have nuclear weapons, or test them, has been continually and nearly universally controversial.21 August 1945: While conducting impromptu experiments on a third core (an alloy of plutonium and gallium) which had been prepared for atomic warfare at Los Alamos National Laboratory, physicist Harry Daghlian received a lethal dose of radiation. He died on 15 September 1945.21 May 1946: While conducting further impromptu experiments on the third plutonium core at Los Alamos National Laboratory, physicist Louis Slotin received a lethal dose of radiation. He died on 30 May 1946. After these 2 incidents, the core was used to construct a bomb for use on the Nevada Test Range.February 13, 1950: a Convair B-36B crashed in northern British Columbia after jettisoning a Mark IV atomic bomb. This was the first such nuclear weapon loss in history.May 22, 1957: a 42,000-pound (19,000 kg) Mark-17 hydrogen bomb accidentally fell from a bomber near Albuquerque, New Mexico. The detonation of the device's conventional explosives destroyed it on impact and formed a crater 25 feet (7.6 m) in diameter on land owned by the University of New Mexico. According to a researcher at the Natural Resources Defense Council, it was one of the most powerful bombs made to date.June 7, 1960: the 1960 Fort Dix IM-99 accident destroyed a Boeing CIM-10 Bomarc nuclear missile and shelter and contaminated the BOMARC Missile Accident Site in New Jersey.January 24, 1961: the 1961 Goldsboro B-52 crash occurred near Goldsboro, North Carolina. A Boeing B-52 Stratofortress carrying two Mark 39 nuclear bombs broke up in mid-air, dropping its nuclear payload in the process.1965 Philippine Sea A-4 crash, where a Skyhawk attack aircraft with a nuclear weapon fell into the sea. The pilot, the aircraft, and the B43 nuclear bomb were never recovered. It was not until 1989 that the Pentagon revealed the loss of the one-megaton bomb.January 17, 1966: the 1966 Palomares B-52 crash occurred when a B-52G bomber of the USAF collided with a KC-135 tanker during mid-air refuelling off the coast of Spain. The KC-135 was completely destroyed when its fuel load ignited, killing all four crew members. The B-52G broke apart, killing three of the seven crew members aboard. Of the four Mk28 type hydrogen bombs the B-52G carried, three were found on land near Almería, Spain. The non-nuclear explosives in two of the weapons detonated upon impact with the ground, resulting in the contamination of a 2-square-kilometer (490-acre) (0.78 square mile) area by radioactive plutonium. The fourth, which fell into the Mediterranean Sea, was recovered intact after a 2½-month-long search.January 21, 1968: the 1968 Thule Air Base B-52 crash involved a United States Air Force (USAF) B-52 bomber. The aircraft was carrying four hydrogen bombs when a cabin fire forced the crew to abandon the aircraft. Six crew members ejected safely, but one who did not have an ejection seat was killed while trying to bail out. The bomber crashed onto sea ice in Greenland, causing the nuclear payload to rupture and disperse, which resulted in widespread radioactive contamination.September 18–19, 1980: the Damascus Accident, occurred in Damascus, Arkansas, where a Titan missile equipped with a nuclear warhead exploded. The accident was caused by a maintenance man who dropped a socket from a socket wrench down an 80-foot (24 m) shaft, puncturing a fuel tank on the rocket. Leaking fuel resulted in a hypergolic fuel explosion, jettisoning the W-53 warhead beyond the launch site. Over 500 atmospheric nuclear weapons tests were conducted at various sites around the world from 1945 to 1980. Radioactive fallout from nuclear weapons testing was first drawn to public attention in 1954 when the Castle Bravo hydrogen bomb test at the Pacific Proving Grounds contaminated the crew and catch of the Japanese fishing boat Lucky Dragon. One of the fishermen died in Japan seven months later, and the fear of contaminated tuna led to a temporary boycotting of the popular staple in Japan. The incident caused widespread concern around the world, especially regarding the effects of nuclear fallout and atmospheric nuclear testing, and ""provided a decisive impetus for the emergence of the anti-nuclear weapons movement in many countries"".As public awareness and concern mounted over the possible health hazards associated with exposure to the nuclear fallout, various studies were done to assess the extent of the hazard. A Centers for Disease Control and Prevention/ National Cancer Institute study claims that fallout from atmospheric nuclear tests would lead to perhaps 11,000 excess deaths amongst people alive during atmospheric testing in the United States from all forms of cancer, including leukemia, from 1951 to well into the 21st century.As of March 2009, the U.S. is the only nation that compensates nuclear test victims. Since the Radiation Exposure Compensation Act of 1990, more than $1.38 billion in compensation has been approved. The money is going to people who took part in the tests, notably at the Nevada Test Site, and to others exposed to the radiation.In addition, leakage of byproducts of nuclear weapon production into groundwater has been an ongoing issue, particularly at the Hanford site.Some scientists estimate that a nuclear war with 100 Hiroshima-size nuclear explosions on cities could cost the lives of tens of millions of people from long term climatic effects alone. The climatology hypothesis is that if each city firestorms, a great deal of soot could be thrown up into the atmosphere which could blanket the earth, cutting out sunlight for years on end, causing the disruption of food chains, in what is termed a nuclear winter.People near the Hiroshima explosion and who managed to survive the explosion subsequently suffered a variety of medical effects:Initial stage—the first 1–9 weeks, in which are the greatest number of deaths, with 90% due to thermal injury and/or blast effects and 10% due to super-lethal radiation exposure.Intermediate stage—from 10–12 weeks. The deaths in this period are from ionizing radiation in the median lethal range – LD50Late period—lasting from 13–20 weeks. This period has some improvement in survivors' condition.Delayed period—from 20+ weeks. Characterized by numerous complications, mostly related to healing of thermal and mechanical injuries, and if the individual was exposed to a few hundred to a thousand millisieverts of radiation, it is coupled with infertility, sub-fertility and blood disorders. Furthermore, ionizing radiation above a dose of around 50–100 millisievert exposure has been shown to statistically begin increasing one's chance of dying of cancer sometime in their lifetime over the normal unexposed rate of ~25%, in the long term, a heightened rate of cancer, proportional to the dose received, would begin to be observed after ~5+ years, with lesser problems such as eye cataracts and other more minor effects in other organs and tissue also being observed over the long term.Fallout exposure – Depending on if further afield individuals shelter in place or evacuate perpendicular to the direction of the wind, and therefore avoid contact with the fallout plume, and stay there for the days and weeks after the nuclear explosion, their exposure to fallout, and therefore their total dose, will vary. With those who do shelter in place, and or evacuate, experiencing a total dose that would be negligible in comparison to someone who just went about their life as normal.Staying indoors until after the most hazardous fallout isotope, I-131 decays away to 0.1% of its initial quantity after ten half lifes – which is represented by 80 days in I-131s case, would make the difference between likely contracting Thyroid cancer or escaping completely from this substance depending on the actions of the individual.Peace movements emerged in Japan and in 1954 they converged to form a unified ""Japanese Council Against Atomic and Hydrogen Bombs"". Japanese opposition to nuclear weapons tests in the Pacific Ocean was widespread, and ""an estimated 35 million signatures were collected on petitions calling for bans on nuclear weapons"".In the United Kingdom, the first Aldermaston March organised by the Campaign for Nuclear Disarmament(CND) took place at Easter 1958, when, according to the CND, several thousand people marched for four days from Trafalgar Square, London, to the Atomic Weapons Research Establishment close to Aldermaston in Berkshire, England, to demonstrate their opposition to nuclear weapons. The Aldermaston marches continued into the late 1960s when tens of thousands of people took part in the four-day marches.In 1959, a letter in the Bulletin of the Atomic Scientists was the start of a successful campaign to stop the Atomic Energy Commission dumping radioactive waste in the sea 19 kilometres from Boston. In 1962, Linus Pauling won the Nobel Peace Prize for his work to stop the atmospheric testing of nuclear weapons, and the ""Ban the Bomb"" movement spread.In 1963, many countries ratified the Partial Test Ban Treaty prohibiting atmospheric nuclear testing. Radioactive fallout became less of an issue and the anti-nuclear weapons movement went into decline for some years. A resurgence of interest occurred amid European and American fears of nuclear war in the 1980s.According to an audit by the Brookings Institution, between 1940 and 1996, the U.S. spent $9.08 trillion in present-day terms on nuclear weapons programs. 57 percent of which was spent on building nuclear weapons delivery systems. 6.3 percent of the total, $570 billion in present-day terms, was spent on environmental remediation and nuclear waste management, for example cleaning up the Hanford site, and 7 percent of the total, $638 billion was spent on making nuclear weapons themselves.Peaceful nuclear explosions are nuclear explosions conducted for non-military purposes, such as activities related to economic development including the creation of canals. During the 1960s and 70s, both the United States and the Soviet Union conducted a number of PNEs. Six of the explosions by the Soviet Union are considered to have been of an applied nature, not just tests.Subsequently, the United States and the Soviet Union halted their programs. Definitions and limits are covered in the Peaceful Nuclear Explosions Treaty of 1976. The Comprehensive Nuclear-Test-Ban Treaty of 1996, once it enters into force, will prohibit all nuclear explosions, regardless of whether they are for peaceful purposes or not.Thomas Powers, ""The Nuclear Worrier"" (review of Daniel Ellsberg, The Doomsday Machine: Confessions of a Nuclear War Planner, New York, Bloomsbury, 2017, ISBN 9781608196708, 420 pp.), The New York Review of Books, vol. LXV, no. 1 (18 January 2018), pp. 13–15.Eric Schlosser, Command and Control: Nuclear Weapons, the Damascus Accident, and the Illusion of Safety, Penguin Press, 2013, ISBN 1594202273. The book became the basis for a 2-hour 2017 PBS American Experience episode, likewise titled ""Command and Control"". Nuclear weapons continue to be equally hazardous to their owners as to their potential targets. Under the 1970 Treaty on the Non-Proliferation of Nuclear Weapons, nuclear-weapon states are obliged to work toward the elimination of nuclear weapons.Nuclear Weapon Archive from Carey Sublette is a reliable source of information and has links to other sources and an informative FAQ.The Federation of American Scientists provide solid information on weapons of mass destruction, including nuclear weapons and their effectsAlsos Digital Library for Nuclear Issues – contains many resources related to nuclear weapons, including a historical and technical overview and searchable bibliography of web and print resources.Video archive of US, Soviet, UK, Chinese and French Nuclear Weapon Testing at sonicbomb.comThe National Museum of Nuclear Science & History (United States) – located in Albuquerque, New Mexico a Smithsonian Affiliate MuseumNuclear Emergency and Radiation ResourcesThe Manhattan Project: Making the Atomic Bomb at AtomicArchive.comLos Alamos National Laboratory: History (U.S. nuclear history)Race for the Superbomb, PBS website on the history of the H-bombRecordings of recollections of the victims of Hiroshima and NagasakiThe Woodrow Wilson Center's Nuclear Proliferation International History Project or NPIHP is a global network of individuals and institutions engaged in the study of international nuclear history through archival documents, oral history interviews and other empirical sources.NUKEMAP3D – a 3D nuclear weapons effects simulator powered by Google Maps.";environmental disaster;Nuclear weapon;0
The Ok Tedi is a river in New Guinea. The Ok Tedi Mine is located near the headwaters of the river, which is sourced in the Star Mountains. Nearly the entirety of the river runs through the North Fly District of the Western Province of Papua New Guinea, but the river crosses the international boundary with Indonesia for less than one kilometre. The largest settlement of the Western Province, Tabubil is located near its banks.Known as the Ok Tedi River by the Yonggom people who live on its western bank, it was renamed the Alice River by the Italian explorer Luigi d'Albertis. Ok is the word for water or river in the Ok languages family. It is a tributary of the Fly River. Tributaries of the Ok Tedi include the Birim.The Kiunga-Tabubil Highway runs parallel with the river for the majority of its course, until just south of Ningerum where the highway veers southeast towards Kiunga, a port town on the Fly River.The river is extremely fast-moving and has a massive capacity. It is situated on a sand bank, which allows for the river to change course quickly without warning. The sand conditions underneath the river and the extremely high rainfall of the catchment area make it one of the fastest moving rivers in the world. The roar from the river can be heard for many kilometres through the dense jungle of the district.The pristine river was devastated by the Ok Tedi environmental disaster, an enormous open pit copper/gold mine which discharges waste directly into the river. Until its recent seizure by the Papua New Guinea government the mine was owned by BHP Billiton, an Anglo-Australian multinational mining, metals and petroleum company headquartered in Melbourne, Australia. It is the world's largest mining company measured by 2013 revenues.Ok Tedi Environmental DisasterDescription and maps of the Fly River system, including the Ok Tedi River;environmental disaster;Ok Tedi;0
"The Ok Tedi environmental disaster caused severe harm to the environment along 1,000 kilometres (620 mi) of the Ok Tedi River and the Fly River in the Western Province of Papua New Guinea between about 1984 and 2013. The lives of 50,000 people have been disrupted. One of the worst environmental disasters caused by humans, it is a consequence of the discharge of about two billion tons of untreated mining waste into the Ok Tedi from the Ok Tedi Mine, an open pit mine in the Western Province of Papua New Guinea.This mining pollution, caused by the collapse of the Ok Tedi tailings dam system in 1984 and consequent switch to riverine disposal (disposal of tailings directly into the river) for several decades, was the subject of class action litigation, naming Ok Tedi Mining Limited and BHP Billiton and brought by local landowners. Villagers downstream from Ok Tedi in the Fly River system in the Middle Fly District and the southern and central areas of the North Fly District, in particular, believe that the effect on their livelihood from this disaster far outweighs the benefits they have received from the mine's presence in their area.In 1999, BHP reported that 90 million tons of mine waste was annually discharged into the river for more than ten years and destroyed downstream villages, agriculture and fisheries. Mine wastes were deposited along 1,000 kilometres (620 mi) of the Ok Tedi and the Fly River below its confluence with the Ok Tedi, and over an area of 100 square kilometres (39 sq mi). BHP's CEO, Paul Anderson, said that the Ok Tedi Mine was ""not compatible with our environmental values and the company should never have become involved."" As of 2006, mine operators continued to discharge 80 million tons of tailings, overburden and mine-induced erosion into the river system each year. About 1,588 square kilometres (613 sq mi) of forest has died or is under stress. As many as 3,000 square kilometres (1,200 sq mi) may eventually be harmed, an area equal to the U.S. state of Rhode Island or the Danish island of Funen.Following heavy rainfall, mine tailings are swept into the surrounding rain forest, swamps and creeks, and have left behind 30 square kilometers of dead forest. Thick gray sludge from the mine is visible throughout the Fly River system, although its effects downriver are not as severe. Chemicals from the tailings killed or contaminated fish, although they are still eaten by the people of the surrounding villages. However, fish counts decrease closer to the mine. The massive amount of mine-derived waste dumped into the river exceeded its carrying capacity. This dumping resulted in the river bed being raised 10 m, causing a relatively deep and slow river to become shallower and develop rapids, thereby disrupting indigenous transportation routes. Flooding, caused by the raised riverbed, left a thick layer of contaminated mud on the flood plain among plantations of taro, bananas and sago palm that are the staples of the local diet. About 1300 square kilometers were damaged in this way. The concentration of copper in the water is about 30 times above the standard level, but it is below the World Health Organisation (WHO) standards.The original plans included an Environmental Impact Statement that required a tailings dam be built. This would allow heavy metals and solid particles to settle, before releasing the clean ‘high-water’ into the river system where remaining contaminants would be diluted. In 1984 an earthquake caused the half built dam to collapse. The company continued operations without the dam, initially because BHP argued that it would be too expensive to rebuild it. Subsequently, the PNG government decided a dam wasn’t necessary, in the wake of the closure of the Panguna mine.Most of Papua New Guinea's land is held under a system of native title, with ownership divided amongst many small clans, while the central government retains control over how resources that lie under the ground are used.There are no waste retention facilities on the premises. This allowed all ore processing residues, waste rock and overburden to be discharged into the Ok Tedi River.In the 1990s the communities of the lower Fly Region, including the Yonggom people, sued BHP and received US$28.6 million in an out-of-court settlement, which was the culmination of an enormous public-relations campaign against the company by environmental groups. As part of the settlement a (limited) dredging operation was put in place and efforts were made to rehabilitate the site around the mine. However the mine is still in operation and waste continues to flow into the river system. BHP was granted legal indemnity from future mine related damages.The Ok Tedi Mine was scheduled to close in 2013. However, the PNG Government has taken over control of the mine and with support of local community, the mine life was extended. Until that time two thirds of the profits go into a long-term fund to enable the mine to continue to contribute to the PNG economy for up to half a century after it closes. The balance is allocated to current development programs in the local area (Western Province) and PNG more generally. Experts have predicted that it will take 300 years to clean up the toxic contamination.In 2013, the PNG Government seized 100% ownership of Ok Tedi Mine and repealed laws that would allow people to sue mining giant BHP Billiton over environmental damage. Ok Tedi Mining Limited launched the OT2025 project that was focused on transitioning the business to a smaller operation in preparation for Mine Life Extension.Community consent for the mine’s life to be extended to 2025 was endorsed by the Mine Associated Communities, which is made up of 156 villages, through the signing of the respective Community Mine Continuation Extension Agreements by the Community representatives and OTML at the end of 2012 and beginning of 2013. The signing of the Agreements facilitated the Company to commence planning for the MLE project throughout 2013.The Ok Tedi Mining LimitedMineral Policy Institute - Cracks in the Facade of BHP's exit from Ok Tedi Mining Disaster Appear (22 Jan 2007)";environmental disaster;Ok Tedi environmental disaster;0
"In mining, overburden (also called waste or spoil) is the material that lies above an area that lends itself to economical exploitation, such as the rock, soil, and ecosystem that lies above a coal seam or ore body. Overburden is distinct from tailings, the material that remains after economically valuable components have been extracted from the generally finely milled ore. Overburden is removed during surface mining, but is typically not contaminated with toxic components. Overburden may also be used to restore an exhausted mining site to a semblance of its appearance before mining began.A related term is interburden, meaning material that lies between two areas of economic interest, such as the material separating coal seams within strata.Overburden may also be used as a term to describe all soil and ancillary material above the bedrock horizon in a given area.By analogy, overburden is also used to describe the soil and other material that lies above a specific geologic feature, such as a buried astrobleme, or above an unexcavated site of archeological interest.In particle physics, the overburden of an underground laboratory may be important to shield the facility from cosmic radiation that can interfere with experiments.In arboriculture, the word is also used for the soil over the top of the roots of a tree collected from the wild.GangueSpoil tipBates, R.L., and Jackson, J.A., (1987) Glossary of geology American Geological Institute, Alexandria, Virginia.Haering, K. C. Daniels W. L. and Roberts J. A. (1993) ""Changes in mine soil properties resulting from overburden weathering"" Journal of environmental quality 22(1): pp. 194–200.McFee, W.W. Byrnes, W.R. and Stockton, J.G. (1981) ""Characteristics of coal mine overburden important to plant growth"" Journal of environmental quality 10(3): pp. 300–308. The dictionary definition of overburden at Wiktionary";environmental disaster;Overburden;0
"The Prestige oil spill was an oil spill in Galicia, Spain, caused by the sinking of the 26 year old structurally deficient oil tanker MV Prestige in November 2002, carrying 77,000 tonnes of heavy fuel oil. During a storm, it burst a tank on November 13, and sank on November 19, 2002. The spill polluted thousands of kilometers of coastline and more than one thousand beaches on the Spanish, French and Portuguese coast, as well as causing great harm to the local fishing industry. The spill is the largest environmental disaster in the history of both Spain and Portugal. The amount of oil spilled was more than the Exxon Valdez and the toxicity considered higher, because of the higher water temperatures.In 2007 the Southern District of New York dismissed a 2003 lawsuit by the Kingdom of Spain against the American Bureau of Shipping, the international classification society which had certified the ""Prestige"" as in compliance with rules and laws, because ABS was a ""person"" per the International Convention on Civil Liability for Oil Pollution Damage and, exempt from direct liability for pollution damage. The 2012 trial of the Galicia regional High Court did not find the merchant shipping company, nor the insurer, the London P&I Club nor any Spanish government official, but only the Captain of the ship guilty and gave him a nine-month suspended sentence for disobedience. By November 2017, the London P&I Club was fined to pay $1 billion.The Prestige was a 26-year-old Greek-operated, single-hulled oil tanker, officially registered in the Bahamas, but with a Liberian-registered single-purpose corporation as the owner.The ship had a deadweight tonnage, or carrying capacity, of approximately 81,000 tons, a measurement that put it at the small end of the Aframax class of tankers, smaller than most carriers of crude oil but larger than most carriers of refined products. It was classed by the American Bureau of Shipping and insured by the London Steam-Ship Owners' Mutual Insurance Association, a shipowners' mutual known as the London Club.On November 13, 2002, the Prestige was carrying 77,000 metric tons of two different grades of heavy fuel oil, crude #4. It encountered a winter storm off Costa de la Muerte, the Coast of Death, in Galicia northwestern Spain. The Greek captain, Apostolos Mangouras, reported a loud bang from the starboard side and as the ship began to take on water from high waves the engines shut down and he  called for help from Spanish rescue workers. The Filipino crew was evacuated with rescue helicopters and the ship drifted within four miles of the Spanish coast already leaking oil. A veteran captain, Serafin Diaz, was lowered onto the ship per the Spanish governments Industry Ministry, to navigate the ship off the Spanish coast northwest, and saw the gaping 50-foot hole on the starboard side. Mangouras argued the ship should be brought into port where the leaking oil might be confined  but under the threat of the Spanish navy Mangouras relented. After pressure from the French government, the vessel was also forced to change its course and head south into Portuguese waters in order to avoid endangering France's southern coast. Fearing for its own shore, the Portuguese authorities ordered its navy to intercept the ailing vessel and prevent it from approaching further.With the French, Spanish and Portuguese governments refusing to allow the ship to dock, and after several days of sailing and towing, it split in half on November 19, 2002. It sank only about 250 kilometers or 130 miles from the Spanish coast, releasing over 20 million US gallons (76,000 m3) of oil into the sea. An earlier oil slick had already reached the coast. The captain of the Prestige was taken into custody, accused of not cooperating with marine salvage crews and of harming the environment.The Prestige oil spill is Spain's worst ecological disaster. After the sinking, the wreck continued to leak approximately 125 tons of oil a day, polluting the seabed and contaminating the coastline, especially along the territory of Galicia. The environmental damage was most severe on the coast of Galicia. The affected area is an important ecological region, supporting coral reefs and many species of sharks and birds, and the fishing industry. The heavy coastal pollution forced the region's government to suspend offshore fishing for six months.Initially, the government thought just 17,000 tons of the tanker's 77,000 tons of oil had been lost, and that the remaining 60,000 tons would freeze and not leak from the sunken tanker. In early 2003, it announced that half of the oil had been lost. As of August 2003, the figure had risen to about 63,000 tons, more than eighty percent of the tanker's 77,000 tons of fuel oil have been spilled off Spain's north-west coast.In March 2006, new oil slicks were detected near the wreck of the Prestige, slicks which investigators found to match the type of oil the Prestige carried. A study released in December 2006 led by José Luis De Pablos, a physicist at Madrid's Center for Energetic and Environmental Research, concluded that 16,000 to 23,000 tons of oil remained in the wreck, as opposed to the 700 to 1300 tons claimed by the Spanish government that bioremediation of the remaining oil failed and that bacteria corroding the hull could soon produce a rupture and quickly release much of the remaining oil and create another catastrophic spill. The report urged the government to take ""prompt"" action.Experts predicted marine life could suffer from the pollution for at least ten years due to the type of oil spill, which contained light fractions with polyaromatic hydrocarbons and could poison plankton, fish eggs and crustaceans with carcinogenic effects in fish and potentially humans as well.In the subsequent months, thousands of volunteers joined the public company TRAGSA (the firm chosen by the regional government to deal with the cleanup) to help clean the affected coastline. The massive cleaning campaign was a success, recovering most portions of coastline from not only the effects of the oil spill but also the accumulated ""regular "" contamination. Galician activists founded the environmental movement Nunca Máis (Galician for Never Again), to denounce the passiveness of the conservative government regarding the disaster.A year after the spill, Galicia had more Blue Flags for its beaches (an award for those beaches with the highest standards in the European Union) than in the previous years.In 2004, remotely operated vehicles (ROVs) like the one which originally explored the wreck of the RMS Titanic drilled small holes in the wreck and removed the remaining 13,000 m³ of cargo oil from the wreck, at 4000 meters below the sea surface. The ROVs also sealed cracks in the tanker's hull, and slowed the leakage to a trickle of 20 litres a day. In total, 20 million US gallons (76,000 m3) of oil were spilled. The oil was then pumped into large aluminium shuttles, specially manufactured for this salvage operation. The filled shuttles were then floated to the surface. The original plan to fill large bags with the oil proved to be too problematic and slow. After the oil removal was completed, a slurry rich in microbiologic agents was pumped in the hold to speed up the breakdown of any remaining oil. The total estimated cost of the operation was over €100 million.The massive environmental and financial costs of the spill resulted in an inquiry into how a structurally deficient ship was able to travel out to sea. The ""Prestige"" had set sail from St. Petersburg, Russia, without being properly inspected. It traveled to the Atlantic via the shallow and vulnerable Baltic Sea. A previous captain in St. Petersburg, Esfraitos Kostazos, who complained to the owners about numerous structural deficiencies within the ship was rebuffed, later resigned in protest, and rather than repairing the defects, he was replaced with Mangouras.The ownership of the Prestige had been unclear, making it difficult to determine exactly who is responsible for the oil spill. Thus the sinking of the ""Prestige"" has exposed the difficulties in regulations posed by flags of convenience. For unknown reasons, the 26 year old ""Prestige"" was loaded with crude oil number 4, and not scrapped.Spanish investigators found that the failure in the hull of the ""Prestige"" had been predicted already: her two sister ships, ""Alexandros"" and ""Centaur"", had been submitted to extensive inspections under the ""Safe Hull"" program in 1996. The organization in charge of the inspections, the American Bureau of Shipping, found that both ""Alexandros"" and ""Centaur"" were in terminal decline. Due to metal fatigue in their hulls, modeling predicted that both ships would fail between frames 61 and 71 within five years. ""Alexandros"", ""Centaur"" and a third sister-ship, ""Apanemo"", were scrapped between 1999 and 2002. Little more than five years after the inspection, Prestuige´s  hull failed between frames 61 and 71.The Spanish government was criticized for its decision to tow the ailing wreck out to sea — where it split in two — rather than in to a port. The refusal to allow the ship to take refuge in a sheltered port has been called a major contributing factor to the scale of the disaster. World Wildlife Fund's senior policy officer for shipping Simon Walmsley believed most of the blame lay with the classification society. ""It was reported as being substandard at one of the ports it visited before Spain. The whole inspection regime needs to be revamped and double-hulled tankers used instead,"" he said. The US and most other countries were planning to phase out single-hulled tankers by 2012.A report by the Galicia-based Barrie de la Maza economic institute in 2003 criticised the Spanish government's handling of the catastrophe. It estimated the cost of the clean-up to the Galician coast alone at €2.5 billion.The 2013 court ruling put the cost of the disaster at 368 million euros ($494 million) to the Spanish state, 145 million euros to the Spanish region of Galicia and 68 million euros to France. The clean-up of the Exxon Valdez cost US$3 billion (almost €2.2 billion).Since the disaster, oil tankers similar to the Prestige have been directed away from the French and Spanish coastlines. The European Commissioner for Transport at the time, Spaniard Loyola de Palacio, pushed for the ban of single-hulled tankers.The immediate legal consequence of the disaster was the arrest of the captain, Captain Mangouras. Captain Mangouras sought refuge for his seriously damaged vessel in a Spanish port. The acceptance of such a request has deep historic roots. Spain refused and launched the criminal charges against Mangouras that he refused to comply immediately with the Spanish demand to restart the engines and steam offshore. Bringing the ship into port and booming around her to contain the leaking oil would have been less harmful than sending her back to sea and almost inevitable sinking.In May 2003, the Kingdom of Spain brought civil suit in the Southern District of New York against the American Bureau of Shipping (ABS), the Houston-based international classification society which had certified the ""Prestige"" as ""in class"" for its final voyage.  The ""in class"" status states that the vessel is in compliance with all applicable rules and laws, not that it is or is not safe. For the world maritime industry, a key issue raised by the incident was whether classification societies could be held responsible for the consequences. International maritime trade publications including TradeWinds, Fairplay and Lloyd's List regularly presented the dispute as a possibly precedent-setting one which could prove fateful for international classification societies, whose assets are dwarfed by the scale of claims to which they could become subject. On 2 January 2007, the docket in that lawsuit (SDNY 03-cv-03573) was dismissed. The presiding judge ruled that ABS is a ""person"" as defined by the International Convention on Civil Liability for Oil Pollution Damage (CLC) and, as such, is exempt from direct liability for pollution damage. Additionally, the Judge ruled that, since the United States was not a signatory to the International CLC, the US Courts lack the necessary jurisdiction to adjudicate the case. Spain's original damage claim against ABS was some $700 million.The Galicia regional High Court set the Prestige oil spill trial date for October 16, 2012, against officers, the insurer London Club, International Fund for Compensation for Oil Pollution Damage, the ship's owner, Liberia-based Mare Shipping Inc with its director general. The harbor master of A Coruña at the time, Ángel del Real, and a Galician government delegate, Arsenio Fernández de Mesa, had also been charged with ""aggravating the disaster by ignoring technical advice"". the hearing began on 16 June 2012 and expected to adjourn in November, the tenth anniversary of the disaster. The trial was held in a specially constructed courtroom in A Coruña’s exhibition complex. It considered evidence from 133 witnesses and 98 experts. Plaintiffs asked the Greek captain to be sentenced up 12 years, and demanded more than 4 billion euros ($5.0 billion) in damages overall.In November 2013, the three Galicia High Court judges concluded, it was impossible to establish criminal responsibility, and Captain Apostolos Mangouras, Chief Engineer Nikolaos Argyropoulos and the former head of Spain's Merchant Navy, Jose Luis Lopez, were found not guilty of crimes against the environment. The captain was however accused of disobeying government authorities who wanted the tanker as far from the coast as possible. According to the court, that decision was correct and Mangouras, 78 at the time, was found guilty of disobedience and given a nine-month suspended sentence. The Spanish government decided to launch an appeal to the ruling against the exemption from civil liability of the captain.On 26 January 2016, Spain’s Supreme Court convicted Captain Mangouras of recklessness resulting in catastrophic environmental damage, and sentenced him to two years in prison.  On 15 November 2017, London Club was ordered to pay a $1 billion fine over the oil spill.Flag of convenienceHealth crisisManfred Gnädinger, human victim of spillManuel Rivas, Galician writerPerte totale suite à avarie de coque du petrolier bahaméen Prestige survenue dans l'ouest de la Galice."" (Archive) - Bureau d'Enquêtes sur les Événements de Mer - 13–19 November 2002 (in French)Plataforma Nunca Máis, Prestige activists (in Galician)Coordination Technical Bureau of Scientific Intervention Program against Accidental Marine Spills, This bureau is responsible for management and coordination works of research projects.Spill Response Experience Coordination Action - SPREEX This project is responsible for management and coordination research needs in Europe.Raul Garcia The Prestige: one year on, a continuing disaster WWF, November 2003, 23 pp.WWF crisis response page on PrestigePrestige Oil Spill - first hand story of a Canadian volunteer, with photosMaps and statistics of affected coastal area";environmental disaster;Prestige oil spill;0
"The radiation effects from the Fukushima Daiichi nuclear disaster are the observed and predicted effects  as a result of the release of radioactive isotopes from the Fukushima Daiichi Nuclear Power Plant following the 2011 Tōhoku earthquake and tsunami. The release of radioactive isotopes from reactor containment vessels was a result of venting in order to reduce gaseous pressure, and the discharge of coolant water into the sea. This resulted in Japanese authorities implementing a 20-km exclusion zone around the power plant and the continued displacement of approximately 156,000 people as of early 2013. Trace quantities of radioactive particles from the incident, including iodine-131 and caesium-134/137, have since been detected around the world.The World Health Organization (WHO) released a report that estimates an increase in risk for specific cancers for certain subsets of the population inside the Fukushima Prefecture. A 2013 WHO report predicts that for populations living in the most affected areas there is a 70% higher risk of developing thyroid cancer for girls exposed as infants (the risk has risen from a lifetime risk of 0.75% to 1.25%), a 7% higher risk of leukemia in males exposed as infants, a 6% higher risk of breast cancer in females exposed as infants and a 4% higher risk, overall, of developing solid cancers for females.Preliminary dose-estimation reports by WHO and the United Nations Scientific Committee on the Effects of Atomic Radiation (UNSCEAR) indicate that, outside the geographical areas most affected by radiation, even in locations within Fukushima prefecture, the predicted risks remain low and no observable increases in cancer above natural variation in baseline rates are anticipated. In comparison, after the Chernobyl accident, only 0.1% of the 110,000 cleanup workers surveyed have so far developed leukemia, although not all cases resulted from the accident. However, 167 Fukushima plant workers received radiation doses that slightly elevate their risk of developing cancer. Estimated effective doses from the accident outside of Japan are considered to be below, or far below the dose levels regarded as very small by the international radiological protection community. The United Nations Scientific Committee on the Effects of Atomic Radiation is expected to release a final report on the effects of radiation exposure from the accident by the end of 2013.A June 2012 Stanford University study estimated, using a linear no-threshold model, that the radioactivity release from the Fukushima Daiichi nuclear plant could cause 130 deaths from cancer globally (the lower bound for the estimate being 15 and the upper bound 1100) and 199 cancer cases in total (the lower bound being 24 and the upper bound 1800), most of which are estimated to occur in Japan. Radiation exposure to workers at the plant was projected to result in 2 to 12 deaths. However, a December 2012 UNSCEAR statement to the Fukushima Ministerial Conference on Nuclear Safety advised that ""because of the great uncertainties in risk estimates at very low doses, UNSCEAR does not recommend multiplying very low doses by large numbers of individuals to estimate numbers of radiation-induced health effects within a population exposed to incremental doses at levels equivalent to or lower than natural background levels.""Preliminary dose-estimation reports by the World Health Organization and United Nations Scientific Committee on the Effects of Atomic Radiation indicate that 167 plant workers received radiation doses that slightly elevate their risk of developing cancer, however like the Chernobyl nuclear disaster that it may not be statistically detectable. After the Chernobyl accident, only 0.1% of the 110,000 cleanup workers surveyed have so far developed leukemia, although not all cases resulted from the accident Estimated effective doses from the accident outside Japan are considered to be below (or far below) the dose levels regarded as very small by the international radiological protection community.According to the Japanese Government, 180,592 people in the general population were screened in March 2011 for radiation exposure and no case was found which affects health. Thirty workers conducting operations at the plant had exposure levels greater than 100 mSv. It is believed that the health effects of the radioactivity release are primarily psychological rather than physical effects. Even in the most severely affected areas, radiation doses never reached more than a quarter of the radiation dose linked to an increase in cancer risk (25 mSv whereas 100 mSv has been linked to an increase in cancer rates among victims at Hiroshima and Nagasaki). However, people who have been evacuated have suffered from depression and other mental health effects.While there were no deaths caused by radiation exposure, approximately 18,500 people died due to the earthquake and tsunami. Very few cancers would be expected as a result of the very low radiation doses received by the public. John Ten Hoeve and Stanford University professor Mark Z. Jacobson suggest that according to the linear no-threshold model (LNT) the accident is most likely to cause an eventual total of 130 (15-1100) cancer deaths, while noting that the validity of the LNT model at such low doses remains the subject of debate. Radiation epidemiologist Roy Shore contends that estimating health effects in a population from the LNT model ""is not wise because of the uncertainties"". The LNT model did not accurately model casualties from Chernobyl, Hiroshima or Nagasaki it greatly overestimated the casualties. Evidence that the LNT model is a gross distortion of damage from radiation has existed since 1946, and was suppressed by Nobel Prize winner Hermann Muller in favour of assertions that no amount of radiation is safe.In 2013 (two years after the incident), the World Health Organization indicated that the residents of the area who were evacuated were exposed to so little radiation that radiation induced health impacts are likely to be below detectable levels. The health risks in the WHO assessment attributable to the Fukushima radioactivity release were calculated by largely applying the conservative Linear no-threshold model of radiation exposure, a model that assumes even the smallest amount of radiation exposure will cause a negative health effect.The World Health Organization (WHO) report released in 2013 predicts that for populations living around the Fukushima nuclear power plant there is a 70% higher relative risk of developing thyroid cancer for females exposed as infants, and a 7% higher relative risk of leukemia in males exposed as infants and a 6% higher relative risk of breast cancer in females exposed as infants. With the WHO communicating that the values stated in that section of their report are relative increases, and not representative of the absolute increase of developing these cancers, as the lifetime absolute baseline chance of developing thyroid cancer in females is 0.75%, with the Radiation-induced cancer chance now predicted to increase that 0.75% to 1.25%, with this 0.75% to 1.25% change being responsible for the ""70% higher relative risk"":These percentages represent estimated relative increases over the baseline rates and are not absolute risks for developing such cancers. Due to the low baseline rates of thyroid cancer, even a large relative increase represents a small absolute increase in risks. For example, the baseline lifetime risk of thyroid cancer for females is just (0.75%)three-quarters of one percent and the additional lifetime risk estimated in this assessment for a female infant exposed in the most affected location is (0.5%)one-half of one percent.The WHO calculations determined that the most at-risk group, infants, who were in the most affected area, would experience an absolute increase in the risk of cancer (of all types) during their lifetime, of approximately 1% due to the accident. With the lifetime risk increase for thyroid cancer, due to the accident, for a female infant, in the most affected radiation location, being estimated to be one half of one percent[0.5%]. Cancer risks for the unborn child are considered to be similar to those in 1 year old infants.The estimated risk of cancer to people who were children and adults during the Fukushima accident, in the most affected area, was determined to be lower again when compared to the most at-risk group—infants. A thyroid ultrasound screening programme is currently (2013) ongoing in the entire Fukushima prefecture this screening programme is, due to the screening effect, likely to lead to an increase in the incidence of thyroid disease due to early detection of non-symptomatic disease cases.About one-third of people (~30%) in industrialized nations are presently diagnosed with cancer during their lifetimes. Radiation exposure can increase cancer risk, with the cancers that arise being indistinguishable from cancers resulting from other causes.In the general population, no increase is expected in the frequency oftissue reactions attributable to radiation exposure and no increase is expected in the incidence of congenital or developmental abnormalities, including cognitive impairment attributable to in-utero radiation exposure. No significant increase in heritable effects has been found in studies of the children of the survivors of the atomic bombings of Hiroshima and Nagasaki or in the offspring of cancer survivors treated with radiotherapy, which indicates that moderate acute radiation exposures have little impacton the overall risk of heritable effects in humans.As of August 2013, there have been more than 40 children newly diagnosed with thyroid cancer and other cancers in Fukushima prefecture. 18 of these were diagnosed with thyroid cancer, but these cancers are not attributed to radiation from Fukushima, as similar patterns occurred before the accident in 2006 in Japan, with 1 in 100,000 children per year developing thyroid cancer in that year, that is, this is not higher than the pre-accident rate. While controversial scientist Christopher Busby disagrees, claiming the rate of thyroid cancer in Japan was 0.0 children per 100,000 in 2005, the Japan Cancer Surveillance Research Group showed a thyroid cancer rate of 1.3 per 100,000 children in 2005 based on official cancer cases. As a point of comparison, thyroid cancer incidence rates after the Chernobyl accident of 1986 did not begin to increase above the prior baseline value of about 0.7 cases per 100,000 people per year until 1989 to 1991, 3 to 5 years after the accident in both the adolescent and children age groups. Therefore, data from Chernobyl suggests that an increase in thyroid cancer around Fukushima is not expected to begin to be seen until at least 3 to 5 years after the accidentAccording to the Tenth Report of the Fukushima Prefecture Health Management Survey released in February 2013, more than 40% of children screened around Fukushima prefecture were diagnosed with thyroid nodules or cysts. Ultrasonographic detectable thyroid nodules and cysts are extremely common and can be found at a frequency of up to 67% in various studies. 186 (0.5%) of these had nodules larger than 5.1 mm and/or cysts larger than 20.1 mm and underwent further investigation. None had thyroid cancer. An RT report into the matter was highly misleading. Fukushima Medical University gives the number of children diagnosed with thyroid cancer as of December 2013 as 33 and concluded, ""[I]t is unlikely that these cancers were caused by the exposure from 131I from the nuclear power plant accident in March 2011"".Thyroid cancer is one of the most survivable cancers, with an approximate 94% survival rate after first diagnosis. That rate increases to a 100% survival rate with catching it early.A 2013 article in the Stars and Stripes asserted that a Japanese government study released in February of that year had found that more than 25 times as many people in the area had developed thyroid cancer compared with data from before the disaster.As part of the ongoing precautionary ultrasound screening program in and around Fukushima, (36%) of children in Fukushima Prefecture in 2012 were found to have thyroid nodules or cysts, but these are not considered abnormal. This screening programme is, due to the screening effect, likely, according to the WHO, to lead to an increase in the incidence of the diagnosis of thyroid disease due to early detection of non-symptomatic disease cases. For example, the overwhelming majority of thyroid growths prior to the accident, and in other parts of the world, are overdiagnosed (that is, a benign growth that will never cause any symptoms, illness, or death for the patient, even if nothing is ever done about the growth) with autopsy studies, again done prior to the accident and in other parts of the world, on people who died from other causes showing that more than one third (33%+), of adults technically has a thyroid growth/cancer, but it is benign/never caused them any harm.Thyroid cancer is one of the most survivable cancers, with an approximate 94% survival rate after first diagnosis, and that rate increases to a 100% survival rate with catching it early. For example, from 1989 to 2005, an excess of 4000 children and adolescent cases of thyroid cancer were observed in those who lived around Chernobyl of these 4000 people, nine have died so far, a 99% survival rate.A survey by the newspaper Mainichi Shimbun computed that there were 1,600 deaths related to the evacuation, comparable to the 1,599 deaths due to the earthquake and tsunami in the Fukushima Prefecture.In the former Soviet Union, many patients with negligible radioactive exposure after the Chernobyl disaster displayed extreme anxiety about low level radiation exposure, and therefore developed many psychosomatic problems, including radiophobia, and with this an increase in fatalistic alcoholism being observed. As Japanese health and radiation specialist Shunichi Yamashita noted: We know from Chernobyl that the psychological consequences are enormous. Life expectancy of the evacuees dropped from 65 to 58 years—not [predominantly] because of cancer, but because of depression, alcoholism and suicide. Relocation is not easy, the stress is very big. We must not only track those problems, but also treat them. Otherwise people will feel they are just guinea pigs in our research.A survey by the Iitate, Fukushima local government obtained responses from approximately 1,743 people who have evacuated from the village, which lies within the emergency evacuation zone around the crippled Fukushima Daiichi Plant. It shows that many residents are experiencing growing frustration and instability due to the nuclear crisis and an inability to return to the lives they were living before the disaster. Sixty percent of respondents stated that their health and the health of their families had deteriorated after evacuating, while 39.9% reported feeling more irritated compared to before the disaster.Summarizing all responses to questions related to evacuees' current family status, one-third of all surveyed families live apart from their children, while 50.1% live away from other family members (including elderly parents) with whom they lived before the disaster. The survey also showed that 34.7% of the evacuees have suffered salary cuts of 50% or more since the outbreak of the nuclear disaster. A total of 36.8% reported a lack of sleep, while 17.9% reported smoking or drinking more than before they evacuated.Experts on the ground in Japan agree that mental health challenges are the most significant issue. Stress, such as that caused by dislocation, uncertainty and concern about unseen toxicants, often manifests in physical ailments, such as heart disease. So even if radiation risks are low, people are still concerned and worried. Behavioral changes can follow, including poor dietary choices, lack of exercise and sleep deprivation, all of which can have long-term negative health consequences. People who lost their homes, villages and family members, and even just those who survived the quake, will likely continue to face mental health challenges and the physical ailments that come with stress. Much of the damage was really the psychological stress of not knowing and of being relocated, according to U.C. Berkeley's McKone.On 24 May 2012, more than a year after the disaster, TEPCO released their estimate of radioactivity releases due to the Fukushima Daiichi Nuclear Disaster. An estimated 538.1  petabecquerels (PBq) of iodine-131, caesium-134 and caesium-137 was released. 520 PBq was released into the atmosphere between 12 and 31 March 2011 and 18.1 PBq into the ocean from 26 March to 30 September 2011. A total of 511 PBq of iodine-131 was released into both the atmosphere and the ocean, 13.5 PBq of caesium-134 and 13.6 PBq of caesium-137. In May 2012, TEPCO reported that at least 900 PBq had been released ""into the atmosphere in March last year [2011] alone"" up from previous estimates of 360-370 PBq total.The primary releases of radioactive nuclides have been iodine and caesium strontium and plutonium have also been found. These elements have been released into the air via steam and into the water leaking into groundwater or the ocean. The expert who prepared a frequently cited Austrian Meteorological Service report asserted that the ""Chernobyl accident emitted much more radioactivity and a wider diversity of radioactive elements than Fukushima Daiichi has so far, but it was iodine and caesium that caused most of the health risk – especially outside the immediate area of the Chernobyl plant."" Iodine-131 has a half-life of 8 days while caesium-137 has a half-life of over 30 years. The IAEA has developed a method that weighs the ""radiological equivalence"" for different elements. TEPCO has published estimates using a simple-sum methodology, As of  25 April 2012 TEPCO has not released a total water and air release estimate.According to a June 2011 report of the International Atomic Energy Agency (IAEA), at that time no confirmed long-term health effects to any person had been reported as a result of radiation exposure from the nuclear accident.According to one expert, the release of radioactivity is about one-tenth that from the Chernobyl disaster and the contaminated area is also about one-tenth that of Chernobyl.A 12 April report prepared by NISA estimated the total release of iodine-131 was 130 PBq and caesium-137 at 6.1 PBq. On 23 April the NSC updated its release estimates, but it did not reestimate the total release, instead indicating that 154 TBq of air release were occurring daily as of 5 April.On 24 August 2011, the Nuclear Safety Commission (NSC) of Japan published the results of the recalculation of the total amount of radioactive materials released into the air during the incident at the Fukushima Daiichi Nuclear Power Station. The total amounts released between 11 March and 5 April were revised downwards to 130 PBq for iodine-131 (I-131) and 11 PBq for caesium-137 (Cs-137). Earlier estimations were 150 PBq and 12 PBq.On 20 September the Japanese government and TEPCO announced the installation of new filters at reactors 1, 2 and 3 to reduce the release of radioactive materials into the air. Gases from the reactors would be decontaminated before they would be released into the air. In the first half of September 2011 the amount of radioactive substances released from the plant was about 200 million becquerels per hour, according to TEPCO, which was approximately one-four millionths of the level of the initial stages of the accident in March.According to TEPCO the emissions immediately after the accident were around 220 billion becquerel readings declined after that, and in November and December 2011 they dropped to 17 thousand becquerel, about one-13 millionth the initial level. But in January 2012 due to human activities at the plant, the emissions rose again up to 19 thousand becquerel. Radioactive materials around reactor 2, where the surroundings were still highly contaminated, got stirred up by the workers going in and out of the building, when they inserted an optical endoscope into the containment vessel as a first step toward decommissioning the reactor.A widely cited Austrian Meteorological Service report estimated the total amount of I-131 released into the air as of 19 March based on extrapolating data from several days of ideal observation at some of its worldwide CTBTO radionuclide measuring facilities (Freiburg, Germany Stockholm, Sweden Takasaki, Japan and Sacramento, USA) during the first 10 days of the accident. The report's estimates of total I-131 emissions based on these worldwide measuring stations ranged from 10 PBq to 700 PBq. This estimate was 1% to 40% of the 1760 PBq of the I-131 estimated to have been released at Chernobyl.A later, 12 April 2011, NISA and NSC report estimated the total air release of iodine-131 at 130 PBq and 150 PBq, respectively – about 30 grams. However, on 23 April, the NSC revised its original estimates of iodine-131 released. The NSC did not estimate the total release size based upon these updated numbers, but estimated a release of 0.14 TBq per hour (0.00014 PBq/hr) on 5 April.On 22 September the results were published of a survey conducted by the Japanese Science Ministry. This survey showed that radioactive iodine was spread northwest and south of the plant. Soil samples were taken at 2,200 locations, mostly in Fukushima Prefecture, in June and July, and with this a map was created of the radioactive contamination as of 14 June. Because of the short half-life of 8 days only 400 locations were still positive. This map showed that iodine-131 spread northwest of the plant, just like caesium-137 as indicated on an earlier map. But I-131 was also found south of the plant at relatively high levels, even higher than those of caesium-137 in coastal areas south of the plant. According to the ministry, clouds moving southwards apparently caught large amounts of iodine-131 that were emitted at the time. The survey was done to determine the risks for thyroid cancer within the population.On 31 October the Japanese ministry of Education, Culture, Sports, Science and Technology released a map showing the contamination of radioactive tellurium-129m within a 100-kilometer radius around the Fukushima No. 1 nuclear plant. The map displayed the concentrations found of tellurium-129m – a byproduct of uranium fission – in the soil at 14 June 2011. High concentrations were discovered northwest of the plant and also at 28 kilometers south near the coast, in the cities of Iwaki, Fukushima Prefecture, and Kitaibaraki, Ibaraki Prefecture. Iodine-131 was also found in the same areas, and most likely the tellurium was deposited at the same time as the iodine. The highest concentration found was 2.66 million becquerels per square meter, two kilometers from the plant in the empty town of Okuma. Tellurium-129m has a half-life of 6 days, so present levels are a very small fraction of the initial contamination. Tellurium has no biological functions, so even when drinks or food were contaminated with it, it would not accumulate in the body, like iodine in the thyroid gland.On 24 March 2011, the Austrian Meteorological Service report estimated the total amount of caesium-137 released into the air as of 19 March based on extrapolating data from several days of ideal observation at a handful of worldwide CTBTO radionuclide measuring facilities. The agency estimated an average being 5 PBq daily. Over the course of the disaster, Chernobyl put out a total of 85 PBq of caesium-137. However, later reporting on 12 April estimated total caesium releases at 6.1 PBq to 12 PBq, respectively by NISA and NSC – about 2–4 kg. On 23 April, NSC updated this number to 0.14 TBq per hour of caesium-137 on 5 April, but did not recalculate the entire release estimate.On 12 October 2011 a concentration of 195 becquerels/kilogram of Strontium-90 was found in the sediment on the roof of an apartment building in the city of Yokohama, south of Tokyo, some 250 km from the plant in Fukushima. This first find of strontium above 100 becquerels per kilogram raised serious concerns that leaked radioactivity might have spread far further than the Japanese government expected. The find was done by a private agency that conducted the test upon the request of a resident. After this find Yokohama city started an investigation of soil samples collected from areas near the building. The science ministry said that the source of the Strontium was still unclear.On 30 September 2011, the Japanese Ministry of Education and Science published the results of a plutonium fallout survey, for which in June and July 50 soil samples were collected from a radius of slightly more than 80 km around the Fukushima Daiichi plant. Plutonium was found in all samples, which is to be expected since plutonium from the nuclear weapon tests of the 1950s and '60s is found everywhere on the planet. The highest levels found (of Pu-239 and Pu-240 combined) were 15 becquerels per square meters in Fukushima prefecture and 9.4 Bq in Ibaraki prefecture, compared to a global average of 0.4 to 3.7 Bq/kg from atomic bomb tests. Earlier in June, university researchers detected smaller amounts of plutonium in soil outside the plant after they collected samples during filming by NHK.A recent study published in Nature found up to 35 bq/kg plutonium 241 in leaf litter in 3 out of 19 sites in the most contaminated zone in Fukushima. They estimated the Pu-241 dose for a person living for 50 years in the vicinity of the most contaminated site to be 0.44 mSv. However, the Cs-137 activity at the sites where Pu-241 was found was very high (up to 4.7 MBq/kg or about 135,000 times greater than the plutonium 241 activity), which suggests that it will be the Cs-137 which prevents habitation rather than the relatively small amounts of plutonium of any isotope in these areas.On 21 April, TEPCO estimated that 520 tons of radioactive water leaked into the sea before leaks in a pit in unit 2 were plugged, totaling 4.7 PBq of water release (calculated by simple sum, which is inconsistent with the IAEA methodology for mixed-nuclide releases) (20,000 times facility's annual limit). TEPCO's detailed estimates were 2.8 PBq of I-131, 0.94 PBq of Cs-134, 0.940 PBq of Cs-137.Another 300,000 tons of relatively less-radioactive water had already been reported to have leaked or been purposefully pumped into the sea to free room for storage of highly radioactively contaminated water. TEPCO had attempted to contain contaminated water in the harbor near the plant by installing ""curtains"" to prevent outflow, but now believes this effort was unsuccessful.According to a report published in October 2011 by the French Institute for Radiological Protection and Nuclear Safety, between 21 March and mid-July around 2.7 × 1016 Bq of caesium-137 (about 8.4 kg) entered the ocean, about 82 percent having flowed into the sea before 8 April. This emission of radioactivity into the sea represents the most important individual emission of artificial radioactivity into the sea ever observed. However, the Fukushima coast has some of the world's strongest currents and these transported the contaminated waters far into the Pacific Ocean, thus causing great dispersion of the radioactive elements. The results of measurements of both the seawater and the coastal sediments led to the supposition that the consequences of the accident, in terms of radioactivity, would be minor for marine life as of autumn 2011 (weak concentration of radioactivity in the water and limited accumulation in sediments). On the other hand, significant pollution of sea water along the coast near the nuclear plant might persist, because of the continuing arrival of radioactive material transported towards the sea by surface water running over contaminated soil. Further, some coastal areas might have less-favorable dilution or sedimentation characteristics than those observed so far. Finally, the possible presence of other persistent radioactive substances, such as strontium-90 or plutonium, has not been sufficiently studied. Recent measurements show persistent contamination of some marine species (mostly fish) caught along the coast of Fukushima district. Organisms that filter water and fish at the top of the food chain are, over time, the most sensitive to caesium pollution. It is thus justified to maintain surveillance of marine life that is fished in the coastal waters off Fukushima. Despite caesium isotopic concentration in the waters off of Japan being 10 to 1000 times above concentration prior to the accident, radiation risks are below what is generally considered harmful to marine animals and human consumers.A year after the disaster, in April 2012, sea fish caught near the Fukushima power plant still contain as much radioactive 134Cs and 137Cs compared to fish caught in the days after the disaster. At the end of October 2012 TEPCO admitted that it could not exclude radioactivity releases into the ocean, although the radiation levels were stabilised. Undetected leaks into the ocean from the reactors, could not be ruled out, because their basements remain flooded with cooling water, and the 2,400-foot-long steel and concrete wall between the site’s reactors and the ocean, that should reach 100 feet underground, was still under construction, and would not be finished before mid-2014. Around August 2012 two greenling were caught close to the Fukushima shore, they contained more than 25 kBq per kilogram of cesium, the highest cesium levels found in fish since the disaster and 250 times the government’s safety limit.In August 2013, a Nuclear Regulatory Authority task force reported that contaminated groundwater had breached an underground barrier, was rising toward the surface and exceeded legal limits of radioactive discharge. The underground barrier was only effective in solidifying the ground at least 1.8 meters below the surface, and water began seeping through shallow areas of earth into the sea.Radiation fluctuated widely on the site after the tsunami and often correlated to fires and explosions on site. Radiation dose rates at one location between reactor units 3 and 4 was measured at 400 mSv/h at 10:22 JST, 13 March, causing experts to urge rapid rotation of emergency crews as a method of limiting exposure to radiation. Dose rates of 1,000 mSv/h were reported (but not confirmed by the IAEA) close to the certain reactor units on 16 March, prompting a temporary evacuation of plant workers, with radiation levels subsequently dropping back to 800–600 mSv/h. At times, radiation monitoring was hampered by a belief that some radiation levels may be higher than 1 Sv/h, but that ""authorities say 1,000 millisieverts [per hour] is the upper limit of their measuring devices.""Prior to the accident, the maximum permissible dose for Japanese nuclear workers was 100 mSv per year, but on 15 March 2011, the Japanese Health and Labor Ministry increased that annual limit to 250 mSv, for emergency situations. This level is below the 500 mSv/year considered acceptable for emergency work by the World Health Organization. Some contract companies working for TEPCO have opted not to use the higher limit. On 15 March, TEPCO decided to work with a skeleton crew (in the media called the Fukushima 50) in order to minimize the number of people exposed to radiation.On 17 March, IAEA reported 17 persons to have suffered deposition of radioactive material on their face the levels of exposure were too low to warrant hospital treatment. On 22 March, World Nuclear News reported that one worker had received over 100 mSv during ""venting work"" at Unit 3. An additional 6 had received over 100 mSv, of which for 1 a level of over 150 mSv was reported for unspecified activities on site. On 24 March, three workers were exposed to high levels of radiation which caused two of them to require hospital treatment after radioactive water seeped through their protective clothes while working in unit 3. Based on the dosimeter values, exposures of 170 mSv were estimated, the injuries indicated exposure to 2000 to 6000 mSv around their ankles. They were not wearing protective boots, as their employing firm's safety manuals ""did not assume a scenario in which its employees would carry out work standing in water at a nuclear power plant"". The amount of the radioactivity of the water was about 3.9 M Bq per cubic centimetre.As of 24 March 19:30 (JST), 17 workers (of which 14 were from plant operator TEPCO) had been exposed to levels of over 100 mSv. By 29 March, the number of workers reported to have been exposed to levels of over 100 mSv had increased to 19. An American physician reported Japanese doctors have considered banking blood for future treatment of workers exposed to radiation. Tepco has started a re-assessment of the approximately 8300 workers and emergency personnel who have been involved in responding to the incident, which has revealed that by 13 July, of the approximately 6700 personnel tested so far, 88 personnel have received between 100 and 150 mSv, 14 have received between 150 and 200 mSv, 3 have received between 200 and 250 mSv, and 6 have received above 250 mSv.TEPCO has been criticized in its provision of safety equipment for its workers. After NISA warned TEPCO that workers were sharing dosimeters, since most of the devices were lost in the disaster, the utility sent more to the plant. Japanese media has reported that workers indicate that standard decontamination procedures are not being observed. Others reports suggest that contract workers are given more dangerous work than TEPCO employees. TEPCO is also seeking workers willing to risk high radiation levels for short periods of time in exchange for high pay. Confidential documents acquired by the Japanese Asahi newspaper suggest that TEPCO hid high levels of radioactive contamination from employees in the days following the incident. In particular, the Asahi reported that radiation levels of 300 mSv/h were detected at least twice on 13 March, but that ""the workers who were trying to bring the disaster under control at the plant were not informed of the levels.""Workers on-site now wear full-body radiation protection gear, including masks and helmets covering their entire heads, but it means they have another enemy: heat. As of 19 July 2011, 33 cases of heat stroke had been recorded. In these harsh working conditions, two workers in their 60s died from heart failure.On 19 July 2013 TEPCO said that 1,973 employees would have a thyroid-radiation dose exceeding 100 millisieverts. 19,592 workers—3,290 TEPCO employees and 16,302 employees of contractor firms—were given health checks. The radiation doses were checked from 522 workers. Those were reported to the World Health Organization in February 2013. From this sample, 178 had experienced a dose of 100 millisieverts or more. After the U.N. Scientific Committee on the Effects of Atomic Radiation, questioned the reliability of TEPCO´s thyroid gland dosage readings, the Japanese Health Ministry ordered TEPCO to review the internal dosage readings.The intake of radioactive iodine was calculated based on the radioactive cesium intake and other factors: the airborne iodine-to-cesium ratio on the days that the people worked at the reactor compound and other data. For one worker a reading was found of more than 1,000 millisieverts.According to the workers, TEPCO did little to inform them about the hazards of the intake of radioactive iodine. All workers with an estimated dose of 100 millisieverts were offered an annual ultrasound thyroid test during their lifetime for free. But TEPCO did not know how many of these people had received a medical screening already. A schedule for the thyroid gland test was not announced. TEPCO did not indicate what would be done if abnormalities were spotted during the tests.Within the primary containment of reactors 1, 2, 3 and 4, widely varying levels of radiation were reported:Outside the primary containment, plant radiation-level measurements have also varied significantly.On 25 March, an analysis of stagnant water in the basement floor of the turbine building of Unit 1 showed heavy contamination.On 27 March, TEPCO reported stagnant water in the basement of unit 2 (inside the reactor/turbine building complex, but outside the primary containment) was measured at 1000 mSv/h or more, which prompted evacuation. The exact dose rate remains unknown as the technicians fled the place after their first measurement went off-scale. Additional basement and trench-area measurements indicated 60 mSv/h in unit 1, ""over 1000"" mSv/h in unit 2, and 750 mSv/h in unit 3. The report indicated the main source was iodine-134 with a half-life of less than an hour, which resulted in a radioactive iodine concentration 10 million times the normal value in the reactor. TEPCO later retracted its report, stating that the measurements were inaccurate and attributed the error to comparing the isotope responsible, iodine-134, to normal levels of another isotope. Measurements were then corrected, stating that the iodine levels were 100,000 times the normal level. On 28 March, the erroneous radiation measurement caused TEPCO to reevaluate the software used in analysis.Measurements within the reactor/turbine buildings, but not in the basement and trench areas, were made on 18 April. These robotic measurements indicated up to 49 mSv/h in unit 1 and 57 mSv/h in unit 3. This is substantially lower than the basement and trench readings, but still exceeds safe working levels without constant worker rotation. Inside primary containment, levels are much higher.By 23 March 2011, neutron radiation had been observed outside the reactors 13 times at the Fukushima I site. While this could indicate ongoing fission, a recriticality event was not believed to account for these readings. Based on those readings and TEPCO reports of high levels of chlorine-38, Dr. Ferenc Dalnoki-Veress speculated that transient criticalities may have occurred. However, Edwin Lyman at the Union of Concerned Scientists was skeptical, believing the reports of chlorine-38 to be in error. TEPCO's chlorine-38 report was later retracted. Noting that limited, uncontrolled chain reactions might occur at Fukushima I, a spokesman for the International Atomic Energy Agency (IAEA) ""emphasized that the nuclear reactors won’t explode.""On 15 April, TEPCO reported that nuclear fuel had melted and fallen to the lower containment sections of three of the Fukushima I reactors, including reactor three. The melted material was not expected to breach one of the lower containers, causing a serious radioactivity release. Instead, the melted fuel was thought to have dispersed uniformly across the lower portions of the containers of reactors No. 1, No. 2 and No. 3, making the resumption of the fission process, known as a ""recriticality,"" most unlikely.On 19 April, TEPCO estimated that the unit-2 turbine basement contained 25,000 cubic meters of contaminated water. The water was measured to have 3 MBq/cm3 of Cs-137 and 13 MBq/cm3 of I-131: TEPCO characterized this level of contamination as ""extremely high."" To attempt to prevent leakage to the sea, TEPCO planned to pump the water from the basement to the Centralized Radiation Waste Treatment Facility.A suspected hole from the melting of fuel in unit 1 has allowed water to leak in an unknown path from unit 1 which has exhibited radiation measurements ""as high as 1,120 mSv/h."" Radioactivity measurements of the water in the unit-3 spent-fuel pool were reported at 140 kBq of radioactive caesium-134 per cubic centimeter, 150 kBq of caesium-137 per cubic centimeter, and 11 kBq per cubic centimeter of iodine-131 on 10 May.TEPCO have reported at three sites 500 meters from the reactors that the caesium-134 and caesium-137 levels in the soil are between 7.1 kBq and 530 kBq per kilo of undried soil.Small traces of plutonium have been found in the soil near the stricken reactors: repeated examinations of the soil suggest that the plutonium level is similar to the background level caused by atomic bomb tests. As the isotope signature of the plutonium is closer to that of power-reactor plutonium, TEPCO suggested that ""two samples out of five may be the direct result of the recent incident."" The more important thing to look at is the curium level in the soil the soil does contain a short-lived isotope (curium-242) which shows that some alpha emitters have been released in small amounts by the accident. The release of the beta/gamma emitters such as caesium-137 has been far greater. In the short and medium term the effects of the iodine and the caesium release will dominate the effect of the accident on farming and the general public. In common with almost all soils, the soil at the reactor site contains uranium, but the concentration of uranium and the isotope signature suggests that the uranium is the normal, natural uranium in the soil.Radioactive strontium-89 and strontium-90 were discovered in soil at the plant on 18 April, amounts detected in soil one-half kilometer from the facility ranging from 3.4 to 4400 Bq/kg of dry soil. Strontium remains in soil from above-ground nuclear testing however, the amounts measured at the facility are approximately 130 times greater than the amount typically associated with previous nuclear testing.The isotope signature of the release looks very different from that of the Chernobyl accident: the Japanese accident has released much less of the involatile plutonium, minor actinides and fission products than Chernobyl did.On 31 March, TEPCO reported that it had measured radioactivity in the plant-site groundwater which was 10,000 times the government limit. The company did not think that this radioactivity had spread to drinking water. NISA questioned the radioactivity measurement and TEPCO is re-evaluating it. Some debris around the plant has been found to be highly radioactive, including a concrete fragment emanating 900 mSv/h.Air outside, but near, unit 3 was reported at 70 mSv/h on 26 April 2011. This was down from radiation levels as high as 130 mSv/h near units 1 and 3 in late March. Removal of debris reduced the radiation measurements from localized highs of up to 900 mSv/h to less than 100 mSv/h at all exterior locations near the reactors however, readings of 160 mSv/h were still measured at the waste-treatment facility.Results revealed on 22 March from a sample taken by TEPCO about 100 m south of the discharge channel of units 1–4 showed elevated levels of Cs-137, caesium-134 (Cs-134) and I-131. A sample of seawater taken on 22 March 330 m south of the discharge channel (30 kilometers off the coastline) had elevated levels of I-131 and Cs-137. Also, north of the plant elevated levels of these isotopes were found on 22 March (as well as Cs-134, tellurium-129 and tellurium-129m (Te-129m)), although the levels were lower. Samples taken on 23 and/or 24 March contained about 80 Bq/mL of iodine-131 (1850 times the statutory limit) and 26 Bq/mL and caesium-137, most likely caused by atmospheric deposition. By 26 and 27 March this level had decreased to 50 Bq/mL (11) iodine-131 and 7 Bq/mL (2.9) caesium-137 (80 times the limit). Hidehiko Nishiyama, a senior NISA official, stated that radionuclide contamination would ""be very diluted by the time it gets consumed by fish and seaweed."" Above the seawater, IAEA reported ""consistently low"" dose rates of 0.04–0.1 μSv/h on 27 March.By 29 March iodine-131 levels in seawater 330 m south of a key discharge outlet had reached 138 Bq/ml (3,355 times the legal limit), and by 30 March, iodine-131 concentrations had reached 180 Bq/ml at the same location near the Fukushima Daiichi nuclear plant, 4,385 times the legal limit. The high levels could be linked to a feared overflow of highly radioactive water that appeared to have leaked from the unit -2 turbine building. On 15 April, I-131 levels were 6,500 times the legal limits. On 16 April, TEPCO began dumping zeolite, a mineral ""that absorbs radioactive substances, aiming to slow down contamination of the ocean.""Seawater radionuclide concentration on 29 March 2011:On 4 April, it was reported that the ""operators of Japan's crippled power plant say they will release more than 10,000 tons of contaminated water into the ocean to make room in their storage tanks for water that is even more radioactive."" Measurements taken on 21 April indicated 186 Bq/l measured 34 km from the Fukushima plant Japanese media reported this level of seawater contamination second to the Sellafield nuclear accident.On 11 May, TEPCO announced it believed it had sealed a leak from unit 3 to the sea TEPCO did not immediately announce the amount of radioactivity released by the leak. On 13 May, Greenpeace announced that 10 of the 22 seaweed samples it had collected near the plant showed 10,000 Bq/Kg or higher, five times the Japanese standard for food of 2 kBq/kg for iodine-131 and 500 Bq/kg for radioactive caesium.In addition to the large releases of contaminated water (520 tons and 4.7 PBq) believed to have leaked from unit 2 from mid-March until early April, another release of radioactive water is believed to have contaminated the sea from unit 3, because on 16 May TEPCO announced seawater measurements of 200 Bq per cubic centimeter of caesium-134, 220 Bq per cubic centimeter of caesium-137, and unspecified high levels of iodine shortly after discovering a unit-3 leak.At two locations 20 kilometers north and south and 3 kilometers from the coast, TEPCO found strontium-89 and strontium-90 in the seabed soil. The samples were taken on 2 June. Up to 44 becquerels per kilogram of strontium-90 were detected, which has a half-life of 29 years. These isotopes were also found in soil and in seawater immediately after the accident. Samples taken from fish and seafood caught off the coast of Ibaraki and Chiba did not contain radioactive stontium.As of October 2012, regular sampling of fish and other sea life off the coast of Fukushima showed that total cesium levels in bottom-dwelling fish were higher off Fukushima than elsewhere, with levels above regulatory limits, leading to a fishing ban for some species. Cesium levels had not decreased 1 year after the accident.Continuous monitoring of radioactivity levels in seafood by the Japanese Ministry of Agriculture, Forestry and Fisheries (MAFF) shows that for the Fukushima prefecture the proportion of catches which exceed Japanese safety standards has been decreasing continuously, falling below 2% in the second half of 2013 and below 0.5% in the fourth quarter of 2014. None of the fish caught in 2014 exceeded the less stringent pre-Fukushima standards. For the rest of Japan, the peak figure using the post-Fukushima standards was 4.7% immediately after the catastrophe, falling below 0.5% by mid-2012, and below 0.1% by mid-2013.In February 2014, NHK reported that TEPCO was reviewing its radioactivity data, after finding much higher levels of radioactivity than was reported earlier. TEPCO now says that levels of 5 MBq of strontium per liter were detected in groundwater collected in July 2013 and not 0.9 MBq, as initially reported.Periodic overall reports of the situation in Japan are provided by the United States Department of Energy.In April 2011, the United States Department of Energy published projections of the radiation risks over the next year (that is, for the future) for people living in the neighborhood of the plant. Potential exposure could exceed 20 mSv/year (2 rems/year) in some areas up to 50 kilometers from the plant. That is the level at which relocation would be considered in the USA, and it is a level that could cause roughly one extra cancer case in 500 young adults. However, natural radiation levels are higher in some parts of the world than the projected level mentioned above, and about 4 people out of 10 can be expected to develop cancer without exposure to radiation. Further, the radiation exposure resulting from the incident for most people living in Fukushima is so small compared to background radiation that it may be impossible to find statistically significant evidence of increases in cancer.The highest detection of radiation outside of Fukushima peaked at 40 mSv. This represents a much lower level then the amount required to increase a persons risk of cancer. 100 mSv represents the level at which a definitive increased risk of cancer occurs. Radiation above this level increases the risk of cancer, and after 400 mSv radiation poisoning can occur, but is unlikely to be fatal.The zone within 20 km from the plant was evacuated on 12 March, while residents within a distance of up to 30 km were advised to stay indoors. IAEA reported on 14 March that about 150 people in the vicinity of the plant ""received monitoring for radiation levels"" 23 of these people were also decontaminated. From 25 March, nearby residents were encouraged to participate in voluntary evacuation.At a distance of 30 km (19 mi) from the site, radiation of 3–170 μSv/h was measured to the north-west on 17 March, while it was 1–5 μSv/h in other directions. Experts said exposure to this amount of radiation for 6 to 7 hours would result in absorption of the maximum level considered safe for one year. On 16 March Japan's ministry of science measured radiation levels of up to 330 μSv/h 20 kilometers northwest of the power plant. At some locations around 30 km from the Fukushima plant, the dose rates rose significantly in 24 hours on 16–17 March: in one location from 80 to 170 μSv/h and in another from 26 to 95 μSv/h. The levels varied according to the direction from the plant. In most locations, the levels remained well below the levels required to damage human health, as the recommended annual maximum limit is well below the level that would affect human health.Natural exposure varies from place to place but delivers a dose equivalent in the vicinity of 2.4 mSv/year, or about 0.3 µSv/h. For comparison, one chest x-ray is about 0.2 mSv and an abdominal CT scan is supposed to be less than 10 mSv (but it has been reported that some abdominal CT scans can deliver as much as 90 mSv). People can mitigate their exposure to radiation through a variety of protection techniques.								On 22 April 2011 a Japanese government report was presented by Minister of Trade Yukio Edano to leaders of the town Futaba. In it predictions were made about radioactivity releases for the years 2012 up to 2132. According to this report, in several parts of Fukushima Prefecture – including Futaba and Okuma – the air would remain dangerously radioactive at levels above 50 millisieverts a year. This was all based on measurements done in November 2011.In August 2012, Japanese academic researchers announced that 10,000 people living near the plant in Minamisoma City at the time of the accident had been exposed to well less than 1 millisievert of radiation. The researchers stated that the health dangers from such exposure was ""negligible"". Said participating researcher Masaharu Tsubokura, ""Exposure levels were much lower than those reported in studies even several years after the Chernobyl incident.""A detailed map was published by the Ministry of Education, Culture, Sports, Science and Technology, going online on 18 October 2011. The map contains the cesium concentrations and radiation levels caused by the airborne radioactivity from the Fukushima nuclear reactor. This website contains both web-based and PDF versions of the maps, providing information by municipality as had been the case previously, but also measurements by district. The maps were intended to help the residents who had called for better information on contamination levels between areas of the same municipalities, using soil and air sample data already released. A grid is laid over a map of most of eastern Japan. Selecting a square in the grid zooms in on that area, at which point users can choose more detailed maps displaying airborne contamination levels, cesium-134 or -137 levels, or total caesium levels. Radiation mapsThe unrecovered bodies of approximately 1,000 quake and tsunami victims within the plant's evacuation zone are believed to be inaccessible at the time of 1 April 2011 due to detectable levels of radiation.Radiation levels in Tokyo on 15 March were at one point measured at 0.809 μSv/hour although they were later reported to be at ""about twice the normal level"". Later, on 15 March 2011, Edano reported that radiation levels were lower and the average radiation dose rate over the whole day was 0.109 μSv/h. The wind direction on 15 March dispersed radioactivity away from the land and back over the Pacific Ocean. On 16 March, the Japanese radiation warning system, SPEEDI, indicated high levels of radioactivity would spread further than 30 km from the plant, but Japanese authorities did not relay the information to citizens because ""the location or the amount of radioactive leakage was not specified at the time."" From 17 March, IAEA received regular updates on radiation from 46 cities and indicated that they had remained stable and were ""well below levels which are dangerous to human health"". In hourly measurements of these cities until 20 March, no significant changes were reported.On 18 June 2012 it became known that from 17 to 19 March 2011 in the days directly after the explosions, American military aircraft gathered radiation data in an area with a radius of 45 kilometers around the plant for the U.S. Department of Energy. The maps revealed radiation levels of more than 125 microsieverts per hour at 25 kilometers northwest of the plant, which means that people in these areas were exposed to the annual permissible dose within eight hours. The maps were neither made public nor used for evacuation of residents.On 18 March 2011 the U.S. government sent the data through the Japanese Foreign Ministry to the NISA under the Ministry of Economy, Trade and Industry, and the Japanese Ministry of Education, Culture, Sports, Science and Technology got the data on 20 March.The data were not forwarded to the prime minister's office and the Nuclear Safety Commission, and subsequently not used to direct the evacuation of the people living around the plant. Because a substantial portion of radioactive materials released from the plant went northwest and fell onto the ground, and some residents were ""evacuated"" in this direction, these people could have avoided unnecessary exposure to radiation had the data been published directly. According to Tetsuya Yamamoto, chief nuclear safety officer of the Nuclear Safety Agency, ""It was very regrettable that we didn't share and utilize the information."" But an official of the Science and Technology Policy Bureau of the technology ministry, Itaru Watanabe, said it was more appropriate for the United States, rather than Japan, to release the data. On 23 March - after the Americans - Japan released its own fallout maps, compiled by Japanese authorities from measurements and predictions from the computer simulations of SPEEDI. On 19 June 2012 Minister of Science Hirofumi Hirano said that Japan would review the decision of the Science Ministry and the Nuclear-Safety Agency in 2011 to ignore the radiation maps provided by the United States. He defended his ministry's handling of the matter with the remark that its task was to measure radiation levels on land. But the government should reconsider its decision not to publish the maps or use the information. Studies would be done by the authorities, whether the maps could have been a help with the evacuations.On 30 March 2011, the IAEA stated that its operational criteria for evacuation were exceeded in the village of Iitate, Fukushima, 39 kilometres (24 miles) north-west of Fukushima I, outside the existing 30 kilometres (19 miles) radiation exclusion zone. The IAEA advised the Japanese authorities to carefully assess the situation there. Experts from Kyoto University and Hiroshima University released a study of soil samples, on 11 April, that revealed that ""as much as 400 times the normal levels of radiation could remain in communities beyond a 30-kilometer radius from the Fukushima"" site.Urine samples taken from 10 children in the capital of Fukushima Prefecture were analyzed in a French laboratory. All of them contained caesium-134. The sample of an eight-year-old girl contained 1.13 becquerels/liter. The children were living up to 60 kilometers away from the troubled nuclear power plant. The Fukushima Network for Saving Children urged the Japanese government to check the children in Fukushima. The Japanese non-profit Radiation Effects Research Foundation said that people should not overreact, because there are no reports known of health problems with these levels of radiation.On 31 October 2011 a scientist from the Worcester Polytechnic Institute, Marco Kaltofen, presented his findings on the releases of radioactive isotopes from the Fukushima accidents at the annual meeting of the American Public Health Association (APHA). Airborne dust contaminated with radioactive particles was released from the reactors into the air. This dust was found in Japanese car filters: they contained cesium-134 and cesium-137, and cobalt at levels as high as 3 nCi total activity per sample. Materials collected during April 2011 from Japan also contained iodine-131. Soil and settled dust were collected from outdoors and inside homes, and also from used children's shoes. High levels of cesium were found on the shoelaces. US air-filter and dust samples did not contain ""hot"" particles, except for air samples collected in Seattle, Washington in April 2011. Dust particles contaminated with radioactive cesium were found more than 100 miles from the Fukushima site, and could be detected on the U.S. West Coast.Tests concluded between 10 and 20 April revealed radioactive caesium in amounts of 2.0 and 3.2 kBq/kg in soil from the Tokyo districts of Chiyoda and Koto, respectively. On 5 May, government officials announced that radioactivity levels in Tokyo sewage had spiked in late March. Simple-sum measurements of all radioactive isotopes in sewage burned at a Tokyo treatment plant measured 170,000 Bq/kg ""in the immediate wake of the Fukushima nuclear crisis"". The government announced that the reason for the spike was unclear, but suspected rainwater. The 5 May announcement further clarified that as of 28 April, the radioactivity level in Tokyo sewage was 16,000 Bq/kg.A detailed map of ground contamination within 80 kilometers of the plant, the joint product of the U.S. Department of Energy and the Japanese Ministry of Education, Culture, Sports, Science and Technology (MEXT), was released on 6 May. The map showed that a belt of contamination, with radioactivity from 3 to 14.7 MBq caesium-137 per square meter, spread to the northwest of the nuclear plant. For comparison, areas with activity levels with more than 0.55 MBq caesium-137 per square meter were abandoned after the 1986 Chernobyl accident. The village of Iitate and the town of Namie are impacted. Similar data was used to establish a map that would calculate the amount of radiation a person would be exposed to if a person were to stay outdoors for eight hours per day through 11 March 2012. Scientists preparing this map, as well as earlier maps, targeted a 20 mSv/a dosage target for evacuation. The government's 20 mSv/a target led to the resignation of Toshiso Kosako, Special Adviser on radiation safety issues to Japanese Prime Minister Naoto Kan, who stated ""I cannot allow this as a scholar"", and argued that the target is too high, especially for children he also criticized the increased limit for plant workers. In response, parents' groups and schools in some smaller towns and cities in Fukushima Prefecture have organized decontamination of soil surrounding schools, defying orders from Tokyo asserting that the schools are safe. Eventually, the Fukushima education board plans to replace the soil at 26 schools with the highest radiation levels.Anomalous ""hot spots"" have been discovered in areas far beyond the adjacent region. For example, experts cannot explain how radioactive caesium from the reactors at Fukushima ended up in Kanagawa more than 300 kilometers (190 mi) to the south.In the first week of September the Ministry of Science published a new map showing radiation levels in Fukushima and four surrounding prefectures, based on the results of an aerial survey. In the map, different colors were used to show the level of radiation at locations one meter above the ground.Red: 19 microsieverts per hour or higher. The red band pointed in a north-west direction and was more than 30 kilometers long.Yellow: radiation between 3.8 and 19 microsieverts per hour. This corresponds to less than a chest X-ray to 3 chest X-rays. This is the threshold to designate an area an evacuation zone. The yellow area extended far beyond the evacuation zone already put into place.Light green: radiation between 0.5 and one microsieverts per hour. This was still far above the annual level of one hundred millisievert, which should cause no harm to people. This zone contained most of Fukushima Prefecture, southern parts of Miyagi Prefecture, and northern parts of Tochigi and Ibaraki prefectures.Up to 307,000 becquerels of cesium per kilogram of soil were detected during a survey held in Fukushima City, 60 kilometers away from the crippled reactors, on 14 September 2011. This was triple the amount for contaminated soil that by Japanese governmental orders should be sealed into concrete. According to ""Citizens Against Fukushima Aging Nuclear Power Plants"", these readings were comparable to the high levels in special regulated zones where evacuation was required after the Chernobyl accident. They urged the government to designate the area as a hot spot, where residents would need to voluntarily evacuate and be eligible for state assistance. Professor Tomoya Yamauchi of the University of Kobe, in charge of the study, in which soil samples were tested from five locations around the district, noted that the decontamination conducted in some of the areas tested has not yet reduced the radiation to pre-accident levels.On 18 October 2011 a hot-spot in a public square was found in the city of Kashiwa, Chiba in the Nedokoyadai district, by a resident walking with a dosimeter. He informed the city council. Their first readings were off the scale, as their Geiger-counter could measure up to 10 microsieverts per hour. Later measurements by the Chiba environment foundation reported a final result of 57.5 microsieverts per hour. On 21 October the roads around the place were sealed off, and the place was covered with sandbags three meters thick. Further investigations and check-ups were planned on 24 October 2011. These investigations showed on 23 October levels up to 276,000 becquerels radioactive cesium per kilogram of soil, 30 centimeters below the surface. The first comments of town officials on the find of 57.7 microsieverts per hour were that there could not be a link with the Fukushima disaster, but after the find of this large amount of cesium, officials of the Science Ministry could not deny the possibility that the cause could be found at the Fukushima-site.In October 2011, radiation levels as high as those in the evacuation zone around Japan's Fukushima nuclear plant were detected in a Tokyo suburb. Japanese officials said the contamination was linked to the Fukushima nuclear disaster. Contamination levels ""as high as those inside Fukushima's no-go zone have been detected, with officials speculating that the hotspot was created after radioactive caesium carried in rain water became concentrated because of a broken gutter"".In October 2011 the Japanese Ministry of Science launched a phone hotline to deal with concerns about radiation exposure outside Fukushima Prefecture. Concerned Japanese citizens had been walking with Geiger-counters through their locality in search of all places with raised radiation levels. Whenever a site was found with a radiation dose at one meter above the ground more than one microsievert per hour and higher than nearby areas, this should be mentioned at the hotline. One microsievert per hour is the limit above this topsoil at school playgrounds would be removed, subsidized by the state of Japan. Local governments were asked to carry out simple decontamination works, such as clearing mud from ditches if necessary. When radiation levels would remain more than one microsievert higher than nearby areas even after the cleaning, the ministry offered to help with further decontamination. On the website of the ministry a guideline was posted on how to measure radiation levels in a proper way, how to hold the dosimeter and how long to wait for a proper reading.In October 2011 hotspots were reported on the grounds of two elementary schools in Abiko in Chiba:11.3 microsieverts per hour was detected on 25 September just above the surface of the ground near a ditch in the compounds of the Abiko Municipal Daiichi Elementary School. At 50 centimeters above the ground the reading was 1.7 microsieverts per hour. The soil in the ditch contained 60,768 becquerels per kilogram. After the soil was removed, the radiation decreased to 0.6 microsieverts per hour at 50 centimeters above groundlevel.10.1 microsieverts per hour was found at the Abiko Municipal Namiki Elementary School near the surface of the ground where sludge removed from the swimming pool of the school had been buried. The area was covered with a waterproof tarp and dirt was put on top of the tarp to decrease the radiation 0.6 microsieverts per hour was measured 50 centimeters above the ground after this was done.Radioactive cesium was found in waste water discharged into Tokyo Bay from a cement factory in the prefecture Chiba east of Tokio. In September and October two water samples were taken, measuring 1,103 becquerels per liter and 1,054 becquerels per liter respectively. These were 14 to 15 times higher than the limit set by NISA. Ash from incinerators in the prefecture constituted the raw material to produce cement. In this process toxic substances are filtered out of the ashes, and the water used to clean these filters was discharged into Tokyo Bay. On 2 November 2011 this waste-water discharge was halted, and the Japanese authorities started a survey on the cesium contamination of the seawater of Tokyo Bay near the plant.On 12 November the Japanese government published a contamination map compiled by helicopter. This map covered a much wider area than before. Six new prefectures Iwate, Yamanashi, Nagano, Shizuoka, Gifu, and Toyama were included in this new map of the soil radioactivity of cesium-134 and cesium-137 in Japan. Contamination between 30,000 and 100,000 becquerels per square meter was found in Ichinoseki and Oshu (prefecture Iwate), in Saku, Karuizawa and Sakuho (prefecture Nagano, in Tabayama (prefecture Yamanashi) and elsewhere.Based on radiation measurements made all over Japan between 20 March and 20 April 2011, and the atmospheric patterns in that period, computer simulations were performed by an international team of researchers, in cooperation with the University of Nagoya, in order to estimate the spread of radioactive materials like cesium-137. Their results, published in two studies on 14 November 2011, suggested that cesium-137 reached up to the northernmost island of Hokkaido, and the regions of Chugoku and Shikoku in western Japan at more than 500 kilometers from the Fukushima plant. Rain accumulated the cesium in the soil. Measured radioactivity per kilogram reached 250 becquerels in eastern Hokkaido, and 25 becquerels in the mountains of western Japan. According to the research group, these levels were not high enough to require decontamination. Professor Tetsuzo Yasunari of the University of Nagoya called for a national soil-testing program because of the nationwide spread of radioactive material, and suggested identified hotspots, places with high radiation levels, should be marked with warning signs.The first study concentrated on cesium-137. Around the nuclear plant, places were found containing up to 40.000 becquerels/kg, 8 times the governmental safety limit of 5.000 becquerels/kg. Places further away were just below this maximum. East and north-east from the plant the soil was contaminated the most. North-west and westwards the soil was less contaminated, because of mountain protection.The second study had a wider scope, and was meant to study the geographic spread of more-radioactive isotopes, like tellurium and iodine. Because these isotopes deposit themselves in the soil with rain, Norikazu Kinoshita and his colleagues observed the effect of two specific rain-showers on 15 and 21 March 2011. The rainfall on 15 March contaminated the grounds around the plant the second shower transported the radioactivity much further from the plant, in the direction of Tokyo. According to the authors, the soil should be decontaminated, but when this is found impossible, farming should be limited.On 13 December 2011 extremely high readings of radioactive cesium – 90,600 becquerels per kilogram, 11 times the governmental limit of 8000 becquerels – were detected in a groundsheet at the Suginami Ward elementary school in Tokyo at a distance of 230 kilometers from Fukushima. The sheet was used to protect the school lawn against frost from 18 March until 6 April 2011. Until November this sheet was stored alongside a gymnasium. In places near this storage area up to 3.95 microsieverts per hour were measured one centimeter above the ground. The school planned to burn the sheet. Further inspections were requested.All citizens of the town Fukushima received dosimeters to measure the precise dose of radiation to which they were exposed. After September the city of Fukushima collected the 36,478 ""glass badges"" of dosimeters from all its citizens for analysis. It turned out that 99 percent had not been exposed to more than 0.3 millisieverts in September 2011, except four young children from one family: a girl, in third year elementary school, had received 1.7 millisieverts, and her three brothers had been exposed to 1.4 to 1.6 millisieverts. Their home was situated near a highly radioactive spot, and after this find the family moved out of Fukushima Prefecture. A city official said that this kind of exposure would not affect their health.Similar results were obtained for a three-month period from September 2011: among a group of 36,767 residents in Fukushima city, 36,657 had been exposed to less than 1 millisievert, and the average dose was 0.26 millisieverts. For 10 residents, the readings ranged from 1.8 to 2.7 millisieverts, but these values are mostly believed to be related to usage errors (dosimeters left outside or exposed to X-ray luggage screening).Due to objections from concerned residents it became more and more difficult to dispose of the ashes of burned household garbage in and around Tokyo. The ashes of waste facilities in the Tohoku, Kanto and Kōshin'etsu regions were proven to be contaminated with radioactive cesium. According to the guidelines of the Ministry of Environment, ashes radiating 8,000 becquerels per kilogram or lower could be buried. Ashes with cesium levels between 8,000 and 100,000 becquerels should be secured, and buried in concrete vessels. A survey was done on 410 sites of waste-disposal facilities, on how the ash disposal was proceeding. At 22 sites, mainly in the Tokyo Metropolitan area, the ashes with levels under 8000 becquerels could not be buried due to the objections of concerned residents. At 42 sites, ashes were found that contained over 8,000 becquerels of cesium, which could not be buried. The ministry made plans to send officials to meetings in the municipalities to explain to the Japanese people that the waste disposal was done safely, and to demonstrate how the disposal of the ashes above 8000 becquerels was conducted.On 5 January 2012 the Nambu (south) Clean Center, a waste incinerator in Kashiwa, Chiba, was taken out of production by the city council because the storage room was completely filled with 200 metric tons of radioactive ash that could not disposed of in landfills. Storage at the plant was full, with 1049 drums, and some 30 tons more were still to be taken out of the incinerator. In September 2011, the factory was closed for two months for the same reason. The Center's special advanced procedures were able to minimize the volume of the ash, but radioactive cesium was concentrated to levels above the national limit of 8.000 becquerels per kilogram for waste disposal in landfills. It was not possible to secure new storage space for the radioactive ash. Radiation levels in Kashiwa were higher than in surrounding areas, and ashes containing up to 70,800 becquerels of radioactive cesium per kilogram – higher than the national limit – were detected in the city. Other cities around Kashiwa were facing the same problem: radioactive ash was piling up. Chiba prefecture asked Abiko and Inzai to accept temporary storage at the Teganuma waste-disposal facility located at their border. But this met strong opposition from their citizens.Radiation monitoring in all 47 prefectures showed wide variation, but an upward trend in 10 of them on 23 March. No deposition could be determined in 28 of them until 25 March The highest value obtained was in Ibaraki (480 Bq/m2 on 25 March) and Yamagata (750 Bq/m2 on 26 March) for iodine-13. For cesium-137, the highest values were in Yamagata at 150 and 1200 Bq/m2 respectively.Measurements made in Japan in a number of locations have shown the presence of radionuclides in the ground. On 19 March, upland soil levels of 8,100 Bq/kg of Cs-137 and 300,000 Bq/kg of I-131 were reported. One day later, the measured levels were 163,000 Bq/kg of Cs-137 and 1,170,000 Bq/kg of I-131.On 19 March, the Japanese Ministry of Health, Labour and Welfare announced that levels of radioactivity exceeding legal limits had been detected in milk produced in the Fukushima area and in certain vegetables in Ibaraki. On 21 March, IAEA confirmed that ""in some areas, iodine-131 in milk and in freshly grown leafy vegetables, such as spinach and spring onions, is significantly above the levels set by Japan for restricting consumption"". One day later, iodine-131 (sometimes above safe levels) and caesium-137 (always at safe levels) detection was reported in Ibaraki prefecture. On 21 March, levels of radioactivity in spinach grown in the open air in Kitaibaraki city in Ibaraki, around 75 kilometers south of the nuclear plant, were 24,000 becquerel (Bq)/kg of iodine-131, 12 times more than the limit of 2,000 Bq/kg, and 690 Bq/kg of caesium, 190 Bq/kg above the limit. In four Prefectures (Ibaraki, Totigi, Gunma, Fukushima), distribution of spinach and kakina was restricted as well as milk from Fukushima. On 23 March, similar restrictions were placed on more leafy vegetables (komatsuna, cabbages) and all flowerheads brassicas (like cauliflower) in Fukushima, while parsley and milk distribution was restricted in Ibaraki. On 24 March, IAEA reported that virtually all milk samples and vegetable samples taken in Fukushima and Ibaraki on 18–21 and 16–22 March respectively were above the limit. Samples from Chiba, Ibaraki and Tochigi also had excessive levels in celery, parsley, spinach and other leafy vegetables. In addition, certain samples of beef mainly taken on 27–show of 29 Marched concentrations of iodine-131 and/or caesium-134 and caesium-137 above the regulatory levels.After the detection of radioactive cesium above legal limits in Sand lances caught off the coast of Ibaraki Prefecture, the government of the prefecture banned such fishing. On 11 May, cesium levels in tea leaves from a prefecture ""just south of Tokyo"" were reported to exceed government limits: this was the first agricultural product from Kanagawa Prefecture that exceeded safety limits. In addition to Kanagawa Prefecture, agricultural products from Tochigi and Ibaraki prefectures have also been found to exceed the government limits, for example, pasture grass collected on 5 May, measured 3,480 Bq/kg of radioactive caesium, approximately 11 times the state limit of 300 becquerels. Even into July radioactive beef was found on sale in eleven prefectures, as far away as Kōchi and Hokkaido. Authorities explained that until that point testing had been performed on the skin and exterior of livestock. Animal feed and meat cuts had not been checked for radioactivity previously.Hay and straw were found contaminated with cesium 80 kilometres (50 mi) from the reactors and outside the evacuation zone. The news of the contamination of foods with radioactive substances leaking from the Fukushima nuclear reactors damaged the mutual trust between local food producers, including farmers, and consumers. The source of cesium was found to be rice straw that had been fed to the animals. A notice from the Japanese government that was sent to cattle farmers after the nuclear accident made no mention of the possibility that rice straw could be contaminated with radioactive materials from the fallout. Beef from Fukushima Prefecture was removed from the distribution channels. Health minister Kohei Otsuka stated on 17 July 2011 that this removal might not be sufficient. The urine of all cattle for sale was tested in order to return those cows that showed levels of radioactive substances higher than the government-set limit to farms so they could be decontaminated by feeding them safe hay. The minister said that the government should try to buy uncontaminated straw and hay in other parts of the country and offer this to the farmers in the affected areas. All transport of beef raised in the prefecture Fukushima was  prohibited after 19 July. The meat of some 132 cows was sold to at least 36 of the 47 prefectures of Japan. In more and more places contaminated meat was found.In March 2012 up to 18,700 becquerels per kilogram radioactive cesium was detected in yamame, or landlocked masu salmon, caught in the Niida river near the town Iitate, which was over 37 times the legal limit of 500 becquerels/kg. The fish was caught for testing purposes prior to the opening of the fishing season. Fishing cooperatives were asked to refrain from catching and eating yamame fish from this river and all streams adjacent to it. No fish was sold in local markets.No fishing was allowed in the river Nojiri in the region Okuaizu in Fukushima after-mid March 2012. The fish caught in this river contained 119 to 139 becquerels of radioactive cesium per kilogram, although this river is located some 130 kilometers from the damaged reactors. In 2011 at this place the fish measured about 50 becquerels per kilogram, and the fishing season was opened as usual. But fishing was not popular in 2011. Local people hoped it would be better in 2012. After the new findings the fishing season was prosponed.On 28 March 2012 smelt caught in the Akagi Onuma lake near the city of Maebashi in the prefecture Gunma was found to be contaminated with 426 becquerels per kilogram of cesium.In April 2012 radioactive cesium concentrations of 110 becquerels per kilogram were found in silver crucian carp fish caught in the Tone River north of Tokyo, some 180 kilometers away from the Fukushima Daiichi Plant. Six fishery cooperatives and 10 towns along the river were asked to stop all shipments of fish caught in the river. In March 2012 fish and shellfish caught in a pond near the same river were found to contain levels above the new legal limits of 100 becquerels per kilogram.The Dutch bio-farming company Waterland International and a Japanese federation of farmers made an agreement in March 2012 to plant and grow camellia on 2000 to 3000 hectare. The seeds will be used to produce bio-diesel, which could be used to produce electricity. According to director William Nolten the region had a big potential for the production of clean energy. Some 800,000 hectares in the region could not be used to produce food anymore, and after the disaster because of fears for contamination the Japanese people refused to buy food produced in the region anyway. Experiments would be done to find out whether camelia was capable of extracting caesium from the soil. An experiment with sunflowers had no success.High levels of radioactive cesium were found in 23 varieties of freshwater fish sampled at five rivers and lakes in Fukushima Prefecture between December 2011 and February 2012 and in 8 locations on the open sea. On 2 July 2012 the Ministry of the Environment published that it had found radioactive cesium between 61 and 2,600 becquerels per kilogram. 2,600 becquerels were found in a kind of goby caught in Mano River, which flows from Iitate Village to the city of Minamisoma, north of the nuclear plant. Water bugs, common food for freshwater fish, also showed high levels of 330 to 670 becquerels per kilogram. Marine fish was found less contaminated and showed levels between 2.15 and 260 Bq/kg. Marine fish might be more capable of excreting cesium from their bodies, because saltwater fish have the ability to excrete salt. The Japanese Ministry of the Environment would closely monitor freshwater fish as radioactive cesium might remain for much longer periods in their bodies. According to Japanese regulations, food is considered safe for consumption up to a maximum of 100 Bq/kg.In August 2012, the Health ministry found that cesium levels had dropped to undetectable levels in most cultivated vegetables from the affected area, while food sourced from forests, rivers or lakes in the Tohoku and northern Kanto regions are showing excessive contamination.In a 'murasoi'-fish (or rock-fish Sebastes pachycephalus) caught in January 2013 at the coast of Fukushima an enormous amount of radioactive cesium was found: 254.000 becquerel/kilogram, or 2540 times the legal limitm in Japan for seafood.On 21 February 2013 a greenling - 38 centimeters long and weighing 564 grams - was caught near a water intake of the reactor units. It did set a new record: containing 740,000 becquerels radioactive cesium per kilogram, 7,400 times the Japanese limit deemed safe for human consumption. The previous record of cesium concentration in fish was 510,000 Bq/kg detected in another greenling. On the sea floor a net was installed by TEPCO, in order to prevent migrating fish to escape from the contaminated area.As of July 2011, the Japanese government has been unable to control the spread of radioactive material into the nation's food, and ""Japanese agricultural officials say meat from more than 500 cattle that were likely to have been contaminated with radioactive caesium has made its way to supermarkets and restaurants across Japan"". On 22 July it became known that at least 1400 cows were shipped from 76 farms that were fed with contaminated hay and rice-straw that had been distributed by agents in Miyagi and farmers in the prefectures of Fukushima and Iwate, near the crippled Fukushima Daiichi nuclear power plant. Supermarkets and other stores were asking their customers to return the meat. Farmers were asking for help, and the Japanese government was considering whether it should buy and burn all this suspect meat. Beef had 2% more Caesium then the governments strict limits.On 26 July more than 2,800 cattle carcasses, fed with cesium-contaminated food, had been shipped for public consumption to 46 of the 47 prefectures in Japan, with only Okinawa remaining free. Part of this beef, which had reached the markets, still needed to be tested. In an attempt to ease consumer concern the Japanese government promised to impose inspections on all this beef, and to buy the meat back when higher-than-permissible cesium levels were detected during the tests. The government planned to eventually pass on the buy-back costs to TEPCO. The same day the Japanese ministry of agriculture urged farmers and merchants to renounce the use and sale of compost made of manure from cows that may have been fed the contaminated straw. The measure also applied to humus from leaves fallen from trees. After developing guidelines for safety levels of radioactive caesium in compost and humus, this voluntary ban could be lifted.On 28 July a ban was imposed on all the shipments on cattle from the prefecture Miyagi. Some 1,031 beasts had been shipped that probably were fed with contaminated rice-straw. Measurements of 6 of them revealed 1,150 becquerels per kilogram, more than twice the governmental set safety level. Because the origins were scattered all over the prefecture, Miyagi became the second prefecture with a ban on all beef-cattle shipments. In the year before 11 March about 33,000 cattle were traded from Miyagi.On 1 August a ban was put on all cattle in the prefecture Iwate, after 6 cows from two villages were found with heavy levels of caesium. Iwate was the third prefecture where this was decided. Shipments of cattle and meat would only be allowed after examination, and when the level of caesium was below the regulatory standard. In Iwate some 36,000 cattle were produced in a year. All cattle would be checked for radioactive contamination before shipment, and the Japanese government asked the prefecture to temporarily reduce the number of shipments to match its inspection capability.On 3 August, the prefecture Shimane, in western Japan, conducted radiation checks on all beef cattle to ease consumer concerns about food safety. Starting from the second week of August all cattle were tested. Late July at one farm in this prefecture rice-straw was discovered with radioactive caesium levels exceeding the government safety guide. Although all other tests of beef cattle found far lower levels of radioactivity than the government standard, prices of beef from Shimane plummeted and wholesalers avoided all cattle from the prefecture. All processed beef would undergo preliminary screening, and meat registering 250 becquerels per kilogram or more of radioactive caesium – half the government safety level – would be tested further.The second week of August the prefecture of Fukushima Prefecture initiated a buy-out of all cattle that could not be sold because the high levels of caesium in the meat. The prefecture decided to buy back all beef cattle that had become too old for shipment due to the shipping suspension in place since July.On 2 August a group of farmers agreed with the Fukushima prefectural government to set up a consultative body to regulate this process. The prefectural government provided the subsidies needed. There was some delay, because the farmers and the local government could not agree about the prices.The problems for the farmers were growing, because they did not know how to protect their cattle from contamination and did not know how to feed their cattle. The farmers said that the buy-back plan needed to be implemented immediately.On 5 August 2011, in response to calls for more support by farmers, the Japanese government revealed a plan to buy up all beef contaminated with radioactive caesium, that had already reached the distribution chains, as an additional measurement to support beef cattle farmers.The plan included:the buy-out of about 3,500 head of cattle suspected to have been fed with contaminated rice straw, with caesium in excess of the safety limit.regardless the fact that some beef could be within the national safety limits.all this meat would be burned, to keep it out of distribution-channelsOther measurements were the expansion of subsidies to beef cattle farmers:Farmers who were unable to ship their cattle due to restrictions received 50,000 yen, (~ 630 dollars) per head of cattle regardless of the cattle's age.financial support was offered to prefectures that were buying up beef cattle, that had become too old to ship due to the ban.The Japanese Government planned to go on to buy all beef containing unsafe levels of radioactive caesium that reached the market through private organizations.On 19 August 2011 was reported, the meat of 4 cows from one Fukushima farm had been found to be contaminated with radioactive caesium in excess of the government-set safety limits. The day after the meat of 5 other cows from this farm was also found to contain radioactive caesium. Because of this the central government delayed lifting a shipment ban on Fukushima beef. The 9 cows were among a total of over 200 head of cattle shipped from the farm and slaughtered at a facility in Yokohama city between 11 March nuclear accident and April. The beef had been stored by a food producer. The farmer denied feeding the cows contaminated rice straw, instead he used imported hay that had been stored at another farm.Japan banned Fukushima beef. These domestic animals were affected by the food supply. It was reported that 136 cows consumed feed affected by radioactive caesium. A number of cows were found to have consumed rice straw containing high levels of radioactive caesium. This meat had already been distributed nationwide and that it ""could have already reached consumers."" They traced contaminated beef on farms near the Fukushima power plant, and on farms 100 km (70 miles) away. ""The government has also acknowledged that the problem could be wider than just Fukushima."" By August 2012, sampling of beef from affected areas revealed that 3 out of 58,460 beef samples contained radioactivity above regulatory limits. Much of the radioactivity is believed to have come from contaminated feed. Radioactivity infiltration into the beef supply has subsided with time, and is projected to continue decreasing.In August 2011, a group of 5 manufacturers of natto, or fermented soybeans, in Mito, Ibaraki planned to seek damages from TEPCO because their sales had fallen by almost 50 percent. Natto is normally packed in rice-straw and after the discovery of caesium contamination, they had lost many customers. The lost sales from April–August 2011 had risen to around 1.3 million dollars.On 3 September 2011 radioactive caesium exceeding the government's safety limit had been detected in tea leaves in Chiba and Saitama prefectures, near Tokyo. This was the ministry's first discovery of radioactive substances beyond legal limits since the tests of food stuffs started in August. These tests were conducted in order to verify local government data using different numbers and kinds of food samples. Tea leaves of one type of tea from Chiba Prefecture contained 2,720 becquerels of radioactive caesium per kilogram, 5 times above the legal safety limit. A maximum of 1,530 becquerels per kilogram was detected in 3 kinds of tea leaves from Saitama Prefecture. Investigations were done to find out where the tea was grown, and to determine how much tea had already made its way to market. Tea producers were asked to recall their products, when necessary. As tea leaves are never directly consumed, tea produced from processed leaves are expected to contain no more than 1/35th the density of caesium (in the case of 2720bq/kg, the tea will show just 77bq/l, below the 200bq/l legal limit at the time)In the prefecture Shizuoka at the beginning of April 2012, tests done on tea-leaves grown inside a greenhouse were found to contain less than 10 becquerels per kilogram, below the new limit of 100 becquerels, The tests were done in a governmental laboratory in Kikugawa city, to probe cesium-concentrations before the at the end of April the tea-harvest season would start.The health ministry published in August 2012, that cesium levels in tea made from ""yacon"" leaves and in samples of Japanese tea ""shot through the ceiling"" this year.On 19 August radioactive caesium was found in a sample of rice. This was in Ibaraki Prefecture, just north of Tokyo, in a sample of rice from the city of Hokota, about 100 miles south of the nuclear plant. The prefecture said the radioactivity was well within safe levels: it measured 52 becquerels per kilogram, about one-tenth of the government-set limit for grains. Two other samples tested at the same time showed no contamination. The Agriculture Ministry said it was the first time that more than trace levels of cesium had been found in rice.On 16 September 2011 the results were published of the measurements of radioactive cesium in rice. The results were known of around 60 percent of all test-locations. Radioactive materials were detected in 94 locations, or 4.3 percent of the total. But the highest level detected so far, in Fukushima prefecture, was 136 becquerels per kilogram, about a quarter of the government's safety limit of 500 Becquerel per kilogram. Tests were conducted in 17 prefectures, and were completed in more than half of them. In 22 locations radioactive materials were detected in harvested rice. The highest level measured was 101.6 becquerels per kilogram, or one fifth of the safety limit. Shipments of rice did start in 15 prefectures, including all 52 municipalities in the prefecture Chiba. In Fukushima shipments of ordinary rice did start in 2 municipalities, and those of early-harvested rice in 20 municipalities.On 23 September 2011 radioactive caesium in concentrations above the governmental safety limit was found in rice samples collected in an area in the northeastern part of the prefecture Fukushima. Rice-samples taken before the harvest showed 500 becquerels per kilogram in the city of Nihonmatsu. The Japanese government ordered a two way testing procedure of samples taken before and after the harvest. Pre-harvest tests were carried out in nine prefectures in the regions of Tohoku and Kanto. After the find of this high level of cesium, the prefectural government dis increase the number of places to be tested within the city from 38 to about 300. The city of Nihonmatsu held an emergency meeting on 24 September with officials from the prefecture government. The farmers, that already had started harvesting, were ordered to store their crop until the post-harvest tests were available.On 16 November 630 becquerels per kilogram of radioactive cesium was detected in rice harvested in the Oonami district in Fukushima City.All rice of the fields nearby was stored and none of this rice had been sold to the market. On 18 November all 154 farmers in the district were asked to suspend all shipments of rice. Tests were ordered on rice samples from all 154 farms in the district. The result of this testing was reported on 25 November: five more farms were found with cesium contaminated rice at a distance of 56 kilometers from the disaster reactors in the Oonami district of Fukushima City, The highest level of cesium detected was 1,270 becquerels per kilogram.On 28 November 2011 the prefecture of Fukushima reported the find of cesium-contaminated rice, up to 1050 Becquerels per kilogram, in samples of 3 farms in the city Date at a distance of 50 kilometers from the Fukushima Daiichi reactors. Some 9 kilo's of this crops were already sold locally before this date. Officials tried to find out who bought this rice. Because of this and earlier finds the government of the prefecture Fukushima decided to control more than 2300 farms in the whole district on cesium-contamination. A more precise number was mentioned by the Japanese newspaper The Mainichi Daily News: on 29 November orders were given to 2381 farms in Nihonmatsu and Motomiya to suspend part of their rice shipments. This number added to the already halted shipments at 1941 farms in 4 other districts including Date, raised the total to 4322 farms.Rice exports from Japan to China became possible again after a bilateral governmental agreement in April 2012. With government-issued certificates of origin Japanese rice produced outside the prefectures Chiba, Fukushima prefecture, Gunma, Ibaraki, Niigata, Nagano, Miyagi, Saitama, Tokyo, Tochigi and Saitama was allowed to be exported. In the first shipment 140.000 tons of Hokkaido rice of the 2011 harvest was sold to China National Cereals, Oils and Foodstuffs Corporation.On 7 February 2012 noodles contaminated with radioactive cesium (258 becquerels of cesium per kilogram) were found in a restaurant in Okinawa. The noodles, called ""Okinawa soba"", were apparently produced with water filtered through contaminated ashes from wood originating from the prefecture Fukushima. On 10 February 2012 the Japanese Agency for Forestry set out a warning not to use ashes from wood or charcoal, even when the wood itself contained less than the governmental set maximum of 40 becquerels per kilo for wood or 280 becquerels for charcoal. When the standards were set, nobody thought about the use of the ashes to be used for the production of foods. But, in Japan it was a custom to use ashes when kneading noodles or to take away a bitter taste, or ""aku"" from ""devil's tongue"" and wild vegetables.On 13 October 2011 the city of Yokohama terminated the use of dried shiitake-mushrooms in school lunches after tests had found radioactive cesium in them up to 350 becquerels per kilogram. In shiitake mushrooms grown outdoors on wood in a city in the prefecture Ibaraki, 170 kilometers from the nuclear plant, samples contained 830 becquerels per kilogram of radioactive cesium, exceeding the government's limit of 500 becquerels. Radioactive contaminated shiitake mushrooms, above 500 becquerels per kilogram, were also found in two cities of prefecture Chiba, therefore restrictions were imposed on the shipments from these cities.On 29 October the government of the prefecture Fukushima Prefecture announced that shiitake mushrooms grown indoors at a farm in Soma, situated at the coast north from the Fukushima Daiichi plant, were contaminated with radioactive cesium: They contained 850 becquerels per kilogram, and exceeded the national safety-limit of 500-becquerel. The mushrooms were grown on beds made of woodchips mixed with other nutrients. The woodchips in the mushroom-beds sold by the agricultural cooperative of Soma were thought to have caused of the contamination. Since 24 October 2011 this farm had shipped 1,070 100-gram packages of shiitake mushrooms to nine supermarkets. Besides these no other shiitake mushrooms produced by the farm were sold to customers.In the city of Yokohama in March and October food was served to 800 people with dried shiitake-mushrooms that came from a farm near this town at a distance of 250 kilometer from Fukushima. The test-results of these mushrooms showed 2,770 Becquerels per kilo in March and 955 Becquerels per kilo in October, far above the limit of 500 Becquerels per kilo set by the Japanese government. The mushrooms were checked for contamination in the first week of November, after requests of concerned people with questions about possible contamination of the food served. No mushrooms were sold elsewhere.On 10 November 2011 some 120 kilometers away southwest from the Fukushima-reactors in the prefecture Tochigi 649 becquerels of radioactive cesium per kilogram was measured in kuritake mushrooms. Four other cities of Tochigi did already stop with the sales and shipments of the mushrooms grown there. The farmers were asked to stop all shipments and to call back the mushrooms already on the market.The regulatory safe level for iodine-131 and caesium-137 in drinking water in Japan are 100 Bq/kg and 200 Bq/kg respectively. The Japanese science ministry said on 20 March that radioactive substances were detected in tap water in Tokyo, as well as Tochigi, Gunma, Chiba and Saitama prefectures. IAEA reported on 24 March that drinking water in Tokyo, Fukushima and Ibaraki had been above regulatory limits between 16 and 21 March. On 26 March, IAEA reported that the values were now within legal limits. On 23 March, Tokyo drinking water exceeded the safe level for infants, prompting the government to distribute bottled water to families with infants. Measured levels were caused by iodine-131 (I-131) and were 103, 137 and 174 Bq/l. On 24 March, iodine-131 was detected in 12 of 47 prefectures, of which the level in Tochigi was the highest at 110 Bq/kg. Caesium-137 was detected in 6 prefectures but always below 10 Bq/kg. On 25 March, tap water was reported to have reduced to 79 Bq/kg and to be safe for infants in Tokyo and Chiba but still exceeded limits in Hitachi and Tokaimura. On 27 April, ""radiation in Tokyo's water supply fell to undetectable levels for the first time since 18 March.""The following graphs show Iodine-131 water contaminations measured in water purifying plants From 16 March to 7 April:								On 2 July samples of tapwater taken in Tokyo Shinjuku ward radioactive caesium-137 was detected for the first time since April. The concentration was 0.14 becquerel per kilogram and none was discovered yesterday, which compares with 0.21 becquerel on 22 April, according to the Tokyo Metropolitan Institute of Public Health. No caesium-134 or iodine-131 was detected. The level was below the safety limit set by the government. ""This is unlikely to be the result of new radioactive materials being introduced, because no other elements were detected, especially the more sensitive iodine"", into the water supply, were the comments of Hironobu Unesaki, a nuclear engineering professor at Kyoto University.Small amounts of radioactive iodine were found in the breast milk of women living east of Tokyo. However, the levels were below the safety limits for tap water consumption by infants. Regulatory limits for infants in Japan are several levels of magnitude beneath what is known to potentially affect human health. Radiation protection standards in Japan are currently stricter than international recommendations and the standards of most other states, including those in North America and Europe . By Nov 2012, no radioactivity was detected in Fukushimas mothers breast milk. 100% of samples contained no detectable amount of radioactivity.Mid November 2011 radioactive cesium was found in milk-powder for baby-food produced by the food company Meiji Co. Although this firm was warned about this matter three times, the matter was taken seriously by its consumer service after it was approached by Kyodo News. Up to 30.8 becquerels per kilogram was found in Meiji Step milk powder. While this is under the governmental safety-limit of 200 becquerels per kilogram, this could be more harmful for young children. Because of this cesium-contaminated milk powder, the Japanese minister of health Yoko Komiyama said on 9 December 2011 at a press conference, that her ministry would start regularly tests on baby food products in connection with the Fukushima Daiichi nuclear plant crisis, every three months and more frequently when necessary. Komiyama said: ""As mothers and other consumers are very concerned (about radiation), we want to carry out regular tests"", Test done by the government in July and August 2011 on 25 baby products did not reveal any contamination.In a survey by the local and central governments conducted on 1,080 children aged 0 to 15 in Iwaki, Kawamata and Iitate on 26–30 March, almost 45 percent of these children had experienced thyroid exposure to radiation with radioactive iodine, although in all cases the amounts of radiation did not warrant further examination, according to the Nuclear Safety Commission on Tuesday 5 July. In October 2011, hormonal irregularies in 10 evacuated children were reported. However, the organization responsible for the study said that no link had been established between the children's condition and exposure to radiation.On 9 October a survey started in the prefecture Fukushima: ultrasonic examinations were done of the thyroid glands of all 360,000 children between 0 and 18 years of age. Follow-up tests will be done for the rest of their lives. This was done in response to concerned parents, alarmed by the evidence showing increased incidence of thyroid cancer among children after the 1986 Chernobyl disaster. The project was done by the Medical University of Fukushima. The results of the tests will be mailed to the children within a month. At the end of 2014 the initial testing of all children should be completed, after this the children will undergo a thyroid checkup every 2 years until they turn 20, and once every 5 years above that age.In November 2011 in urine-samples of 1500 pre-school-children (ages 6 years or younger) from the city of Minamisoma in the prefecture Fukushima radioactive cesium was found in 104 cases. Most had levels between 20 and 30 becquerels per liter, just above the detection limit, but 187 becquerels was found in the urine of a one-year-old baby boy. The parents had been concerned about internal exposure. Local governments covered the tests for elementary schoolchildren and older students. According to RHC JAPAN a medical consultancy firm in Tokyo, these levels could not harm the health of the children. But director Makoto Akashi of the National Institute of Radiological Sciences said, that although those test results should be verified, this still proved the possibility of internal exposure in the children of Fukushima, but that the internal exposure would not increase, when all food was tested for radioactivity before consumption.Also in July citizens groups reported that a survey of soil at four places in the city of Fukushima taken on 26 June proved that all samples were contaminated with radioactive caesium, measuring 16,000 to 46,000 becquerels per kilogram and exceeding the legal limit of 10,000 becquerels per kg, A study published by the PNAS found that cesium 137 had ""strongly contaminated the soils in large areas of eastern and northeastern Japan.""After the find of 8,000 becquerels of caesium per kilogram in wild mushrooms, and a wild boar that was found with radioactivity amounts about 6 times the safety limit, Professor Yasuyuki Muramatsu at the Gakushuin University urged detailed checks on wild plants and animals. Radioactive caesium in soil and fallen leaves in forests in his opinion would be easily absorbed by mushrooms and edible plants. He said that wild animals like boars were bound to accumulate high levels of radioactivity by eating contaminated mushrooms and plants. The professor added that detailed studies wereon wild plants and animals. Across Europe the Chernobyl-incident had likewise effects on wild fauna and flora.The first study of the effects of radioactive contamination following the Fukushima Daiichi nuclear disaster suggested, through standard point count censuses that the abundance of birds was negatively correlated with radioactive contamination, and that among the 14 species in common between the Fukushima and the Chernobyl regions, the decline in abundance was presently steeper in Fukushima. However criticism of this conclusion is that naturally there would be less bird species living on a smaller amount of land, that is, in the most contaminated areas, than the number one would find living in a larger body of land, that is, in the broader area.Scientists in Alaska are testing seals struck with an unknown illness to see if it is connected to radiation from Fukushima.Japanese bloggers are also reporting sick birds that cannot fly in the Fukushima area.About a year after the nuclear disaster some Japanese scientists found what they regarded was an increased number of mutated butterflies. In their paper, they said, this was an unexpected finding, as ""insects are very resistant to radiation."" Since these are recent findings, the study suggests that these mutations have been passed down from older generations. Timothy Jorgensen, of the Department of Radiation Medicine and the Health Physics Program of Georgetown University raised a number of issues with this ""simply not credible"" paper, in the journal Nature and concluded that the team's paper is ""highly suspect due to both their internal inconsistencies and their incompatibility with earlier and more comprehensive radiation biology research on insects"".Radioactive cesium was found in high concentration in plankton in the sea near the Fukushima Daiichi Nuclear Power Plant. Samples were taken up to 60 kilometers from the coast of Iwaki city in July 2011 by scientists of the Tokyo University of Marine Science and Technology. Up to 669 becquerels per kilogram of radioactive cesium was measured in samples of animal plankton taken 3 kilometers offshore. The leader of the research-group Professor Takashi Ishimaru, said that the sea current continuously carried contaminated water southwards from the plant. Further studies to determine the effect on the food-chain and fish would be needed.Detectable levels of radiation were found in an apartment building in Nihonmatsu, Fukushima, where the foundation was made using concrete containing crushed stone collected from a quarry near the troubled Fukushima Daiichi nuclear power plant, situated inside the evacuation-zone. Of the 12 households living there were 10 households relocated after the quake. After inspection at the quarry – situated inside the evacuation-zone around the nuclear plant—in the town of Namie, Fukushima between 11 and 40 microsieverts of radiation per hour were detected one meter above gravel held at eight storage sites in the open, while 16 to 21 microsieverts were detected in three locations covered by roofs. From this place about 5,200 metric tons of gravel was shipped from this place and used as building material. On 21 January 2012 the association of quarry agents in the prefecture Fukushima asked its members to voluntarily check their products for radioactivity to ease public concerns over radioactive contamination of building materials. The minister of Industry Yukio Edano did instruct TEPCO to pay compensation for the economical damages. Raised radiation levels were found on many buildings constructed after the quake. Schools, private houses, roads. Because of the public anger raised by these finds. the government of Nihonmatsu, Fukushima decided to examine all 224 city construction projects started after the quake. Some 200 construction companies received stone from the Namie-quarry, and the material was used in at least 1000 building-sites. The contaminated stone was found in some 49 houses and apartments. Radiation levels of 0.8 mSv per hour were found, almost as high as the radiation levels outside the homes. None of these represents a potential danger to human health.On 22 January 2012, the Japanese government survey had identified around 60 houses built with the radioactive contaminated concrete. Even after 12 April 2011, when the area was declared to be an evacuation zone, the shipments continued, and the stone was used for building purposes.In the first weeks of February 2012 up to 214,200 becquerels of radioactive caesium per kilogram was measured in samples gravel in the quarry near Namie, situated inside the evacuation zone. The gravel stored outside showed about 60,000–210,000 becquerels of caesium in most samples. From the 25 quarries in the evacuation zones, up to 122,400 becquerels of radioactive caesium was found at one that has been closed since the nuclear crisis broke out on 11 March 2011. In one quarry, that is still operational 5,170 becquerels per kilogram was found. Inspections were done at some 150 of the 1.100 construction sites, where the gravel form the Namie-quarry was suspected to be used. At 27 locations the radioactivity levels were higher than the surrounding area.On 6 May 2012 it became known that according to documents of the municipal education board reports submitted by each school in Fukushima prefecture in April at least 14 elementary schools, 7 junior high and 5 nursery schools so called ""hot spots"" existed, where the radiation exposure was more than 3.8 microsieverts per hour, resulting in an annual cumulative dose above 20 millisieverts. However all restrictions, that limited the maximum time to three hours for the children to play outside at the playgrounds of the schools, were lifted at the beginning of the new academic year in April by the education board. The documents were obtained by a group of civilians after a formal request to disclose the information. Tokiko Noguchi, the foreman of a group of civilians, insisted that the education board would restore the restrictions.On 22 December 2011 the Japanese government announced new limits for radioactive cesium in food. The new norms would be enforced in April 2012.On 31 March 2012 the Ministry of Health, Labor and Welfare of Japan published a report on radioactive cesium found in food. Between January and around 15 March 2012 at 421 occasions food was found containing more than 100 becquerels per kilogram cesium. All was found within 8 prefectures: Chiba, Fukushima Prefecture (285 finds), Gunma, Ibaraki (36 finds), Iwate, Miyagi, Tochigi (29 finds) and Yamagata. Most times it involved fish: landlocked salmon and flounder, seafood, after this: Shiitake-mushrooms or the meat of wild animals.In the first week of April 2012 cesium-contamination above legal limits was found in:Shiitake mushrooms in Manazuru Kanagawa prefecture situated at 300 kilometers from Fukushima: 141 becquerels/kgbamboo-shoots in two cities in Chiba prefecturebamboo-shoots and Shiitake-mushrooms in 5 cities in the region Kantō, Ibaraki prefectureIn Gunma prefecture 106 becquerels/kg was found in beef. Sharper limits for meat would be taken effect in October 2012, but in order to ease consumer concern the farmers were asked to refrain from shipping.In the last week of August Prime Minister Naoto Kan informed the Governor of Fukushima Prefecture about the plans to build a central storage facility to store and treat nuclear waste including contaminated soil in Fukushima. On 27 August at a meeting in Fukushima City Governor Yuhei Sato spoke out his concern about the sudden proposals, and the implications that this would have for the prefecture and its inhabitants, that had already endured so much from the nuclear accident. Kan said, that the government had no intention to make the plant a final facility, but the request was needed in order to make a start with decontamination.Radioactivity from the disaster was found in kelp off of Coastal California.According to a Professor at Stanford, there were some meteorological effects involved and that ""81 percent of all the emissions were deposited over the ocean"" instead of mainly being spread inland.Seawater containing measurable levels of iodine-131 and caesium-137 were collected by Japan Agency for Marine-Earth Science and Technology (JAMSTEC) on 22–23 March at several points 30 km from the coastline iodine concentrations were ""at or above Japanese regulatory limits"" while caesium was ""well below those limits"" according to an IAEA report on 24 March. On 25 March, IAEA indicated that in the long term, caesium-137 (with a half-life of 30 years) would be the most relevant isotope as far as doses was concerned and indicated the possibility ""to follow this nuclide over long distances for several years."" The organization also said it could take months or years for the isotope to reach ""other shores of the Pacific"".The survey by the Japan Agency for Marine-Earth Science and Technology (JAMSTEC) reveals that radioactive cesium released from Fukushima I Nuclear Power Plant reached the ocean 2000 kilometers from the plant and 5000 meters deep one month after the accident. It is considered that airborne cesium particles fell on the ocean surface, and sank as they were attached to the bodies of dead plankton. The survey result was announced in a symposium held on 20 November in Tokyo. From 18 to 30 April, JAMSTEC collected ""marine snow"", sub-millimeter particles made mostly of dead plankton and sand, off the coast of Kamchatka Peninsula, 2000 kilometers away from Fukushima, and off the coast of Ogasawara Islands, 1000 kilometers away, at 5000 meters below the ocean surface. The Agency detected radioactive cesium in both locations, and from the ratio of cesium-137 and cesium-134 and other observations it was determined that it was from Fukushima I Nuclear Power Plant. The density of radioactive cesium is still being analyzed, according to the Agency. It has been thus confirmed that radioactive materials in the ocean are moving and spreading not just by ocean currents but by various other means.The United Nations predicted that the initial radioactivity plume from the stricken Japanese reactors would reach the United States by 18 March. Health and nuclear experts emphasized that radioactivity in the plume would be diluted as it traveled and, at worst, would have extremely minor health consequences in the United States. A simulation by the Belgian Institute for Space Aeronomy indicated that trace amounts of radioactivity would reach California and Mexico around 19 March. These predictions were tested by a worldwide network of highly sensitive radiative isotope measuring equipment, with the resulting data used to assess any potential impact to human health as well as the status of the reactors in Japan. Consequently, by 18 March radioactive fallout including isotopes of iodine-131, iodine-132, tellurium-132, iodine-133, caesium-134 and caesium-137 was detected in air filters at the University of Washington, Seattle, USA.Due to an anticyclone south of Japan, favorable westerly winds were dominant during most of the first week of the accident, depositing most of the radioactive material out to sea and away from population centers, with some unfavorable wind directions depositing radioactive material over Tokyo. Low-pressure area over Eastern Japan gave less favorable wind directions 21–22 March. Wind shift to north takes place Tuesday midnight. After the shift, the plume would again be pushed out to the sea for the next becoming days. Roughly similar prediction results are presented for the next 36 hours by the Finnish Meteorological Institute. In spite of winds blowing towards Tokyo during 21–22 March, he comments, ""From what I've been able to gather from official reports of radioactivity releases from the Fukushima plant, Tokyo will not receive levels of radiation dangerous to human health in the coming days, should emissions continue at current levels.""Norwegian Institute for Air Research have continuous forecasts of the radioactive cloud and its movement. These are based on the FLEXPART model, originally designed for forecasting the spread of radioactivity from the Chernobyl disaster.As of 28 April, the Washington State Department of Health, located in the U.S state closest to Japan, reported that levels of radioactive material from the Fukushima plant had dropped significantly, and were now often below levels that could be detected with standard tests.Fear of radiation from Japan prompted a global rush for iodine pills, including in the United States, Canada, Russia, Korea, China, Malaysia and Finland. There is a rush for iodized salt in China. A rush for iodine antiseptic solution appeared in Malaysia. WHO warned against consumption of iodine pills without consulting a doctor and also warned against drinking iodine antiseptic solution. The United States Pentagon said troops are receiving potassium iodide before missions to areas where possible radiation exposure is likely.The World Health Organisation (WHO) says it has received reports of people being admitted to poison centres around the world after taking iodine tablets in response to fears about harmful levels of radiation coming out of the damaged nuclear power plant in Fukushima.In Operation Tomodachi, the United States Navy dispatched the aircraft carrier USS Ronald Reagan and other vessels in the Seventh Fleet to fly a series of helicopter operations. A U.S. military spokesperson said that low-level radiation forced a change of course en route to Sendai. The Reagan and sailors aboard were exposed to ""a month's worth of natural background radiation from the sun, rocks or soil"" in an hour and the carrier was repositioned. Seventeen sailors were decontaminated after they and their three helicopters were found to have been exposed to low levels of radioactivity.The aircraft carrier USS George Washington was docked for maintenance at Yokosuka Naval Base, about 280 kilometres (170 mi) from the plant, when instruments detected radiation at 07:00 JST on 15 March. Rear Admiral Richard Wren stated that the nuclear crisis in Fukushima, 320 kilometres (200 mi) from Yokosuka, was too distant to warrant a discussion about evacuating the base. Daily monitoring and some precautionary measures were recommended for Yokosuka and Atsugi bases, such as limiting outdoor activities and securing external ventilation systems. As a precaution, the Washington was pulled out of its Yokosuka port later in the week. The Navy also temporarily stopped moving its personnel to Japan.The isotope iodine-131 is easily absorbed by the thyroid. Persons exposed to releases of I-131 from any source have a higher risk for developing thyroid cancer or thyroid disease, or both. Iodine-131 has a short half-life at approximately 8 days, and therefore is an issue mostly in the first weeks after the incident. Children are more vulnerable to I-131 than adults. Increased risk for thyroid neoplasm remains elevated for at least 40 years after exposure. Potassium iodide tablets prevent iodine-131 absorption by saturating the thyroid with non-radioactive iodine. Japan's Nuclear Safety Commission recommended local authorities to instruct evacuees leaving the 20-kilometre area to ingest stable (not radioactive) iodine. CBS News reported that the number of doses of potassium iodide available to the public in Japan was inadequate to meet the perceived needs for an extensive radioactive contamination event.Caesium-137 is also a particular threat because it behaves like potassium and is taken up by cells throughout the body. Additionally, it has a long, 30-year half-life. Cs-137 can cause acute radiation sickness, and increase the risk for cancer because of exposure to high-energy gamma radiation. Internal exposure to Cs-137, through ingestion or inhalation, allows the radioactive material to be distributed in the soft tissues, especially muscle tissue, exposing these tissues to the beta particles and gamma radiation and increasing cancer risk. Prussian blue helps the body excrete caesium-137.Strontium-90 behaves like calcium, and tends to deposit in bone and blood-forming tissue (bone marrow). 20–30% of ingested Sr-90 is absorbed and deposited in the bone. Internal exposure to Sr-90 is linked to bone cancer, cancer of the soft tissue near the bone, and leukemia. Risk of cancer increases with increased exposure to Sr-90.Plutonium is also present in the MOX fuel of the Unit 3 reactor and in spent fuel rods. Officials at the International Atomic Energy Agency say the presence of MOX fuel does not add significantly to the dangers. Plutonium-239 is long-lived and potentially toxic with a half-life of 24,000 years. Some hazardous for tens of thousands of years. Radioactive products with long half-lives release less radioactivity per unit time than products with a short half life, as isotopes with a longer half life emit particles much less frequently. For example, one mole (131 grams) of 131I releases 6x1023 decays 99.9% of them within three months, whilst one mole (238 grams) of 238U releases 6x1023 decays 99.9% of them within 45 billion years, but only about 40 parts per trillion in the first three months. Experts commented that the long-term risk associated with plutonium toxicity is ""highly dependent on the geochemistry of the particular site.""An overview for regulatory levels in Japan is shown in the table below:On 11 March, Japanese authorities reported that there had been no ""release of radiation"" from any of the power plants.On 12 March, the day after the earthquake, increased levels of iodine-131 and caesium-137 were reported near Unit 1 on the plant site.On 13 March, venting to release pressure started at several reactors resulting in the release of radioactive material.From 12 to 15 March the people of Namie were evacuated by the local officials to a place in the north of the town. This may have been in an area directly affected by a cloud of radioactive materials from the plants. There are conflicting reports about whether or not the government knew at the time the extent of the danger, or even how much danger there was.Chief Cabinet Secretary Yukio Edano announced on 15 March 2011 that radiation dose rates had been measured as high as 30 mSv/h on the site of the plant between units 2 and 3, as high as 400 mSv/h near unit 3, between it and unit 4, and 100 mSv/h near unit 4. He said, ""there is no doubt that unlike in the past, the figures are the level at which human health can be affected."" Prime Minister Naoto Kan urged people living between 20 and 30 kilometers of the plant to stay indoors, ""The danger of further radiation leaks (from the plant) is increasing"", Kan warned the public at a press conference, while asking people to ""act calmly"". A spokesman for Japan's nuclear safety agency said TEPCO had told it that radiation levels in Ibaraki, between Fukushima and Tokyo, had risen but did not pose a health risk. Edano reported that the average radiation dose rate over the whole day was 0.109 μSv/h. 23 out of 150 tested persons living close to the plant were decontaminatedOn 16 March power plant staff were briefly evacuated after smoke rose above the plant and radiation levels measured at the gate increased to 10 mSv/h. Media reported 1,000 mSv/h close to the leaking reactor, with radiation levels subsequently dropping back to 800–600 mSv. Japan's defence ministry criticized the nuclear-safety agency and TEPCO after some of its troops were possibly exposed to radiation when working on the site. Japan's ministry of science (MEXT) measured radiation levels of up to 0.33 mSv/h 20 kilometers northwest of the power plant. Japan's Nuclear Safety Commission recommended local authorities to instruct evacuees leaving the 20-kilometre area to ingest stable (not radioactive) iodine.On 17 March IAEA radiation monitoring over 47 cities showed that levels of radiation in Tokyo had not risen. Although at some locations around 30 km from the Fukushima plant, the dose rates had risen significantly in the preceding 24 hours (in one location from 80 to 170 μSv/h and in another from 26 to 95 μSv/h), levels varied according to the direction from the plant. Spinach grown in open air around 75 kilometers south of the nuclear plant had elevated levels of radioactive iodine and caesiumOn 18 March IAEA clarified that, contrary to several news reports, the IAEA had not received any notification from the Japanese authorities of people sickened by radiation contamination.On 19 March MEXT said a trace amount of radioactive substances was detected in tap water in Tokyo, as well as Tochigi, Gunma, Chiba and Saitama prefectures. The Japanese Ministry of Health, Labour and Welfare announced that radioactivity levels exceeding legal limits had been detected in milk produced in the Fukushima area and in certain vegetables in Ibaraki. Measurements made by Japan in a number of locations have shown the presence of radionuclides such as iodine-131 (I-131) and caesium-137 (Cs-137) on the ground.On 23 March, MEXT released new environmental data. Radioactivity readings for soil and pond samples were highest at one location 40 km northwest of the plant. On 19 March, upland soil there contained 28.1 kBq/kg of Cs-137 and 300 kBq/kg of I-131. One day later, these same figures were 163 kBq/kg of Cs-137 and 1,170 kBq/kg of I-131. Cs-137 of 163 kBq/kg is equal to 3,260 kBq/m2.On 24 March, three workers were exposed to high levels of radiation which caused two of them to require hospital treatment after radioactive water seeped through their protective clothes while working in unit 3. It rained in Tokyo from the morning of 21 March to 24 March. The rain brought radioactive fallout there. In Shinjuku, based on the research by Tokyo Metropolitan Institute of Public Health, 83900 Bq/m2 of I-131, 6310 Bq/m2 of Cs-134, and 6350 Bq/m2 of Cs-137 were detected for these four days in total as radioactive fallout, including 24 hours from 20 March 9:00 am to 21 March 9:00 am.On 25 March the German Ministry of the Environment announced that small amounts of radioactive iodine had been observed in three places within the German atmosphere.On 26 March, Japan's nuclear safety agency said that contamination from iodine-131 in seawater near the discharge had increased to 1,850 times the limit.27 March: Levels of ""over 1000"" (the upper limit of the measuring device) and 750 mSv/h were reported from water within unit 2 (but outside the containment structure) and 3 respectively. A statement that this level was ""ten million times the normal level"" in unit 2 was later retracted and attributed to iodine-134 rather than to a longer-lived element. Japan's Nuclear and Industrial Safety Agency indicated that ""The level of radiation is greater than 1,000 millisieverts. It is certain that it comes from atomic fission [...]. But we are not sure how it came from the reactor.""29 March: iodine-131 levels in seawater 330m south of a key discharge outlet had reached 138 Bq/ml (3,355 times the legal limit)30 March: iodine-131 concentrations in seawater had reached 180 Bq/ml at a location 330m south of a plant discharge, 4,385 times the legal limit. Tests indicating 3.7 MBq/m2 of Cs-137 caused the IAEA to state that its criteria for evacuation were exceeded in the village of Iitate, Fukushima, outside the existing 30 kilometres (19 miles) radiation exclusion zone.On 31 March, IAEA corrected the value of iodine-131 that had been detected in the Iitate village to 20 million Bq/m2. The value that had been announced at a press interview was about 2 million Bq/m2.On 1 April, besides leafy vegetables and parsley, also beef with iodine-131 and/or caesium-134 and caesium-137 levels above the regulatory limit was reported.3 April: Health officials reported radioactive substances higher than the legal limits were found in mushrooms. The Japanese government publicly stated that it expected ongoing radioactive-material releases for ""months"" assuming normal containment measures were used.4 to 10 April TEPCO announced it had begun dumping 9,100 tons of water that was 100 times the contamination limit from a wastewater treatment plant, and dumping would take 6 days.5 April: Fish caught 50 miles off the coast of Japan had radioactivity exceeding safe levels.15 April: Iodine-131 in seawater was measured at 6,500 times the legal limit, while levels of caesium-134 and caesium-137 rose nearly fourfold, possibly due to installation of steel plates meant to reduce the possibility of water leaking into the ocean.18 April: High levels of radioactive strontium-90 were discovered in soil at the plant, prompting the government to begin regularly testing for the element.22 April: The Japanese government asked residents to leave Iitate and four other villages within a month due to radiation levels.2011 Japanese nuclear accidentsFukushima 50Hibakusha (surviving victims of the atomic bombings of Hiroshima and Nagasaki)Lists of nuclear disasters and radioactive incidentsHealth risk assessment from the nuclear accident after the 2011 Great East Japan earthquake and tsunami, based on a preliminary dose estimation. World Health Organization. 2013. ISBN 978-92-4-150513-0. PM Information on contaminated water leakage at TEPCO's Fukushima Daiichi Nuclear Power Station, Prime Minister of Japan and His CabinetMOFA Information on contaminated water leakage at TEPCO’s Fukushima Daiichi Nuclear Power Station, Ministry of Foreign AffairsTEPCO News Releases, Tokyo Electric Power CompanyNRA, Japan, Nuclear Regulation AuthorityNISA, Nuclear and Industrial Safety Agency, former organizationIAEA Update on Japan Earthquake, International Atomic Energy AgencyNavigating Fukushima: Lessons from Chernobyl, Potential Radiation Effects, and Other Health Impacts, Q&A with Dr. Scott Davis about the mechanics of the crisis in Fukushima and how it compares to ChernobylChristodouleas, John P. Forrest, Robert D. Ainsley, Christopher G. Tochner, Zelig Hahn, Stephen M. Glatstein, Eli (2011). ""Short-Term and Long-Term Health Risks of Nuclear-Power-Plant Accidents"". New England Journal of Medicine. 364 (24): 2334–41. doi:10.1056/NEJMra1103676. PMID 21506737. Detailed measurements of radiation levels in air at Fukushima IFukushima: A Nuclear War without a War: The Unspoken Crisis of Worldwide Nuclear Radiation | Global Research - Centre for Research on Globalization (Scroll halfway down to table of contents.)";environmental disaster;Radiation effects from Fukushima Daiichi nuclear disaster;0
